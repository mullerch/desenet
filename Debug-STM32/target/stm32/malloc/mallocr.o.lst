   1              		.syntax unified
   2              		.cpu cortex-m3
   3              		.fpu softvfp
   4              		.eabi_attribute 20, 1
   5              		.eabi_attribute 21, 1
   6              		.eabi_attribute 23, 3
   7              		.eabi_attribute 24, 1
   8              		.eabi_attribute 25, 1
   9              		.eabi_attribute 26, 1
  10              		.eabi_attribute 30, 6
  11              		.eabi_attribute 18, 4
  12              		.thumb
  13              		.file	"mallocr.c"
  23              	.Ltext0:
  24              		.file 1 "../target/stm32/malloc/mallocr.c"
 3121              		.align	2
 3124              	__malloc_av_:
 3125 0000 00000000 		.word	0
 3126 0004 00000000 		.word	0
 3127 0008 00000000 		.word	__malloc_av_
 3128 000c 00000000 		.word	__malloc_av_
 3129 0010 08000000 		.word	__malloc_av_+8
 3130 0014 08000000 		.word	__malloc_av_+8
 3131 0018 10000000 		.word	__malloc_av_+16
 3132 001c 10000000 		.word	__malloc_av_+16
 3133 0020 18000000 		.word	__malloc_av_+24
 3134 0024 18000000 		.word	__malloc_av_+24
 3135 0028 20000000 		.word	__malloc_av_+32
 3136 002c 20000000 		.word	__malloc_av_+32
 3137 0030 28000000 		.word	__malloc_av_+40
 3138 0034 28000000 		.word	__malloc_av_+40
 3139 0038 30000000 		.word	__malloc_av_+48
 3140 003c 30000000 		.word	__malloc_av_+48
 3141 0040 38000000 		.word	__malloc_av_+56
 3142 0044 38000000 		.word	__malloc_av_+56
 3143 0048 40000000 		.word	__malloc_av_+64
 3144 004c 40000000 		.word	__malloc_av_+64
 3145 0050 48000000 		.word	__malloc_av_+72
 3146 0054 48000000 		.word	__malloc_av_+72
 3147 0058 50000000 		.word	__malloc_av_+80
 3148 005c 50000000 		.word	__malloc_av_+80
 3149 0060 58000000 		.word	__malloc_av_+88
 3150 0064 58000000 		.word	__malloc_av_+88
 3151 0068 60000000 		.word	__malloc_av_+96
 3152 006c 60000000 		.word	__malloc_av_+96
 3153 0070 68000000 		.word	__malloc_av_+104
 3154 0074 68000000 		.word	__malloc_av_+104
 3155 0078 70000000 		.word	__malloc_av_+112
 3156 007c 70000000 		.word	__malloc_av_+112
 3157 0080 78000000 		.word	__malloc_av_+120
 3158 0084 78000000 		.word	__malloc_av_+120
 3159 0088 80000000 		.word	__malloc_av_+128
 3160 008c 80000000 		.word	__malloc_av_+128
 3161 0090 88000000 		.word	__malloc_av_+136
 3162 0094 88000000 		.word	__malloc_av_+136
 3163 0098 90000000 		.word	__malloc_av_+144
 3164 009c 90000000 		.word	__malloc_av_+144
 3165 00a0 98000000 		.word	__malloc_av_+152
 3166 00a4 98000000 		.word	__malloc_av_+152
 3167 00a8 A0000000 		.word	__malloc_av_+160
 3168 00ac A0000000 		.word	__malloc_av_+160
 3169 00b0 A8000000 		.word	__malloc_av_+168
 3170 00b4 A8000000 		.word	__malloc_av_+168
 3171 00b8 B0000000 		.word	__malloc_av_+176
 3172 00bc B0000000 		.word	__malloc_av_+176
 3173 00c0 B8000000 		.word	__malloc_av_+184
 3174 00c4 B8000000 		.word	__malloc_av_+184
 3175 00c8 C0000000 		.word	__malloc_av_+192
 3176 00cc C0000000 		.word	__malloc_av_+192
 3177 00d0 C8000000 		.word	__malloc_av_+200
 3178 00d4 C8000000 		.word	__malloc_av_+200
 3179 00d8 D0000000 		.word	__malloc_av_+208
 3180 00dc D0000000 		.word	__malloc_av_+208
 3181 00e0 D8000000 		.word	__malloc_av_+216
 3182 00e4 D8000000 		.word	__malloc_av_+216
 3183 00e8 E0000000 		.word	__malloc_av_+224
 3184 00ec E0000000 		.word	__malloc_av_+224
 3185 00f0 E8000000 		.word	__malloc_av_+232
 3186 00f4 E8000000 		.word	__malloc_av_+232
 3187 00f8 F0000000 		.word	__malloc_av_+240
 3188 00fc F0000000 		.word	__malloc_av_+240
 3189 0100 F8000000 		.word	__malloc_av_+248
 3190 0104 F8000000 		.word	__malloc_av_+248
 3191 0108 00010000 		.word	__malloc_av_+256
 3192 010c 00010000 		.word	__malloc_av_+256
 3193 0110 08010000 		.word	__malloc_av_+264
 3194 0114 08010000 		.word	__malloc_av_+264
 3195 0118 10010000 		.word	__malloc_av_+272
 3196 011c 10010000 		.word	__malloc_av_+272
 3197 0120 18010000 		.word	__malloc_av_+280
 3198 0124 18010000 		.word	__malloc_av_+280
 3199 0128 20010000 		.word	__malloc_av_+288
 3200 012c 20010000 		.word	__malloc_av_+288
 3201 0130 28010000 		.word	__malloc_av_+296
 3202 0134 28010000 		.word	__malloc_av_+296
 3203 0138 30010000 		.word	__malloc_av_+304
 3204 013c 30010000 		.word	__malloc_av_+304
 3205 0140 38010000 		.word	__malloc_av_+312
 3206 0144 38010000 		.word	__malloc_av_+312
 3207 0148 40010000 		.word	__malloc_av_+320
 3208 014c 40010000 		.word	__malloc_av_+320
 3209 0150 48010000 		.word	__malloc_av_+328
 3210 0154 48010000 		.word	__malloc_av_+328
 3211 0158 50010000 		.word	__malloc_av_+336
 3212 015c 50010000 		.word	__malloc_av_+336
 3213 0160 58010000 		.word	__malloc_av_+344
 3214 0164 58010000 		.word	__malloc_av_+344
 3215 0168 60010000 		.word	__malloc_av_+352
 3216 016c 60010000 		.word	__malloc_av_+352
 3217 0170 68010000 		.word	__malloc_av_+360
 3218 0174 68010000 		.word	__malloc_av_+360
 3219 0178 70010000 		.word	__malloc_av_+368
 3220 017c 70010000 		.word	__malloc_av_+368
 3221 0180 78010000 		.word	__malloc_av_+376
 3222 0184 78010000 		.word	__malloc_av_+376
 3223 0188 80010000 		.word	__malloc_av_+384
 3224 018c 80010000 		.word	__malloc_av_+384
 3225 0190 88010000 		.word	__malloc_av_+392
 3226 0194 88010000 		.word	__malloc_av_+392
 3227 0198 90010000 		.word	__malloc_av_+400
 3228 019c 90010000 		.word	__malloc_av_+400
 3229 01a0 98010000 		.word	__malloc_av_+408
 3230 01a4 98010000 		.word	__malloc_av_+408
 3231 01a8 A0010000 		.word	__malloc_av_+416
 3232 01ac A0010000 		.word	__malloc_av_+416
 3233 01b0 A8010000 		.word	__malloc_av_+424
 3234 01b4 A8010000 		.word	__malloc_av_+424
 3235 01b8 B0010000 		.word	__malloc_av_+432
 3236 01bc B0010000 		.word	__malloc_av_+432
 3237 01c0 B8010000 		.word	__malloc_av_+440
 3238 01c4 B8010000 		.word	__malloc_av_+440
 3239 01c8 C0010000 		.word	__malloc_av_+448
 3240 01cc C0010000 		.word	__malloc_av_+448
 3241 01d0 C8010000 		.word	__malloc_av_+456
 3242 01d4 C8010000 		.word	__malloc_av_+456
 3243 01d8 D0010000 		.word	__malloc_av_+464
 3244 01dc D0010000 		.word	__malloc_av_+464
 3245 01e0 D8010000 		.word	__malloc_av_+472
 3246 01e4 D8010000 		.word	__malloc_av_+472
 3247 01e8 E0010000 		.word	__malloc_av_+480
 3248 01ec E0010000 		.word	__malloc_av_+480
 3249 01f0 E8010000 		.word	__malloc_av_+488
 3250 01f4 E8010000 		.word	__malloc_av_+488
 3251 01f8 F0010000 		.word	__malloc_av_+496
 3252 01fc F0010000 		.word	__malloc_av_+496
 3253 0200 F8010000 		.word	__malloc_av_+504
 3254 0204 F8010000 		.word	__malloc_av_+504
 3255 0208 00020000 		.word	__malloc_av_+512
 3256 020c 00020000 		.word	__malloc_av_+512
 3257 0210 08020000 		.word	__malloc_av_+520
 3258 0214 08020000 		.word	__malloc_av_+520
 3259 0218 10020000 		.word	__malloc_av_+528
 3260 021c 10020000 		.word	__malloc_av_+528
 3261 0220 18020000 		.word	__malloc_av_+536
 3262 0224 18020000 		.word	__malloc_av_+536
 3263 0228 20020000 		.word	__malloc_av_+544
 3264 022c 20020000 		.word	__malloc_av_+544
 3265 0230 28020000 		.word	__malloc_av_+552
 3266 0234 28020000 		.word	__malloc_av_+552
 3267 0238 30020000 		.word	__malloc_av_+560
 3268 023c 30020000 		.word	__malloc_av_+560
 3269 0240 38020000 		.word	__malloc_av_+568
 3270 0244 38020000 		.word	__malloc_av_+568
 3271 0248 40020000 		.word	__malloc_av_+576
 3272 024c 40020000 		.word	__malloc_av_+576
 3273 0250 48020000 		.word	__malloc_av_+584
 3274 0254 48020000 		.word	__malloc_av_+584
 3275 0258 50020000 		.word	__malloc_av_+592
 3276 025c 50020000 		.word	__malloc_av_+592
 3277 0260 58020000 		.word	__malloc_av_+600
 3278 0264 58020000 		.word	__malloc_av_+600
 3279 0268 60020000 		.word	__malloc_av_+608
 3280 026c 60020000 		.word	__malloc_av_+608
 3281 0270 68020000 		.word	__malloc_av_+616
 3282 0274 68020000 		.word	__malloc_av_+616
 3283 0278 70020000 		.word	__malloc_av_+624
 3284 027c 70020000 		.word	__malloc_av_+624
 3285 0280 78020000 		.word	__malloc_av_+632
 3286 0284 78020000 		.word	__malloc_av_+632
 3287 0288 80020000 		.word	__malloc_av_+640
 3288 028c 80020000 		.word	__malloc_av_+640
 3289 0290 88020000 		.word	__malloc_av_+648
 3290 0294 88020000 		.word	__malloc_av_+648
 3291 0298 90020000 		.word	__malloc_av_+656
 3292 029c 90020000 		.word	__malloc_av_+656
 3293 02a0 98020000 		.word	__malloc_av_+664
 3294 02a4 98020000 		.word	__malloc_av_+664
 3295 02a8 A0020000 		.word	__malloc_av_+672
 3296 02ac A0020000 		.word	__malloc_av_+672
 3297 02b0 A8020000 		.word	__malloc_av_+680
 3298 02b4 A8020000 		.word	__malloc_av_+680
 3299 02b8 B0020000 		.word	__malloc_av_+688
 3300 02bc B0020000 		.word	__malloc_av_+688
 3301 02c0 B8020000 		.word	__malloc_av_+696
 3302 02c4 B8020000 		.word	__malloc_av_+696
 3303 02c8 C0020000 		.word	__malloc_av_+704
 3304 02cc C0020000 		.word	__malloc_av_+704
 3305 02d0 C8020000 		.word	__malloc_av_+712
 3306 02d4 C8020000 		.word	__malloc_av_+712
 3307 02d8 D0020000 		.word	__malloc_av_+720
 3308 02dc D0020000 		.word	__malloc_av_+720
 3309 02e0 D8020000 		.word	__malloc_av_+728
 3310 02e4 D8020000 		.word	__malloc_av_+728
 3311 02e8 E0020000 		.word	__malloc_av_+736
 3312 02ec E0020000 		.word	__malloc_av_+736
 3313 02f0 E8020000 		.word	__malloc_av_+744
 3314 02f4 E8020000 		.word	__malloc_av_+744
 3315 02f8 F0020000 		.word	__malloc_av_+752
 3316 02fc F0020000 		.word	__malloc_av_+752
 3317 0300 F8020000 		.word	__malloc_av_+760
 3318 0304 F8020000 		.word	__malloc_av_+760
 3319 0308 00030000 		.word	__malloc_av_+768
 3320 030c 00030000 		.word	__malloc_av_+768
 3321 0310 08030000 		.word	__malloc_av_+776
 3322 0314 08030000 		.word	__malloc_av_+776
 3323 0318 10030000 		.word	__malloc_av_+784
 3324 031c 10030000 		.word	__malloc_av_+784
 3325 0320 18030000 		.word	__malloc_av_+792
 3326 0324 18030000 		.word	__malloc_av_+792
 3327 0328 20030000 		.word	__malloc_av_+800
 3328 032c 20030000 		.word	__malloc_av_+800
 3329 0330 28030000 		.word	__malloc_av_+808
 3330 0334 28030000 		.word	__malloc_av_+808
 3331 0338 30030000 		.word	__malloc_av_+816
 3332 033c 30030000 		.word	__malloc_av_+816
 3333 0340 38030000 		.word	__malloc_av_+824
 3334 0344 38030000 		.word	__malloc_av_+824
 3335 0348 40030000 		.word	__malloc_av_+832
 3336 034c 40030000 		.word	__malloc_av_+832
 3337 0350 48030000 		.word	__malloc_av_+840
 3338 0354 48030000 		.word	__malloc_av_+840
 3339 0358 50030000 		.word	__malloc_av_+848
 3340 035c 50030000 		.word	__malloc_av_+848
 3341 0360 58030000 		.word	__malloc_av_+856
 3342 0364 58030000 		.word	__malloc_av_+856
 3343 0368 60030000 		.word	__malloc_av_+864
 3344 036c 60030000 		.word	__malloc_av_+864
 3345 0370 68030000 		.word	__malloc_av_+872
 3346 0374 68030000 		.word	__malloc_av_+872
 3347 0378 70030000 		.word	__malloc_av_+880
 3348 037c 70030000 		.word	__malloc_av_+880
 3349 0380 78030000 		.word	__malloc_av_+888
 3350 0384 78030000 		.word	__malloc_av_+888
 3351 0388 80030000 		.word	__malloc_av_+896
 3352 038c 80030000 		.word	__malloc_av_+896
 3353 0390 88030000 		.word	__malloc_av_+904
 3354 0394 88030000 		.word	__malloc_av_+904
 3355 0398 90030000 		.word	__malloc_av_+912
 3356 039c 90030000 		.word	__malloc_av_+912
 3357 03a0 98030000 		.word	__malloc_av_+920
 3358 03a4 98030000 		.word	__malloc_av_+920
 3359 03a8 A0030000 		.word	__malloc_av_+928
 3360 03ac A0030000 		.word	__malloc_av_+928
 3361 03b0 A8030000 		.word	__malloc_av_+936
 3362 03b4 A8030000 		.word	__malloc_av_+936
 3363 03b8 B0030000 		.word	__malloc_av_+944
 3364 03bc B0030000 		.word	__malloc_av_+944
 3365 03c0 B8030000 		.word	__malloc_av_+952
 3366 03c4 B8030000 		.word	__malloc_av_+952
 3367 03c8 C0030000 		.word	__malloc_av_+960
 3368 03cc C0030000 		.word	__malloc_av_+960
 3369 03d0 C8030000 		.word	__malloc_av_+968
 3370 03d4 C8030000 		.word	__malloc_av_+968
 3371 03d8 D0030000 		.word	__malloc_av_+976
 3372 03dc D0030000 		.word	__malloc_av_+976
 3373 03e0 D8030000 		.word	__malloc_av_+984
 3374 03e4 D8030000 		.word	__malloc_av_+984
 3375 03e8 E0030000 		.word	__malloc_av_+992
 3376 03ec E0030000 		.word	__malloc_av_+992
 3377 03f0 E8030000 		.word	__malloc_av_+1000
 3378 03f4 E8030000 		.word	__malloc_av_+1000
 3379 03f8 F0030000 		.word	__malloc_av_+1008
 3380 03fc F0030000 		.word	__malloc_av_+1008
 3381 0400 F8030000 		.word	__malloc_av_+1016
 3382 0404 F8030000 		.word	__malloc_av_+1016
 3383              		.global	__malloc_trim_threshold
 3384              		.section	.data.__malloc_trim_threshold,"aw",%progbits
 3385              		.align	2
 3388              	__malloc_trim_threshold:
 3389 0000 00000200 		.word	131072
 3390              		.global	__malloc_top_pad
 3391              		.section	.bss.__malloc_top_pad,"aw",%nobits
 3392              		.align	2
 3395              	__malloc_top_pad:
 3396 0000 00000000 		.space	4
 3397              		.global	__malloc_sbrk_base
 3398              		.section	.data.__malloc_sbrk_base,"aw",%progbits
 3399              		.align	2
 3402              	__malloc_sbrk_base:
 3403 0000 FFFFFFFF 		.word	-1
 3404              		.global	__malloc_max_sbrked_mem
 3405              		.section	.bss.__malloc_max_sbrked_mem,"aw",%nobits
 3406              		.align	2
 3409              	__malloc_max_sbrked_mem:
 3410 0000 00000000 		.space	4
 3411              		.global	__malloc_max_total_mem
 3412              		.section	.bss.__malloc_max_total_mem,"aw",%nobits
 3413              		.align	2
 3416              	__malloc_max_total_mem:
 3417 0000 00000000 		.space	4
 3418              		.global	__malloc_current_mallinfo
 3419              		.section	.bss.__malloc_current_mallinfo,"aw",%nobits
 3420              		.align	2
 3423              	__malloc_current_mallinfo:
 3424 0000 00000000 		.space	40
 3424      00000000 
 3424      00000000 
 3424      00000000 
 3424      00000000 
 3425              		.section	.text.malloc_extend_top,"ax",%progbits
 3426              		.align	2
 3427              		.thumb
 3428              		.thumb_func
 3430              	malloc_extend_top:
 3431              	.LFB0:
   1:../target/stm32/malloc/mallocr.c **** #ifdef MALLOC_PROVIDED
   2:../target/stm32/malloc/mallocr.c **** int _dummy_mallocr = 1;
   3:../target/stm32/malloc/mallocr.c **** #else
   4:../target/stm32/malloc/mallocr.c **** /* ---------- To make a malloc.h, start cutting here ------------ */
   5:../target/stm32/malloc/mallocr.c **** 
   6:../target/stm32/malloc/mallocr.c **** /* 
   7:../target/stm32/malloc/mallocr.c ****   A version of malloc/free/realloc written by Doug Lea and released to the 
   8:../target/stm32/malloc/mallocr.c ****   public domain.  Send questions/comments/complaints/performance data
   9:../target/stm32/malloc/mallocr.c ****   to dl@cs.oswego.edu
  10:../target/stm32/malloc/mallocr.c **** 
  11:../target/stm32/malloc/mallocr.c **** * VERSION 2.6.5  Wed Jun 17 15:55:16 1998  Doug Lea  (dl at gee)
  12:../target/stm32/malloc/mallocr.c ****   
  13:../target/stm32/malloc/mallocr.c ****    Note: There may be an updated version of this malloc obtainable at
  14:../target/stm32/malloc/mallocr.c ****            ftp://g.oswego.edu/pub/misc/malloc.c
  15:../target/stm32/malloc/mallocr.c ****          Check before installing!
  16:../target/stm32/malloc/mallocr.c **** 
  17:../target/stm32/malloc/mallocr.c ****    Note: This version differs from 2.6.4 only by correcting a
  18:../target/stm32/malloc/mallocr.c ****          statement ordering error that could cause failures only
  19:../target/stm32/malloc/mallocr.c ****          when calls to this malloc are interposed with calls to
  20:../target/stm32/malloc/mallocr.c ****          other memory allocators.
  21:../target/stm32/malloc/mallocr.c **** 
  22:../target/stm32/malloc/mallocr.c **** * Why use this malloc?
  23:../target/stm32/malloc/mallocr.c **** 
  24:../target/stm32/malloc/mallocr.c ****   This is not the fastest, most space-conserving, most portable, or
  25:../target/stm32/malloc/mallocr.c ****   most tunable malloc ever written. However it is among the fastest
  26:../target/stm32/malloc/mallocr.c ****   while also being among the most space-conserving, portable and tunable.
  27:../target/stm32/malloc/mallocr.c ****   Consistent balance across these factors results in a good general-purpose 
  28:../target/stm32/malloc/mallocr.c ****   allocator. For a high-level description, see 
  29:../target/stm32/malloc/mallocr.c ****      http://g.oswego.edu/dl/html/malloc.html
  30:../target/stm32/malloc/mallocr.c **** 
  31:../target/stm32/malloc/mallocr.c **** * Synopsis of public routines
  32:../target/stm32/malloc/mallocr.c **** 
  33:../target/stm32/malloc/mallocr.c ****   (Much fuller descriptions are contained in the program documentation below.)
  34:../target/stm32/malloc/mallocr.c **** 
  35:../target/stm32/malloc/mallocr.c ****   malloc(size_t n);
  36:../target/stm32/malloc/mallocr.c ****      Return a pointer to a newly allocated chunk of at least n bytes, or null
  37:../target/stm32/malloc/mallocr.c ****      if no space is available.
  38:../target/stm32/malloc/mallocr.c ****   free(Void_t* p);
  39:../target/stm32/malloc/mallocr.c ****      Release the chunk of memory pointed to by p, or no effect if p is null.
  40:../target/stm32/malloc/mallocr.c ****   realloc(Void_t* p, size_t n);
  41:../target/stm32/malloc/mallocr.c ****      Return a pointer to a chunk of size n that contains the same data
  42:../target/stm32/malloc/mallocr.c ****      as does chunk p up to the minimum of (n, p's size) bytes, or null
  43:../target/stm32/malloc/mallocr.c ****      if no space is available. The returned pointer may or may not be
  44:../target/stm32/malloc/mallocr.c ****      the same as p. If p is null, equivalent to malloc.  Unless the
  45:../target/stm32/malloc/mallocr.c ****      #define REALLOC_ZERO_BYTES_FREES below is set, realloc with a
  46:../target/stm32/malloc/mallocr.c ****      size argument of zero (re)allocates a minimum-sized chunk.
  47:../target/stm32/malloc/mallocr.c ****   memalign(size_t alignment, size_t n);
  48:../target/stm32/malloc/mallocr.c ****      Return a pointer to a newly allocated chunk of n bytes, aligned
  49:../target/stm32/malloc/mallocr.c ****      in accord with the alignment argument, which must be a power of
  50:../target/stm32/malloc/mallocr.c ****      two.
  51:../target/stm32/malloc/mallocr.c ****   valloc(size_t n);
  52:../target/stm32/malloc/mallocr.c ****      Equivalent to memalign(pagesize, n), where pagesize is the page
  53:../target/stm32/malloc/mallocr.c ****      size of the system (or as near to this as can be figured out from
  54:../target/stm32/malloc/mallocr.c ****      all the includes/defines below.)
  55:../target/stm32/malloc/mallocr.c ****   pvalloc(size_t n);
  56:../target/stm32/malloc/mallocr.c ****      Equivalent to valloc(minimum-page-that-holds(n)), that is,
  57:../target/stm32/malloc/mallocr.c ****      round up n to nearest pagesize.
  58:../target/stm32/malloc/mallocr.c ****   calloc(size_t unit, size_t quantity);
  59:../target/stm32/malloc/mallocr.c ****      Returns a pointer to quantity * unit bytes, with all locations
  60:../target/stm32/malloc/mallocr.c ****      set to zero.
  61:../target/stm32/malloc/mallocr.c ****   cfree(Void_t* p);
  62:../target/stm32/malloc/mallocr.c ****      Equivalent to free(p).
  63:../target/stm32/malloc/mallocr.c ****   malloc_trim(size_t pad);
  64:../target/stm32/malloc/mallocr.c ****      Release all but pad bytes of freed top-most memory back 
  65:../target/stm32/malloc/mallocr.c ****      to the system. Return 1 if successful, else 0.
  66:../target/stm32/malloc/mallocr.c ****   malloc_usable_size(Void_t* p);
  67:../target/stm32/malloc/mallocr.c ****      Report the number usable allocated bytes associated with allocated
  68:../target/stm32/malloc/mallocr.c ****      chunk p. This may or may not report more bytes than were requested,
  69:../target/stm32/malloc/mallocr.c ****      due to alignment and minimum size constraints.
  70:../target/stm32/malloc/mallocr.c ****   malloc_stats();
  71:../target/stm32/malloc/mallocr.c ****      Prints brief summary statistics on stderr.
  72:../target/stm32/malloc/mallocr.c ****   mallinfo()
  73:../target/stm32/malloc/mallocr.c ****      Returns (by copy) a struct containing various summary statistics.
  74:../target/stm32/malloc/mallocr.c ****   mallopt(int parameter_number, int parameter_value)
  75:../target/stm32/malloc/mallocr.c ****      Changes one of the tunable parameters described below. Returns
  76:../target/stm32/malloc/mallocr.c ****      1 if successful in changing the parameter, else 0.
  77:../target/stm32/malloc/mallocr.c **** 
  78:../target/stm32/malloc/mallocr.c **** * Vital statistics:
  79:../target/stm32/malloc/mallocr.c **** 
  80:../target/stm32/malloc/mallocr.c ****   Alignment:                            8-byte
  81:../target/stm32/malloc/mallocr.c ****        8 byte alignment is currently hardwired into the design.  This
  82:../target/stm32/malloc/mallocr.c ****        seems to suffice for all current machines and C compilers.
  83:../target/stm32/malloc/mallocr.c **** 
  84:../target/stm32/malloc/mallocr.c ****   Assumed pointer representation:       4 or 8 bytes
  85:../target/stm32/malloc/mallocr.c ****        Code for 8-byte pointers is untested by me but has worked
  86:../target/stm32/malloc/mallocr.c ****        reliably by Wolfram Gloger, who contributed most of the
  87:../target/stm32/malloc/mallocr.c ****        changes supporting this.
  88:../target/stm32/malloc/mallocr.c **** 
  89:../target/stm32/malloc/mallocr.c ****   Assumed size_t  representation:       4 or 8 bytes
  90:../target/stm32/malloc/mallocr.c ****        Note that size_t is allowed to be 4 bytes even if pointers are 8.        
  91:../target/stm32/malloc/mallocr.c **** 
  92:../target/stm32/malloc/mallocr.c ****   Minimum overhead per allocated chunk: 4 or 8 bytes
  93:../target/stm32/malloc/mallocr.c ****        Each malloced chunk has a hidden overhead of 4 bytes holding size
  94:../target/stm32/malloc/mallocr.c ****        and status information.  
  95:../target/stm32/malloc/mallocr.c **** 
  96:../target/stm32/malloc/mallocr.c ****   Minimum allocated size: 4-byte ptrs:  16 bytes    (including 4 overhead)
  97:../target/stm32/malloc/mallocr.c ****                           8-byte ptrs:  24/32 bytes (including, 4/8 overhead)
  98:../target/stm32/malloc/mallocr.c ****                                      
  99:../target/stm32/malloc/mallocr.c ****        When a chunk is freed, 12 (for 4byte ptrs) or 20 (for 8 byte
 100:../target/stm32/malloc/mallocr.c ****        ptrs but 4 byte size) or 24 (for 8/8) additional bytes are 
 101:../target/stm32/malloc/mallocr.c ****        needed; 4 (8) for a trailing size field
 102:../target/stm32/malloc/mallocr.c ****        and 8 (16) bytes for free list pointers. Thus, the minimum
 103:../target/stm32/malloc/mallocr.c ****        allocatable size is 16/24/32 bytes.
 104:../target/stm32/malloc/mallocr.c **** 
 105:../target/stm32/malloc/mallocr.c ****        Even a request for zero bytes (i.e., malloc(0)) returns a
 106:../target/stm32/malloc/mallocr.c ****        pointer to something of the minimum allocatable size.
 107:../target/stm32/malloc/mallocr.c **** 
 108:../target/stm32/malloc/mallocr.c ****   Maximum allocated size: 4-byte size_t: 2^31 -  8 bytes
 109:../target/stm32/malloc/mallocr.c ****                           8-byte size_t: 2^63 - 16 bytes
 110:../target/stm32/malloc/mallocr.c **** 
 111:../target/stm32/malloc/mallocr.c ****        It is assumed that (possibly signed) size_t bit values suffice to
 112:../target/stm32/malloc/mallocr.c ****        represent chunk sizes. `Possibly signed' is due to the fact
 113:../target/stm32/malloc/mallocr.c ****        that `size_t' may be defined on a system as either a signed or
 114:../target/stm32/malloc/mallocr.c ****        an unsigned type. To be conservative, values that would appear
 115:../target/stm32/malloc/mallocr.c ****        as negative numbers are avoided.  
 116:../target/stm32/malloc/mallocr.c ****        Requests for sizes with a negative sign bit will return a
 117:../target/stm32/malloc/mallocr.c ****        minimum-sized chunk.
 118:../target/stm32/malloc/mallocr.c **** 
 119:../target/stm32/malloc/mallocr.c ****   Maximum overhead wastage per allocated chunk: normally 15 bytes
 120:../target/stm32/malloc/mallocr.c **** 
 121:../target/stm32/malloc/mallocr.c ****        Alignnment demands, plus the minimum allocatable size restriction
 122:../target/stm32/malloc/mallocr.c ****        make the normal worst-case wastage 15 bytes (i.e., up to 15
 123:../target/stm32/malloc/mallocr.c ****        more bytes will be allocated than were requested in malloc), with 
 124:../target/stm32/malloc/mallocr.c ****        two exceptions:
 125:../target/stm32/malloc/mallocr.c ****          1. Because requests for zero bytes allocate non-zero space,
 126:../target/stm32/malloc/mallocr.c ****             the worst case wastage for a request of zero bytes is 24 bytes.
 127:../target/stm32/malloc/mallocr.c ****          2. For requests >= mmap_threshold that are serviced via
 128:../target/stm32/malloc/mallocr.c ****             mmap(), the worst case wastage is 8 bytes plus the remainder
 129:../target/stm32/malloc/mallocr.c ****             from a system page (the minimal mmap unit); typically 4096 bytes.
 130:../target/stm32/malloc/mallocr.c **** 
 131:../target/stm32/malloc/mallocr.c **** * Limitations
 132:../target/stm32/malloc/mallocr.c **** 
 133:../target/stm32/malloc/mallocr.c ****     Here are some features that are NOT currently supported
 134:../target/stm32/malloc/mallocr.c **** 
 135:../target/stm32/malloc/mallocr.c ****     * No user-definable hooks for callbacks and the like.
 136:../target/stm32/malloc/mallocr.c ****     * No automated mechanism for fully checking that all accesses
 137:../target/stm32/malloc/mallocr.c ****       to malloced memory stay within their bounds.
 138:../target/stm32/malloc/mallocr.c ****     * No support for compaction.
 139:../target/stm32/malloc/mallocr.c **** 
 140:../target/stm32/malloc/mallocr.c **** * Synopsis of compile-time options:
 141:../target/stm32/malloc/mallocr.c **** 
 142:../target/stm32/malloc/mallocr.c ****     People have reported using previous versions of this malloc on all
 143:../target/stm32/malloc/mallocr.c ****     versions of Unix, sometimes by tweaking some of the defines
 144:../target/stm32/malloc/mallocr.c ****     below. It has been tested most extensively on Solaris and
 145:../target/stm32/malloc/mallocr.c ****     Linux. It is also reported to work on WIN32 platforms.
 146:../target/stm32/malloc/mallocr.c ****     People have also reported adapting this malloc for use in
 147:../target/stm32/malloc/mallocr.c ****     stand-alone embedded systems.
 148:../target/stm32/malloc/mallocr.c **** 
 149:../target/stm32/malloc/mallocr.c ****     The implementation is in straight, hand-tuned ANSI C.  Among other
 150:../target/stm32/malloc/mallocr.c ****     consequences, it uses a lot of macros.  Because of this, to be at
 151:../target/stm32/malloc/mallocr.c ****     all usable, this code should be compiled using an optimizing compiler
 152:../target/stm32/malloc/mallocr.c ****     (for example gcc -O2) that can simplify expressions and control
 153:../target/stm32/malloc/mallocr.c ****     paths.
 154:../target/stm32/malloc/mallocr.c **** 
 155:../target/stm32/malloc/mallocr.c ****   __STD_C                  (default: derived from C compiler defines)
 156:../target/stm32/malloc/mallocr.c ****      Nonzero if using ANSI-standard C compiler, a C++ compiler, or
 157:../target/stm32/malloc/mallocr.c ****      a C compiler sufficiently close to ANSI to get away with it.
 158:../target/stm32/malloc/mallocr.c ****   DEBUG                    (default: NOT defined)
 159:../target/stm32/malloc/mallocr.c ****      Define to enable debugging. Adds fairly extensive assertion-based 
 160:../target/stm32/malloc/mallocr.c ****      checking to help track down memory errors, but noticeably slows down
 161:../target/stm32/malloc/mallocr.c ****      execution.
 162:../target/stm32/malloc/mallocr.c ****   SEPARATE_OBJECTS	   (default: NOT defined)
 163:../target/stm32/malloc/mallocr.c ****      Define this to compile into separate .o files.  You must then
 164:../target/stm32/malloc/mallocr.c ****      compile malloc.c several times, defining a DEFINE_* macro each
 165:../target/stm32/malloc/mallocr.c ****      time.  The list of DEFINE_* macros appears below.
 166:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK		   (default: NOT defined)
 167:../target/stm32/malloc/mallocr.c ****   MALLOC_UNLOCK		   (default: NOT defined)
 168:../target/stm32/malloc/mallocr.c ****      Define these to C expressions which are run to lock and unlock
 169:../target/stm32/malloc/mallocr.c ****      the malloc data structures.  Calls may be nested; that is,
 170:../target/stm32/malloc/mallocr.c ****      MALLOC_LOCK may be called more than once before the corresponding
 171:../target/stm32/malloc/mallocr.c ****      MALLOC_UNLOCK calls.  MALLOC_LOCK must avoid waiting for a lock
 172:../target/stm32/malloc/mallocr.c ****      that it already holds.
 173:../target/stm32/malloc/mallocr.c ****   MALLOC_ALIGNMENT          (default: NOT defined)
 174:../target/stm32/malloc/mallocr.c ****      Define this to 16 if you need 16 byte alignment instead of 8 byte alignment
 175:../target/stm32/malloc/mallocr.c ****      which is the normal default.
 176:../target/stm32/malloc/mallocr.c ****   REALLOC_ZERO_BYTES_FREES (default: NOT defined) 
 177:../target/stm32/malloc/mallocr.c ****      Define this if you think that realloc(p, 0) should be equivalent
 178:../target/stm32/malloc/mallocr.c ****      to free(p). Otherwise, since malloc returns a unique pointer for
 179:../target/stm32/malloc/mallocr.c ****      malloc(0), so does realloc(p, 0).
 180:../target/stm32/malloc/mallocr.c ****   HAVE_MEMCPY               (default: defined)
 181:../target/stm32/malloc/mallocr.c ****      Define if you are not otherwise using ANSI STD C, but still 
 182:../target/stm32/malloc/mallocr.c ****      have memcpy and memset in your C library and want to use them.
 183:../target/stm32/malloc/mallocr.c ****      Otherwise, simple internal versions are supplied.
 184:../target/stm32/malloc/mallocr.c ****   USE_MEMCPY               (default: 1 if HAVE_MEMCPY is defined, 0 otherwise)
 185:../target/stm32/malloc/mallocr.c ****      Define as 1 if you want the C library versions of memset and
 186:../target/stm32/malloc/mallocr.c ****      memcpy called in realloc and calloc (otherwise macro versions are used). 
 187:../target/stm32/malloc/mallocr.c ****      At least on some platforms, the simple macro versions usually
 188:../target/stm32/malloc/mallocr.c ****      outperform libc versions.
 189:../target/stm32/malloc/mallocr.c ****   HAVE_MMAP                 (default: defined as 1)
 190:../target/stm32/malloc/mallocr.c ****      Define to non-zero to optionally make malloc() use mmap() to
 191:../target/stm32/malloc/mallocr.c ****      allocate very large blocks.  
 192:../target/stm32/malloc/mallocr.c ****   HAVE_MREMAP                 (default: defined as 0 unless Linux libc set)
 193:../target/stm32/malloc/mallocr.c ****      Define to non-zero to optionally make realloc() use mremap() to
 194:../target/stm32/malloc/mallocr.c ****      reallocate very large blocks.  
 195:../target/stm32/malloc/mallocr.c ****   malloc_getpagesize        (default: derived from system #includes)
 196:../target/stm32/malloc/mallocr.c ****      Either a constant or routine call returning the system page size.
 197:../target/stm32/malloc/mallocr.c ****   HAVE_USR_INCLUDE_MALLOC_H (default: NOT defined) 
 198:../target/stm32/malloc/mallocr.c ****      Optionally define if you are on a system with a /usr/include/malloc.h
 199:../target/stm32/malloc/mallocr.c ****      that declares struct mallinfo. It is not at all necessary to
 200:../target/stm32/malloc/mallocr.c ****      define this even if you do, but will ensure consistency.
 201:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T           (default: size_t)
 202:../target/stm32/malloc/mallocr.c ****      Define to a 32-bit type (probably `unsigned int') if you are on a 
 203:../target/stm32/malloc/mallocr.c ****      64-bit machine, yet do not want or need to allow malloc requests of 
 204:../target/stm32/malloc/mallocr.c ****      greater than 2^31 to be handled. This saves space, especially for
 205:../target/stm32/malloc/mallocr.c ****      very small chunks.
 206:../target/stm32/malloc/mallocr.c ****   INTERNAL_LINUX_C_LIB      (default: NOT defined)
 207:../target/stm32/malloc/mallocr.c ****      Defined only when compiled as part of Linux libc.
 208:../target/stm32/malloc/mallocr.c ****      Also note that there is some odd internal name-mangling via defines
 209:../target/stm32/malloc/mallocr.c ****      (for example, internally, `malloc' is named `mALLOc') needed
 210:../target/stm32/malloc/mallocr.c ****      when compiling in this case. These look funny but don't otherwise
 211:../target/stm32/malloc/mallocr.c ****      affect anything.
 212:../target/stm32/malloc/mallocr.c ****   INTERNAL_NEWLIB	    (default: NOT defined)
 213:../target/stm32/malloc/mallocr.c ****      Defined only when compiled as part of the Cygnus newlib
 214:../target/stm32/malloc/mallocr.c ****      distribution.
 215:../target/stm32/malloc/mallocr.c ****   WIN32                     (default: undefined)
 216:../target/stm32/malloc/mallocr.c ****      Define this on MS win (95, nt) platforms to compile in sbrk emulation.
 217:../target/stm32/malloc/mallocr.c ****   LACKS_UNISTD_H            (default: undefined)
 218:../target/stm32/malloc/mallocr.c ****      Define this if your system does not have a <unistd.h>.
 219:../target/stm32/malloc/mallocr.c ****   MORECORE                  (default: sbrk)
 220:../target/stm32/malloc/mallocr.c ****      The name of the routine to call to obtain more memory from the system.
 221:../target/stm32/malloc/mallocr.c ****   MORECORE_FAILURE          (default: -1)
 222:../target/stm32/malloc/mallocr.c ****      The value returned upon failure of MORECORE.
 223:../target/stm32/malloc/mallocr.c ****   MORECORE_CLEARS           (default 1)
 224:../target/stm32/malloc/mallocr.c ****      True (1) if the routine mapped to MORECORE zeroes out memory (which
 225:../target/stm32/malloc/mallocr.c ****      holds for sbrk).
 226:../target/stm32/malloc/mallocr.c ****   DEFAULT_TRIM_THRESHOLD
 227:../target/stm32/malloc/mallocr.c ****   DEFAULT_TOP_PAD       
 228:../target/stm32/malloc/mallocr.c ****   DEFAULT_MMAP_THRESHOLD
 229:../target/stm32/malloc/mallocr.c ****   DEFAULT_MMAP_MAX      
 230:../target/stm32/malloc/mallocr.c ****      Default values of tunable parameters (described in detail below)
 231:../target/stm32/malloc/mallocr.c ****      controlling interaction with host system routines (sbrk, mmap, etc).
 232:../target/stm32/malloc/mallocr.c ****      These values may also be changed dynamically via mallopt(). The
 233:../target/stm32/malloc/mallocr.c ****      preset defaults are those that give best performance for typical
 234:../target/stm32/malloc/mallocr.c ****      programs/systems.
 235:../target/stm32/malloc/mallocr.c **** 
 236:../target/stm32/malloc/mallocr.c **** 
 237:../target/stm32/malloc/mallocr.c **** */
 238:../target/stm32/malloc/mallocr.c **** 
 239:../target/stm32/malloc/mallocr.c **** 
 240:../target/stm32/malloc/mallocr.c **** 
 241:../target/stm32/malloc/mallocr.c **** 
 242:../target/stm32/malloc/mallocr.c **** /* Preliminaries */
 243:../target/stm32/malloc/mallocr.c **** 
 244:../target/stm32/malloc/mallocr.c **** #ifndef __STD_C
 245:../target/stm32/malloc/mallocr.c **** #ifdef __STDC__
 246:../target/stm32/malloc/mallocr.c **** #define __STD_C     1
 247:../target/stm32/malloc/mallocr.c **** #else
 248:../target/stm32/malloc/mallocr.c **** #if __cplusplus
 249:../target/stm32/malloc/mallocr.c **** #define __STD_C     1
 250:../target/stm32/malloc/mallocr.c **** #else
 251:../target/stm32/malloc/mallocr.c **** #define __STD_C     0
 252:../target/stm32/malloc/mallocr.c **** #endif /*__cplusplus*/
 253:../target/stm32/malloc/mallocr.c **** #endif /*__STDC__*/
 254:../target/stm32/malloc/mallocr.c **** #endif /*__STD_C*/
 255:../target/stm32/malloc/mallocr.c **** 
 256:../target/stm32/malloc/mallocr.c **** #ifndef Void_t
 257:../target/stm32/malloc/mallocr.c **** #if __STD_C
 258:../target/stm32/malloc/mallocr.c **** #define Void_t      void
 259:../target/stm32/malloc/mallocr.c **** #else
 260:../target/stm32/malloc/mallocr.c **** #define Void_t      char
 261:../target/stm32/malloc/mallocr.c **** #endif
 262:../target/stm32/malloc/mallocr.c **** #endif /*Void_t*/
 263:../target/stm32/malloc/mallocr.c **** 
 264:../target/stm32/malloc/mallocr.c **** #if __STD_C
 265:../target/stm32/malloc/mallocr.c **** #include <stddef.h>   /* for size_t */
 266:../target/stm32/malloc/mallocr.c **** #else
 267:../target/stm32/malloc/mallocr.c **** #include <sys/types.h>
 268:../target/stm32/malloc/mallocr.c **** #endif
 269:../target/stm32/malloc/mallocr.c **** 
 270:../target/stm32/malloc/mallocr.c **** #ifdef __cplusplus
 271:../target/stm32/malloc/mallocr.c **** extern "C" {
 272:../target/stm32/malloc/mallocr.c **** #endif
 273:../target/stm32/malloc/mallocr.c **** 
 274:../target/stm32/malloc/mallocr.c **** #include <stdio.h>    /* needed for malloc_stats */
 275:../target/stm32/malloc/mallocr.c **** #include <limits.h>   /* needed for overflow checks */
 276:../target/stm32/malloc/mallocr.c **** #include <errno.h>    /* needed to set errno to ENOMEM */
 277:../target/stm32/malloc/mallocr.c **** 
 278:../target/stm32/malloc/mallocr.c **** #ifdef WIN32
 279:../target/stm32/malloc/mallocr.c **** #define WIN32_LEAN_AND_MEAN
 280:../target/stm32/malloc/mallocr.c **** #include <windows.h>
 281:../target/stm32/malloc/mallocr.c **** #endif
 282:../target/stm32/malloc/mallocr.c **** 
 283:../target/stm32/malloc/mallocr.c **** /*
 284:../target/stm32/malloc/mallocr.c ****   Compile-time options
 285:../target/stm32/malloc/mallocr.c **** */
 286:../target/stm32/malloc/mallocr.c **** 
 287:../target/stm32/malloc/mallocr.c **** 
 288:../target/stm32/malloc/mallocr.c **** /*
 289:../target/stm32/malloc/mallocr.c **** 
 290:../target/stm32/malloc/mallocr.c ****   Special defines for Cygnus newlib distribution.
 291:../target/stm32/malloc/mallocr.c **** 
 292:../target/stm32/malloc/mallocr.c ****  */
 293:../target/stm32/malloc/mallocr.c **** 
 294:../target/stm32/malloc/mallocr.c **** #ifdef INTERNAL_NEWLIB
 295:../target/stm32/malloc/mallocr.c **** 
 296:../target/stm32/malloc/mallocr.c **** #include <sys/config.h>
 297:../target/stm32/malloc/mallocr.c **** 
 298:../target/stm32/malloc/mallocr.c **** /*
 299:../target/stm32/malloc/mallocr.c ****   In newlib, all the publically visible routines take a reentrancy
 300:../target/stm32/malloc/mallocr.c ****   pointer.  We don't currently do anything much with it, but we do
 301:../target/stm32/malloc/mallocr.c ****   pass it to the lock routine.
 302:../target/stm32/malloc/mallocr.c ****  */
 303:../target/stm32/malloc/mallocr.c **** 
 304:../target/stm32/malloc/mallocr.c **** #include <reent.h>
 305:../target/stm32/malloc/mallocr.c **** 
 306:../target/stm32/malloc/mallocr.c **** #define POINTER_UINT unsigned _POINTER_INT
 307:../target/stm32/malloc/mallocr.c **** #define SEPARATE_OBJECTS
 308:../target/stm32/malloc/mallocr.c **** #define HAVE_MMAP 0
 309:../target/stm32/malloc/mallocr.c **** #define MORECORE(size) _sbrk_r(reent_ptr, (size))
 310:../target/stm32/malloc/mallocr.c **** #define MORECORE_CLEARS 0
 311:../target/stm32/malloc/mallocr.c **** #define MALLOC_LOCK __malloc_lock(reent_ptr)
 312:../target/stm32/malloc/mallocr.c **** #define MALLOC_UNLOCK __malloc_unlock(reent_ptr)
 313:../target/stm32/malloc/mallocr.c **** 
 314:../target/stm32/malloc/mallocr.c **** #ifdef __CYGWIN__
 315:../target/stm32/malloc/mallocr.c **** # undef _WIN32
 316:../target/stm32/malloc/mallocr.c **** # undef WIN32
 317:../target/stm32/malloc/mallocr.c **** #endif
 318:../target/stm32/malloc/mallocr.c **** 
 319:../target/stm32/malloc/mallocr.c **** #ifndef _WIN32
 320:../target/stm32/malloc/mallocr.c **** #ifdef SMALL_MEMORY
 321:../target/stm32/malloc/mallocr.c **** #define malloc_getpagesize (128)
 322:../target/stm32/malloc/mallocr.c **** #else
 323:../target/stm32/malloc/mallocr.c **** #define malloc_getpagesize (4096)
 324:../target/stm32/malloc/mallocr.c **** #endif
 325:../target/stm32/malloc/mallocr.c **** #endif
 326:../target/stm32/malloc/mallocr.c **** 
 327:../target/stm32/malloc/mallocr.c **** #if __STD_C
 328:../target/stm32/malloc/mallocr.c **** extern void __malloc_lock(struct _reent *);
 329:../target/stm32/malloc/mallocr.c **** extern void __malloc_unlock(struct _reent *);
 330:../target/stm32/malloc/mallocr.c **** #else
 331:../target/stm32/malloc/mallocr.c **** extern void __malloc_lock();
 332:../target/stm32/malloc/mallocr.c **** extern void __malloc_unlock();
 333:../target/stm32/malloc/mallocr.c **** #endif
 334:../target/stm32/malloc/mallocr.c **** 
 335:../target/stm32/malloc/mallocr.c **** #if __STD_C
 336:../target/stm32/malloc/mallocr.c **** #define RARG struct _reent *reent_ptr,
 337:../target/stm32/malloc/mallocr.c **** #define RONEARG struct _reent *reent_ptr
 338:../target/stm32/malloc/mallocr.c **** #else
 339:../target/stm32/malloc/mallocr.c **** #define RARG reent_ptr
 340:../target/stm32/malloc/mallocr.c **** #define RONEARG reent_ptr
 341:../target/stm32/malloc/mallocr.c **** #define RDECL struct _reent *reent_ptr;
 342:../target/stm32/malloc/mallocr.c **** #endif
 343:../target/stm32/malloc/mallocr.c **** 
 344:../target/stm32/malloc/mallocr.c **** #define RERRNO reent_ptr->_errno
 345:../target/stm32/malloc/mallocr.c **** #define RCALL reent_ptr,
 346:../target/stm32/malloc/mallocr.c **** #define RONECALL reent_ptr
 347:../target/stm32/malloc/mallocr.c **** 
 348:../target/stm32/malloc/mallocr.c **** #else /* ! INTERNAL_NEWLIB */
 349:../target/stm32/malloc/mallocr.c **** 
 350:../target/stm32/malloc/mallocr.c **** #define POINTER_UINT unsigned long
 351:../target/stm32/malloc/mallocr.c **** #define RARG
 352:../target/stm32/malloc/mallocr.c **** #define RONEARG
 353:../target/stm32/malloc/mallocr.c **** #define RDECL
 354:../target/stm32/malloc/mallocr.c **** #define RERRNO errno
 355:../target/stm32/malloc/mallocr.c **** #define RCALL
 356:../target/stm32/malloc/mallocr.c **** #define RONECALL
 357:../target/stm32/malloc/mallocr.c **** 
 358:../target/stm32/malloc/mallocr.c **** #endif /* ! INTERNAL_NEWLIB */
 359:../target/stm32/malloc/mallocr.c **** 
 360:../target/stm32/malloc/mallocr.c **** /*
 361:../target/stm32/malloc/mallocr.c ****     Debugging:
 362:../target/stm32/malloc/mallocr.c **** 
 363:../target/stm32/malloc/mallocr.c ****     Because freed chunks may be overwritten with link fields, this
 364:../target/stm32/malloc/mallocr.c ****     malloc will often die when freed memory is overwritten by user
 365:../target/stm32/malloc/mallocr.c ****     programs.  This can be very effective (albeit in an annoying way)
 366:../target/stm32/malloc/mallocr.c ****     in helping track down dangling pointers.
 367:../target/stm32/malloc/mallocr.c **** 
 368:../target/stm32/malloc/mallocr.c ****     If you compile with -DDEBUG, a number of assertion checks are
 369:../target/stm32/malloc/mallocr.c ****     enabled that will catch more memory errors. You probably won't be
 370:../target/stm32/malloc/mallocr.c ****     able to make much sense of the actual assertion errors, but they
 371:../target/stm32/malloc/mallocr.c ****     should help you locate incorrectly overwritten memory.  The
 372:../target/stm32/malloc/mallocr.c ****     checking is fairly extensive, and will slow down execution
 373:../target/stm32/malloc/mallocr.c ****     noticeably. Calling malloc_stats or mallinfo with DEBUG set will
 374:../target/stm32/malloc/mallocr.c ****     attempt to check every non-mmapped allocated and free chunk in the
 375:../target/stm32/malloc/mallocr.c ****     course of computing the summmaries. (By nature, mmapped regions
 376:../target/stm32/malloc/mallocr.c ****     cannot be checked very much automatically.)
 377:../target/stm32/malloc/mallocr.c **** 
 378:../target/stm32/malloc/mallocr.c ****     Setting DEBUG may also be helpful if you are trying to modify 
 379:../target/stm32/malloc/mallocr.c ****     this code. The assertions in the check routines spell out in more 
 380:../target/stm32/malloc/mallocr.c ****     detail the assumptions and invariants underlying the algorithms.
 381:../target/stm32/malloc/mallocr.c **** 
 382:../target/stm32/malloc/mallocr.c **** */
 383:../target/stm32/malloc/mallocr.c **** 
 384:../target/stm32/malloc/mallocr.c **** #if DEBUG 
 385:../target/stm32/malloc/mallocr.c **** #include <assert.h>
 386:../target/stm32/malloc/mallocr.c **** #else
 387:../target/stm32/malloc/mallocr.c **** #define assert(x) ((void)0)
 388:../target/stm32/malloc/mallocr.c **** #endif
 389:../target/stm32/malloc/mallocr.c **** 
 390:../target/stm32/malloc/mallocr.c **** 
 391:../target/stm32/malloc/mallocr.c **** /*
 392:../target/stm32/malloc/mallocr.c ****   SEPARATE_OBJECTS should be defined if you want each function to go
 393:../target/stm32/malloc/mallocr.c ****   into a separate .o file.  You must then compile malloc.c once per
 394:../target/stm32/malloc/mallocr.c ****   function, defining the appropriate DEFINE_ macro.  See below for the
 395:../target/stm32/malloc/mallocr.c ****   list of macros.
 396:../target/stm32/malloc/mallocr.c ****  */
 397:../target/stm32/malloc/mallocr.c **** 
 398:../target/stm32/malloc/mallocr.c **** #ifndef SEPARATE_OBJECTS
 399:../target/stm32/malloc/mallocr.c **** #define DEFINE_MALLOC
 400:../target/stm32/malloc/mallocr.c **** #define DEFINE_FREE
 401:../target/stm32/malloc/mallocr.c **** #define DEFINE_REALLOC
 402:../target/stm32/malloc/mallocr.c **** #define DEFINE_CALLOC
 403:../target/stm32/malloc/mallocr.c **** #define DEFINE_CFREE
 404:../target/stm32/malloc/mallocr.c **** #define DEFINE_MEMALIGN
 405:../target/stm32/malloc/mallocr.c **** #define DEFINE_VALLOC
 406:../target/stm32/malloc/mallocr.c **** #define DEFINE_PVALLOC
 407:../target/stm32/malloc/mallocr.c **** #define DEFINE_MALLINFO
 408:../target/stm32/malloc/mallocr.c **** #define DEFINE_MALLOC_STATS
 409:../target/stm32/malloc/mallocr.c **** #define DEFINE_MALLOC_USABLE_SIZE
 410:../target/stm32/malloc/mallocr.c **** #define DEFINE_MALLOPT
 411:../target/stm32/malloc/mallocr.c **** 
 412:../target/stm32/malloc/mallocr.c **** #define STATIC static
 413:../target/stm32/malloc/mallocr.c **** #else
 414:../target/stm32/malloc/mallocr.c **** #define STATIC
 415:../target/stm32/malloc/mallocr.c **** #endif
 416:../target/stm32/malloc/mallocr.c **** 
 417:../target/stm32/malloc/mallocr.c **** /*
 418:../target/stm32/malloc/mallocr.c ****    Define MALLOC_LOCK and MALLOC_UNLOCK to C expressions to run to
 419:../target/stm32/malloc/mallocr.c ****    lock and unlock the malloc data structures.  MALLOC_LOCK may be
 420:../target/stm32/malloc/mallocr.c ****    called recursively.
 421:../target/stm32/malloc/mallocr.c ****  */
 422:../target/stm32/malloc/mallocr.c **** 
 423:../target/stm32/malloc/mallocr.c **** #ifndef MALLOC_LOCK
 424:../target/stm32/malloc/mallocr.c **** #define MALLOC_LOCK
 425:../target/stm32/malloc/mallocr.c **** #endif
 426:../target/stm32/malloc/mallocr.c **** 
 427:../target/stm32/malloc/mallocr.c **** #ifndef MALLOC_UNLOCK
 428:../target/stm32/malloc/mallocr.c **** #define MALLOC_UNLOCK
 429:../target/stm32/malloc/mallocr.c **** #endif
 430:../target/stm32/malloc/mallocr.c **** 
 431:../target/stm32/malloc/mallocr.c **** /*
 432:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T is the word-size used for internal bookkeeping
 433:../target/stm32/malloc/mallocr.c ****   of chunk sizes. On a 64-bit machine, you can reduce malloc
 434:../target/stm32/malloc/mallocr.c ****   overhead by defining INTERNAL_SIZE_T to be a 32 bit `unsigned int'
 435:../target/stm32/malloc/mallocr.c ****   at the expense of not being able to handle requests greater than
 436:../target/stm32/malloc/mallocr.c ****   2^31. This limitation is hardly ever a concern; you are encouraged
 437:../target/stm32/malloc/mallocr.c ****   to set this. However, the default version is the same as size_t.
 438:../target/stm32/malloc/mallocr.c **** */
 439:../target/stm32/malloc/mallocr.c **** 
 440:../target/stm32/malloc/mallocr.c **** #ifndef INTERNAL_SIZE_T
 441:../target/stm32/malloc/mallocr.c **** #define INTERNAL_SIZE_T size_t
 442:../target/stm32/malloc/mallocr.c **** #endif
 443:../target/stm32/malloc/mallocr.c **** 
 444:../target/stm32/malloc/mallocr.c **** /*
 445:../target/stm32/malloc/mallocr.c ****   Following is needed on implementations whereby long > size_t.
 446:../target/stm32/malloc/mallocr.c ****   The problem is caused because the code performs subtractions of
 447:../target/stm32/malloc/mallocr.c ****   size_t values and stores the result in long values.  In the case
 448:../target/stm32/malloc/mallocr.c ****   where long > size_t and the first value is actually less than
 449:../target/stm32/malloc/mallocr.c ****   the second value, the resultant value is positive.  For example,
 450:../target/stm32/malloc/mallocr.c ****   (long)(x - y) where x = 0 and y is 1 ends up being 0x00000000FFFFFFFF
 451:../target/stm32/malloc/mallocr.c ****   which is 2*31 - 1 instead of 0xFFFFFFFFFFFFFFFF.  This is due to the
 452:../target/stm32/malloc/mallocr.c ****   fact that assignment from unsigned to signed won't sign extend.
 453:../target/stm32/malloc/mallocr.c **** */
 454:../target/stm32/malloc/mallocr.c **** 
 455:../target/stm32/malloc/mallocr.c **** #define long_sub_size_t(x, y)				\
 456:../target/stm32/malloc/mallocr.c ****   (sizeof (long) > sizeof (INTERNAL_SIZE_T) && x < y	\
 457:../target/stm32/malloc/mallocr.c ****    ? -(long) (y - x)					\
 458:../target/stm32/malloc/mallocr.c ****    : (long) (x - y))
 459:../target/stm32/malloc/mallocr.c **** 
 460:../target/stm32/malloc/mallocr.c **** /*
 461:../target/stm32/malloc/mallocr.c ****   REALLOC_ZERO_BYTES_FREES should be set if a call to
 462:../target/stm32/malloc/mallocr.c ****   realloc with zero bytes should be the same as a call to free.
 463:../target/stm32/malloc/mallocr.c ****   Some people think it should. Otherwise, since this malloc
 464:../target/stm32/malloc/mallocr.c ****   returns a unique pointer for malloc(0), so does realloc(p, 0). 
 465:../target/stm32/malloc/mallocr.c **** */
 466:../target/stm32/malloc/mallocr.c **** 
 467:../target/stm32/malloc/mallocr.c **** 
 468:../target/stm32/malloc/mallocr.c **** /*   #define REALLOC_ZERO_BYTES_FREES */
 469:../target/stm32/malloc/mallocr.c **** 
 470:../target/stm32/malloc/mallocr.c **** 
 471:../target/stm32/malloc/mallocr.c **** /* 
 472:../target/stm32/malloc/mallocr.c ****   WIN32 causes an emulation of sbrk to be compiled in
 473:../target/stm32/malloc/mallocr.c ****   mmap-based options are not currently supported in WIN32.
 474:../target/stm32/malloc/mallocr.c **** */
 475:../target/stm32/malloc/mallocr.c **** 
 476:../target/stm32/malloc/mallocr.c **** /* #define WIN32 */
 477:../target/stm32/malloc/mallocr.c **** #ifdef WIN32
 478:../target/stm32/malloc/mallocr.c **** #define MORECORE wsbrk
 479:../target/stm32/malloc/mallocr.c **** #define HAVE_MMAP 0
 480:../target/stm32/malloc/mallocr.c **** #endif
 481:../target/stm32/malloc/mallocr.c **** 
 482:../target/stm32/malloc/mallocr.c **** 
 483:../target/stm32/malloc/mallocr.c **** /*
 484:../target/stm32/malloc/mallocr.c ****   HAVE_MEMCPY should be defined if you are not otherwise using
 485:../target/stm32/malloc/mallocr.c ****   ANSI STD C, but still have memcpy and memset in your C library
 486:../target/stm32/malloc/mallocr.c ****   and want to use them in calloc and realloc. Otherwise simple
 487:../target/stm32/malloc/mallocr.c ****   macro versions are defined here.
 488:../target/stm32/malloc/mallocr.c **** 
 489:../target/stm32/malloc/mallocr.c ****   USE_MEMCPY should be defined as 1 if you actually want to
 490:../target/stm32/malloc/mallocr.c ****   have memset and memcpy called. People report that the macro
 491:../target/stm32/malloc/mallocr.c ****   versions are often enough faster than libc versions on many
 492:../target/stm32/malloc/mallocr.c ****   systems that it is better to use them. 
 493:../target/stm32/malloc/mallocr.c **** 
 494:../target/stm32/malloc/mallocr.c **** */
 495:../target/stm32/malloc/mallocr.c **** 
 496:../target/stm32/malloc/mallocr.c **** #define HAVE_MEMCPY 
 497:../target/stm32/malloc/mallocr.c **** 
 498:../target/stm32/malloc/mallocr.c **** /* Although the original macro is called USE_MEMCPY, newlib actually
 499:../target/stm32/malloc/mallocr.c ****    uses memmove to handle cases whereby a platform's memcpy implementation
 500:../target/stm32/malloc/mallocr.c ****    copies backwards and thus destructive overlap may occur in realloc
 501:../target/stm32/malloc/mallocr.c ****    whereby we are reclaiming free memory prior to the old allocation.  */
 502:../target/stm32/malloc/mallocr.c **** #ifndef USE_MEMCPY
 503:../target/stm32/malloc/mallocr.c **** #ifdef HAVE_MEMCPY
 504:../target/stm32/malloc/mallocr.c **** #define USE_MEMCPY 1
 505:../target/stm32/malloc/mallocr.c **** #else
 506:../target/stm32/malloc/mallocr.c **** #define USE_MEMCPY 0
 507:../target/stm32/malloc/mallocr.c **** #endif
 508:../target/stm32/malloc/mallocr.c **** #endif
 509:../target/stm32/malloc/mallocr.c **** 
 510:../target/stm32/malloc/mallocr.c **** #if (__STD_C || defined(HAVE_MEMCPY)) 
 511:../target/stm32/malloc/mallocr.c **** 
 512:../target/stm32/malloc/mallocr.c **** #if __STD_C
 513:../target/stm32/malloc/mallocr.c **** void* memset(void*, int, size_t);
 514:../target/stm32/malloc/mallocr.c **** void* memcpy(void*, const void*, size_t);
 515:../target/stm32/malloc/mallocr.c **** void* memmove(void*, const void*, size_t);
 516:../target/stm32/malloc/mallocr.c **** #else
 517:../target/stm32/malloc/mallocr.c **** Void_t* memset();
 518:../target/stm32/malloc/mallocr.c **** Void_t* memcpy();
 519:../target/stm32/malloc/mallocr.c **** Void_t* memmove();
 520:../target/stm32/malloc/mallocr.c **** #endif
 521:../target/stm32/malloc/mallocr.c **** #endif
 522:../target/stm32/malloc/mallocr.c **** 
 523:../target/stm32/malloc/mallocr.c **** #if USE_MEMCPY
 524:../target/stm32/malloc/mallocr.c **** 
 525:../target/stm32/malloc/mallocr.c **** /* The following macros are only invoked with (2n+1)-multiples of
 526:../target/stm32/malloc/mallocr.c ****    INTERNAL_SIZE_T units, with a positive integer n. This is exploited
 527:../target/stm32/malloc/mallocr.c ****    for fast inline execution when n is small. */
 528:../target/stm32/malloc/mallocr.c **** 
 529:../target/stm32/malloc/mallocr.c **** #define MALLOC_ZERO(charp, nbytes)                                            \
 530:../target/stm32/malloc/mallocr.c **** do {                                                                          \
 531:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T mzsz = (nbytes);                                            \
 532:../target/stm32/malloc/mallocr.c ****   if(mzsz <= 9*sizeof(mzsz)) {                                                \
 533:../target/stm32/malloc/mallocr.c ****     INTERNAL_SIZE_T* mz = (INTERNAL_SIZE_T*) (charp);                         \
 534:../target/stm32/malloc/mallocr.c ****     if(mzsz >= 5*sizeof(mzsz)) {     *mz++ = 0;                               \
 535:../target/stm32/malloc/mallocr.c ****                                      *mz++ = 0;                               \
 536:../target/stm32/malloc/mallocr.c ****       if(mzsz >= 7*sizeof(mzsz)) {   *mz++ = 0;                               \
 537:../target/stm32/malloc/mallocr.c ****                                      *mz++ = 0;                               \
 538:../target/stm32/malloc/mallocr.c ****         if(mzsz >= 9*sizeof(mzsz)) { *mz++ = 0;                               \
 539:../target/stm32/malloc/mallocr.c ****                                      *mz++ = 0; }}}                           \
 540:../target/stm32/malloc/mallocr.c ****                                      *mz++ = 0;                               \
 541:../target/stm32/malloc/mallocr.c ****                                      *mz++ = 0;                               \
 542:../target/stm32/malloc/mallocr.c ****                                      *mz   = 0;                               \
 543:../target/stm32/malloc/mallocr.c ****   } else memset((charp), 0, mzsz);                                            \
 544:../target/stm32/malloc/mallocr.c **** } while(0)
 545:../target/stm32/malloc/mallocr.c **** 
 546:../target/stm32/malloc/mallocr.c **** #define MALLOC_COPY(dest,src,nbytes)                                          \
 547:../target/stm32/malloc/mallocr.c **** do {                                                                          \
 548:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T mcsz = (nbytes);                                            \
 549:../target/stm32/malloc/mallocr.c ****   if(mcsz <= 9*sizeof(mcsz)) {                                                \
 550:../target/stm32/malloc/mallocr.c ****     INTERNAL_SIZE_T* mcsrc = (INTERNAL_SIZE_T*) (src);                        \
 551:../target/stm32/malloc/mallocr.c ****     INTERNAL_SIZE_T* mcdst = (INTERNAL_SIZE_T*) (dest);                       \
 552:../target/stm32/malloc/mallocr.c ****     if(mcsz >= 5*sizeof(mcsz)) {     *mcdst++ = *mcsrc++;                     \
 553:../target/stm32/malloc/mallocr.c ****                                      *mcdst++ = *mcsrc++;                     \
 554:../target/stm32/malloc/mallocr.c ****       if(mcsz >= 7*sizeof(mcsz)) {   *mcdst++ = *mcsrc++;                     \
 555:../target/stm32/malloc/mallocr.c ****                                      *mcdst++ = *mcsrc++;                     \
 556:../target/stm32/malloc/mallocr.c ****         if(mcsz >= 9*sizeof(mcsz)) { *mcdst++ = *mcsrc++;                     \
 557:../target/stm32/malloc/mallocr.c ****                                      *mcdst++ = *mcsrc++; }}}                 \
 558:../target/stm32/malloc/mallocr.c ****                                      *mcdst++ = *mcsrc++;                     \
 559:../target/stm32/malloc/mallocr.c ****                                      *mcdst++ = *mcsrc++;                     \
 560:../target/stm32/malloc/mallocr.c ****                                      *mcdst   = *mcsrc  ;                     \
 561:../target/stm32/malloc/mallocr.c ****   } else memmove(dest, src, mcsz);                                             \
 562:../target/stm32/malloc/mallocr.c **** } while(0)
 563:../target/stm32/malloc/mallocr.c **** 
 564:../target/stm32/malloc/mallocr.c **** #else /* !USE_MEMCPY */
 565:../target/stm32/malloc/mallocr.c **** 
 566:../target/stm32/malloc/mallocr.c **** /* Use Duff's device for good zeroing/copying performance. */
 567:../target/stm32/malloc/mallocr.c **** 
 568:../target/stm32/malloc/mallocr.c **** #define MALLOC_ZERO(charp, nbytes)                                            \
 569:../target/stm32/malloc/mallocr.c **** do {                                                                          \
 570:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T* mzp = (INTERNAL_SIZE_T*)(charp);                           \
 571:../target/stm32/malloc/mallocr.c ****   long mctmp = (nbytes)/sizeof(INTERNAL_SIZE_T), mcn;                         \
 572:../target/stm32/malloc/mallocr.c ****   if (mctmp < 8) mcn = 0; else { mcn = (mctmp-1)/8; mctmp %= 8; }             \
 573:../target/stm32/malloc/mallocr.c ****   switch (mctmp) {                                                            \
 574:../target/stm32/malloc/mallocr.c ****     case 0: for(;;) { *mzp++ = 0;                                             \
 575:../target/stm32/malloc/mallocr.c ****     case 7:           *mzp++ = 0;                                             \
 576:../target/stm32/malloc/mallocr.c ****     case 6:           *mzp++ = 0;                                             \
 577:../target/stm32/malloc/mallocr.c ****     case 5:           *mzp++ = 0;                                             \
 578:../target/stm32/malloc/mallocr.c ****     case 4:           *mzp++ = 0;                                             \
 579:../target/stm32/malloc/mallocr.c ****     case 3:           *mzp++ = 0;                                             \
 580:../target/stm32/malloc/mallocr.c ****     case 2:           *mzp++ = 0;                                             \
 581:../target/stm32/malloc/mallocr.c ****     case 1:           *mzp++ = 0; if(mcn <= 0) break; mcn--; }                \
 582:../target/stm32/malloc/mallocr.c ****   }                                                                           \
 583:../target/stm32/malloc/mallocr.c **** } while(0)
 584:../target/stm32/malloc/mallocr.c **** 
 585:../target/stm32/malloc/mallocr.c **** #define MALLOC_COPY(dest,src,nbytes)                                          \
 586:../target/stm32/malloc/mallocr.c **** do {                                                                          \
 587:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T* mcsrc = (INTERNAL_SIZE_T*) src;                            \
 588:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T* mcdst = (INTERNAL_SIZE_T*) dest;                           \
 589:../target/stm32/malloc/mallocr.c ****   long mctmp = (nbytes)/sizeof(INTERNAL_SIZE_T), mcn;                         \
 590:../target/stm32/malloc/mallocr.c ****   if (mctmp < 8) mcn = 0; else { mcn = (mctmp-1)/8; mctmp %= 8; }             \
 591:../target/stm32/malloc/mallocr.c ****   switch (mctmp) {                                                            \
 592:../target/stm32/malloc/mallocr.c ****     case 0: for(;;) { *mcdst++ = *mcsrc++;                                    \
 593:../target/stm32/malloc/mallocr.c ****     case 7:           *mcdst++ = *mcsrc++;                                    \
 594:../target/stm32/malloc/mallocr.c ****     case 6:           *mcdst++ = *mcsrc++;                                    \
 595:../target/stm32/malloc/mallocr.c ****     case 5:           *mcdst++ = *mcsrc++;                                    \
 596:../target/stm32/malloc/mallocr.c ****     case 4:           *mcdst++ = *mcsrc++;                                    \
 597:../target/stm32/malloc/mallocr.c ****     case 3:           *mcdst++ = *mcsrc++;                                    \
 598:../target/stm32/malloc/mallocr.c ****     case 2:           *mcdst++ = *mcsrc++;                                    \
 599:../target/stm32/malloc/mallocr.c ****     case 1:           *mcdst++ = *mcsrc++; if(mcn <= 0) break; mcn--; }       \
 600:../target/stm32/malloc/mallocr.c ****   }                                                                           \
 601:../target/stm32/malloc/mallocr.c **** } while(0)
 602:../target/stm32/malloc/mallocr.c **** 
 603:../target/stm32/malloc/mallocr.c **** #endif
 604:../target/stm32/malloc/mallocr.c **** 
 605:../target/stm32/malloc/mallocr.c **** 
 606:../target/stm32/malloc/mallocr.c **** /*
 607:../target/stm32/malloc/mallocr.c ****   Define HAVE_MMAP to optionally make malloc() use mmap() to
 608:../target/stm32/malloc/mallocr.c ****   allocate very large blocks.  These will be returned to the
 609:../target/stm32/malloc/mallocr.c ****   operating system immediately after a free().
 610:../target/stm32/malloc/mallocr.c **** */
 611:../target/stm32/malloc/mallocr.c **** 
 612:../target/stm32/malloc/mallocr.c **** #ifndef HAVE_MMAP
 613:../target/stm32/malloc/mallocr.c **** #define HAVE_MMAP 1
 614:../target/stm32/malloc/mallocr.c **** #endif
 615:../target/stm32/malloc/mallocr.c **** 
 616:../target/stm32/malloc/mallocr.c **** /*
 617:../target/stm32/malloc/mallocr.c ****   Define HAVE_MREMAP to make realloc() use mremap() to re-allocate
 618:../target/stm32/malloc/mallocr.c ****   large blocks.  This is currently only possible on Linux with
 619:../target/stm32/malloc/mallocr.c ****   kernel versions newer than 1.3.77.
 620:../target/stm32/malloc/mallocr.c **** */
 621:../target/stm32/malloc/mallocr.c **** 
 622:../target/stm32/malloc/mallocr.c **** #ifndef HAVE_MREMAP
 623:../target/stm32/malloc/mallocr.c **** #ifdef INTERNAL_LINUX_C_LIB
 624:../target/stm32/malloc/mallocr.c **** #define HAVE_MREMAP 1
 625:../target/stm32/malloc/mallocr.c **** #else
 626:../target/stm32/malloc/mallocr.c **** #define HAVE_MREMAP 0
 627:../target/stm32/malloc/mallocr.c **** #endif
 628:../target/stm32/malloc/mallocr.c **** #endif
 629:../target/stm32/malloc/mallocr.c **** 
 630:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
 631:../target/stm32/malloc/mallocr.c **** 
 632:../target/stm32/malloc/mallocr.c **** #include <unistd.h>
 633:../target/stm32/malloc/mallocr.c **** #include <fcntl.h>
 634:../target/stm32/malloc/mallocr.c **** #include <sys/mman.h>
 635:../target/stm32/malloc/mallocr.c **** 
 636:../target/stm32/malloc/mallocr.c **** #if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)
 637:../target/stm32/malloc/mallocr.c **** #define MAP_ANONYMOUS MAP_ANON
 638:../target/stm32/malloc/mallocr.c **** #endif
 639:../target/stm32/malloc/mallocr.c **** 
 640:../target/stm32/malloc/mallocr.c **** #endif /* HAVE_MMAP */
 641:../target/stm32/malloc/mallocr.c **** 
 642:../target/stm32/malloc/mallocr.c **** /*
 643:../target/stm32/malloc/mallocr.c ****   Access to system page size. To the extent possible, this malloc
 644:../target/stm32/malloc/mallocr.c ****   manages memory from the system in page-size units.
 645:../target/stm32/malloc/mallocr.c ****   
 646:../target/stm32/malloc/mallocr.c ****   The following mechanics for getpagesize were adapted from 
 647:../target/stm32/malloc/mallocr.c ****   bsd/gnu getpagesize.h 
 648:../target/stm32/malloc/mallocr.c **** */
 649:../target/stm32/malloc/mallocr.c **** 
 650:../target/stm32/malloc/mallocr.c **** #ifndef LACKS_UNISTD_H
 651:../target/stm32/malloc/mallocr.c **** #  include <unistd.h>
 652:../target/stm32/malloc/mallocr.c **** #endif
 653:../target/stm32/malloc/mallocr.c **** 
 654:../target/stm32/malloc/mallocr.c **** #ifndef malloc_getpagesize
 655:../target/stm32/malloc/mallocr.c **** #  ifdef _SC_PAGESIZE         /* some SVR4 systems omit an underscore */
 656:../target/stm32/malloc/mallocr.c **** #    ifndef _SC_PAGE_SIZE
 657:../target/stm32/malloc/mallocr.c **** #      define _SC_PAGE_SIZE _SC_PAGESIZE
 658:../target/stm32/malloc/mallocr.c **** #    endif
 659:../target/stm32/malloc/mallocr.c **** #  endif
 660:../target/stm32/malloc/mallocr.c **** #  ifdef _SC_PAGE_SIZE
 661:../target/stm32/malloc/mallocr.c **** #    define malloc_getpagesize sysconf(_SC_PAGE_SIZE)
 662:../target/stm32/malloc/mallocr.c **** #  else
 663:../target/stm32/malloc/mallocr.c **** #    if defined(BSD) || defined(DGUX) || defined(HAVE_GETPAGESIZE)
 664:../target/stm32/malloc/mallocr.c ****        extern size_t getpagesize();
 665:../target/stm32/malloc/mallocr.c **** #      define malloc_getpagesize getpagesize()
 666:../target/stm32/malloc/mallocr.c **** #    else
 667:../target/stm32/malloc/mallocr.c **** #      include <sys/param.h>
 668:../target/stm32/malloc/mallocr.c **** #      ifdef EXEC_PAGESIZE
 669:../target/stm32/malloc/mallocr.c **** #        define malloc_getpagesize EXEC_PAGESIZE
 670:../target/stm32/malloc/mallocr.c **** #      else
 671:../target/stm32/malloc/mallocr.c **** #        ifdef NBPG
 672:../target/stm32/malloc/mallocr.c **** #          ifndef CLSIZE
 673:../target/stm32/malloc/mallocr.c **** #            define malloc_getpagesize NBPG
 674:../target/stm32/malloc/mallocr.c **** #          else
 675:../target/stm32/malloc/mallocr.c **** #            define malloc_getpagesize (NBPG * CLSIZE)
 676:../target/stm32/malloc/mallocr.c **** #          endif
 677:../target/stm32/malloc/mallocr.c **** #        else 
 678:../target/stm32/malloc/mallocr.c **** #          ifdef NBPC
 679:../target/stm32/malloc/mallocr.c **** #            define malloc_getpagesize NBPC
 680:../target/stm32/malloc/mallocr.c **** #          else
 681:../target/stm32/malloc/mallocr.c **** #            ifdef PAGESIZE
 682:../target/stm32/malloc/mallocr.c **** #              define malloc_getpagesize PAGESIZE
 683:../target/stm32/malloc/mallocr.c **** #            else
 684:../target/stm32/malloc/mallocr.c **** #              define malloc_getpagesize (4096) /* just guess */
 685:../target/stm32/malloc/mallocr.c **** #            endif
 686:../target/stm32/malloc/mallocr.c **** #          endif
 687:../target/stm32/malloc/mallocr.c **** #        endif 
 688:../target/stm32/malloc/mallocr.c **** #      endif
 689:../target/stm32/malloc/mallocr.c **** #    endif 
 690:../target/stm32/malloc/mallocr.c **** #  endif
 691:../target/stm32/malloc/mallocr.c **** #endif
 692:../target/stm32/malloc/mallocr.c **** 
 693:../target/stm32/malloc/mallocr.c **** 
 694:../target/stm32/malloc/mallocr.c **** 
 695:../target/stm32/malloc/mallocr.c **** /*
 696:../target/stm32/malloc/mallocr.c **** 
 697:../target/stm32/malloc/mallocr.c ****   This version of malloc supports the standard SVID/XPG mallinfo
 698:../target/stm32/malloc/mallocr.c ****   routine that returns a struct containing the same kind of
 699:../target/stm32/malloc/mallocr.c ****   information you can get from malloc_stats. It should work on
 700:../target/stm32/malloc/mallocr.c ****   any SVID/XPG compliant system that has a /usr/include/malloc.h
 701:../target/stm32/malloc/mallocr.c ****   defining struct mallinfo. (If you'd like to install such a thing
 702:../target/stm32/malloc/mallocr.c ****   yourself, cut out the preliminary declarations as described above
 703:../target/stm32/malloc/mallocr.c ****   and below and save them in a malloc.h file. But there's no
 704:../target/stm32/malloc/mallocr.c ****   compelling reason to bother to do this.)
 705:../target/stm32/malloc/mallocr.c **** 
 706:../target/stm32/malloc/mallocr.c ****   The main declaration needed is the mallinfo struct that is returned
 707:../target/stm32/malloc/mallocr.c ****   (by-copy) by mallinfo().  The SVID/XPG malloinfo struct contains a
 708:../target/stm32/malloc/mallocr.c ****   bunch of fields, most of which are not even meaningful in this
 709:../target/stm32/malloc/mallocr.c ****   version of malloc. Some of these fields are are instead filled by
 710:../target/stm32/malloc/mallocr.c ****   mallinfo() with other numbers that might possibly be of interest.
 711:../target/stm32/malloc/mallocr.c **** 
 712:../target/stm32/malloc/mallocr.c ****   HAVE_USR_INCLUDE_MALLOC_H should be set if you have a
 713:../target/stm32/malloc/mallocr.c ****   /usr/include/malloc.h file that includes a declaration of struct
 714:../target/stm32/malloc/mallocr.c ****   mallinfo.  If so, it is included; else an SVID2/XPG2 compliant
 715:../target/stm32/malloc/mallocr.c ****   version is declared below.  These must be precisely the same for
 716:../target/stm32/malloc/mallocr.c ****   mallinfo() to work.
 717:../target/stm32/malloc/mallocr.c **** 
 718:../target/stm32/malloc/mallocr.c **** */
 719:../target/stm32/malloc/mallocr.c **** 
 720:../target/stm32/malloc/mallocr.c **** /* #define HAVE_USR_INCLUDE_MALLOC_H */
 721:../target/stm32/malloc/mallocr.c **** 
 722:../target/stm32/malloc/mallocr.c **** #if HAVE_USR_INCLUDE_MALLOC_H
 723:../target/stm32/malloc/mallocr.c **** #include "/usr/include/malloc.h"
 724:../target/stm32/malloc/mallocr.c **** #else
 725:../target/stm32/malloc/mallocr.c **** 
 726:../target/stm32/malloc/mallocr.c **** /* SVID2/XPG mallinfo structure */
 727:../target/stm32/malloc/mallocr.c **** 
 728:../target/stm32/malloc/mallocr.c **** struct mallinfo {
 729:../target/stm32/malloc/mallocr.c ****   int arena;    /* total space allocated from system */
 730:../target/stm32/malloc/mallocr.c ****   int ordblks;  /* number of non-inuse chunks */
 731:../target/stm32/malloc/mallocr.c ****   int smblks;   /* unused -- always zero */
 732:../target/stm32/malloc/mallocr.c ****   int hblks;    /* number of mmapped regions */
 733:../target/stm32/malloc/mallocr.c ****   int hblkhd;   /* total space in mmapped regions */
 734:../target/stm32/malloc/mallocr.c ****   int usmblks;  /* unused -- always zero */
 735:../target/stm32/malloc/mallocr.c ****   int fsmblks;  /* unused -- always zero */
 736:../target/stm32/malloc/mallocr.c ****   int uordblks; /* total allocated space */
 737:../target/stm32/malloc/mallocr.c ****   int fordblks; /* total non-inuse space */
 738:../target/stm32/malloc/mallocr.c ****   int keepcost; /* top-most, releasable (via malloc_trim) space */
 739:../target/stm32/malloc/mallocr.c **** };	
 740:../target/stm32/malloc/mallocr.c **** 
 741:../target/stm32/malloc/mallocr.c **** /* SVID2/XPG mallopt options */
 742:../target/stm32/malloc/mallocr.c **** 
 743:../target/stm32/malloc/mallocr.c **** #define M_MXFAST  1    /* UNUSED in this malloc */
 744:../target/stm32/malloc/mallocr.c **** #define M_NLBLKS  2    /* UNUSED in this malloc */
 745:../target/stm32/malloc/mallocr.c **** #define M_GRAIN   3    /* UNUSED in this malloc */
 746:../target/stm32/malloc/mallocr.c **** #define M_KEEP    4    /* UNUSED in this malloc */
 747:../target/stm32/malloc/mallocr.c **** 
 748:../target/stm32/malloc/mallocr.c **** #endif
 749:../target/stm32/malloc/mallocr.c **** 
 750:../target/stm32/malloc/mallocr.c **** /* mallopt options that actually do something */
 751:../target/stm32/malloc/mallocr.c **** 
 752:../target/stm32/malloc/mallocr.c **** #define M_TRIM_THRESHOLD    -1
 753:../target/stm32/malloc/mallocr.c **** #define M_TOP_PAD           -2
 754:../target/stm32/malloc/mallocr.c **** #define M_MMAP_THRESHOLD    -3
 755:../target/stm32/malloc/mallocr.c **** #define M_MMAP_MAX          -4
 756:../target/stm32/malloc/mallocr.c **** 
 757:../target/stm32/malloc/mallocr.c **** 
 758:../target/stm32/malloc/mallocr.c **** 
 759:../target/stm32/malloc/mallocr.c **** #ifndef DEFAULT_TRIM_THRESHOLD
 760:../target/stm32/malloc/mallocr.c **** #define DEFAULT_TRIM_THRESHOLD (128L * 1024L)
 761:../target/stm32/malloc/mallocr.c **** #endif
 762:../target/stm32/malloc/mallocr.c **** 
 763:../target/stm32/malloc/mallocr.c **** /*
 764:../target/stm32/malloc/mallocr.c ****     M_TRIM_THRESHOLD is the maximum amount of unused top-most memory 
 765:../target/stm32/malloc/mallocr.c ****       to keep before releasing via malloc_trim in free().
 766:../target/stm32/malloc/mallocr.c **** 
 767:../target/stm32/malloc/mallocr.c ****       Automatic trimming is mainly useful in long-lived programs.
 768:../target/stm32/malloc/mallocr.c ****       Because trimming via sbrk can be slow on some systems, and can
 769:../target/stm32/malloc/mallocr.c ****       sometimes be wasteful (in cases where programs immediately
 770:../target/stm32/malloc/mallocr.c ****       afterward allocate more large chunks) the value should be high
 771:../target/stm32/malloc/mallocr.c ****       enough so that your overall system performance would improve by
 772:../target/stm32/malloc/mallocr.c ****       releasing.  
 773:../target/stm32/malloc/mallocr.c **** 
 774:../target/stm32/malloc/mallocr.c ****       The trim threshold and the mmap control parameters (see below)
 775:../target/stm32/malloc/mallocr.c ****       can be traded off with one another. Trimming and mmapping are
 776:../target/stm32/malloc/mallocr.c ****       two different ways of releasing unused memory back to the
 777:../target/stm32/malloc/mallocr.c ****       system. Between these two, it is often possible to keep
 778:../target/stm32/malloc/mallocr.c ****       system-level demands of a long-lived program down to a bare
 779:../target/stm32/malloc/mallocr.c ****       minimum. For example, in one test suite of sessions measuring
 780:../target/stm32/malloc/mallocr.c ****       the XF86 X server on Linux, using a trim threshold of 128K and a
 781:../target/stm32/malloc/mallocr.c ****       mmap threshold of 192K led to near-minimal long term resource
 782:../target/stm32/malloc/mallocr.c ****       consumption.  
 783:../target/stm32/malloc/mallocr.c **** 
 784:../target/stm32/malloc/mallocr.c ****       If you are using this malloc in a long-lived program, it should
 785:../target/stm32/malloc/mallocr.c ****       pay to experiment with these values.  As a rough guide, you
 786:../target/stm32/malloc/mallocr.c ****       might set to a value close to the average size of a process
 787:../target/stm32/malloc/mallocr.c ****       (program) running on your system.  Releasing this much memory
 788:../target/stm32/malloc/mallocr.c ****       would allow such a process to run in memory.  Generally, it's
 789:../target/stm32/malloc/mallocr.c ****       worth it to tune for trimming rather tham memory mapping when a
 790:../target/stm32/malloc/mallocr.c ****       program undergoes phases where several large chunks are
 791:../target/stm32/malloc/mallocr.c ****       allocated and released in ways that can reuse each other's
 792:../target/stm32/malloc/mallocr.c ****       storage, perhaps mixed with phases where there are no such
 793:../target/stm32/malloc/mallocr.c ****       chunks at all.  And in well-behaved long-lived programs,
 794:../target/stm32/malloc/mallocr.c ****       controlling release of large blocks via trimming versus mapping
 795:../target/stm32/malloc/mallocr.c ****       is usually faster.
 796:../target/stm32/malloc/mallocr.c **** 
 797:../target/stm32/malloc/mallocr.c ****       However, in most programs, these parameters serve mainly as
 798:../target/stm32/malloc/mallocr.c ****       protection against the system-level effects of carrying around
 799:../target/stm32/malloc/mallocr.c ****       massive amounts of unneeded memory. Since frequent calls to
 800:../target/stm32/malloc/mallocr.c ****       sbrk, mmap, and munmap otherwise degrade performance, the default
 801:../target/stm32/malloc/mallocr.c ****       parameters are set to relatively high values that serve only as
 802:../target/stm32/malloc/mallocr.c ****       safeguards.
 803:../target/stm32/malloc/mallocr.c **** 
 804:../target/stm32/malloc/mallocr.c ****       The default trim value is high enough to cause trimming only in
 805:../target/stm32/malloc/mallocr.c ****       fairly extreme (by current memory consumption standards) cases.
 806:../target/stm32/malloc/mallocr.c ****       It must be greater than page size to have any useful effect.  To
 807:../target/stm32/malloc/mallocr.c ****       disable trimming completely, you can set to (unsigned long)(-1);
 808:../target/stm32/malloc/mallocr.c **** 
 809:../target/stm32/malloc/mallocr.c **** 
 810:../target/stm32/malloc/mallocr.c **** */
 811:../target/stm32/malloc/mallocr.c **** 
 812:../target/stm32/malloc/mallocr.c **** 
 813:../target/stm32/malloc/mallocr.c **** #ifndef DEFAULT_TOP_PAD
 814:../target/stm32/malloc/mallocr.c **** #define DEFAULT_TOP_PAD        (0)
 815:../target/stm32/malloc/mallocr.c **** #endif
 816:../target/stm32/malloc/mallocr.c **** 
 817:../target/stm32/malloc/mallocr.c **** /*
 818:../target/stm32/malloc/mallocr.c ****     M_TOP_PAD is the amount of extra `padding' space to allocate or 
 819:../target/stm32/malloc/mallocr.c ****       retain whenever sbrk is called. It is used in two ways internally:
 820:../target/stm32/malloc/mallocr.c **** 
 821:../target/stm32/malloc/mallocr.c ****       * When sbrk is called to extend the top of the arena to satisfy
 822:../target/stm32/malloc/mallocr.c ****         a new malloc request, this much padding is added to the sbrk
 823:../target/stm32/malloc/mallocr.c ****         request.
 824:../target/stm32/malloc/mallocr.c **** 
 825:../target/stm32/malloc/mallocr.c ****       * When malloc_trim is called automatically from free(),
 826:../target/stm32/malloc/mallocr.c ****         it is used as the `pad' argument.
 827:../target/stm32/malloc/mallocr.c **** 
 828:../target/stm32/malloc/mallocr.c ****       In both cases, the actual amount of padding is rounded 
 829:../target/stm32/malloc/mallocr.c ****       so that the end of the arena is always a system page boundary.
 830:../target/stm32/malloc/mallocr.c **** 
 831:../target/stm32/malloc/mallocr.c ****       The main reason for using padding is to avoid calling sbrk so
 832:../target/stm32/malloc/mallocr.c ****       often. Having even a small pad greatly reduces the likelihood
 833:../target/stm32/malloc/mallocr.c ****       that nearly every malloc request during program start-up (or
 834:../target/stm32/malloc/mallocr.c ****       after trimming) will invoke sbrk, which needlessly wastes
 835:../target/stm32/malloc/mallocr.c ****       time. 
 836:../target/stm32/malloc/mallocr.c **** 
 837:../target/stm32/malloc/mallocr.c ****       Automatic rounding-up to page-size units is normally sufficient
 838:../target/stm32/malloc/mallocr.c ****       to avoid measurable overhead, so the default is 0.  However, in
 839:../target/stm32/malloc/mallocr.c ****       systems where sbrk is relatively slow, it can pay to increase
 840:../target/stm32/malloc/mallocr.c ****       this value, at the expense of carrying around more memory than 
 841:../target/stm32/malloc/mallocr.c ****       the program needs.
 842:../target/stm32/malloc/mallocr.c **** 
 843:../target/stm32/malloc/mallocr.c **** */
 844:../target/stm32/malloc/mallocr.c **** 
 845:../target/stm32/malloc/mallocr.c **** 
 846:../target/stm32/malloc/mallocr.c **** #ifndef DEFAULT_MMAP_THRESHOLD
 847:../target/stm32/malloc/mallocr.c **** #define DEFAULT_MMAP_THRESHOLD (128 * 1024)
 848:../target/stm32/malloc/mallocr.c **** #endif
 849:../target/stm32/malloc/mallocr.c **** 
 850:../target/stm32/malloc/mallocr.c **** /*
 851:../target/stm32/malloc/mallocr.c **** 
 852:../target/stm32/malloc/mallocr.c ****     M_MMAP_THRESHOLD is the request size threshold for using mmap() 
 853:../target/stm32/malloc/mallocr.c ****       to service a request. Requests of at least this size that cannot 
 854:../target/stm32/malloc/mallocr.c ****       be allocated using already-existing space will be serviced via mmap.  
 855:../target/stm32/malloc/mallocr.c ****       (If enough normal freed space already exists it is used instead.)
 856:../target/stm32/malloc/mallocr.c **** 
 857:../target/stm32/malloc/mallocr.c ****       Using mmap segregates relatively large chunks of memory so that
 858:../target/stm32/malloc/mallocr.c ****       they can be individually obtained and released from the host
 859:../target/stm32/malloc/mallocr.c ****       system. A request serviced through mmap is never reused by any
 860:../target/stm32/malloc/mallocr.c ****       other request (at least not directly; the system may just so
 861:../target/stm32/malloc/mallocr.c ****       happen to remap successive requests to the same locations).
 862:../target/stm32/malloc/mallocr.c **** 
 863:../target/stm32/malloc/mallocr.c ****       Segregating space in this way has the benefit that mmapped space
 864:../target/stm32/malloc/mallocr.c ****       can ALWAYS be individually released back to the system, which
 865:../target/stm32/malloc/mallocr.c ****       helps keep the system level memory demands of a long-lived
 866:../target/stm32/malloc/mallocr.c ****       program low. Mapped memory can never become `locked' between
 867:../target/stm32/malloc/mallocr.c ****       other chunks, as can happen with normally allocated chunks, which
 868:../target/stm32/malloc/mallocr.c ****       menas that even trimming via malloc_trim would not release them.
 869:../target/stm32/malloc/mallocr.c **** 
 870:../target/stm32/malloc/mallocr.c ****       However, it has the disadvantages that:
 871:../target/stm32/malloc/mallocr.c **** 
 872:../target/stm32/malloc/mallocr.c ****          1. The space cannot be reclaimed, consolidated, and then
 873:../target/stm32/malloc/mallocr.c ****             used to service later requests, as happens with normal chunks. 
 874:../target/stm32/malloc/mallocr.c ****          2. It can lead to more wastage because of mmap page alignment
 875:../target/stm32/malloc/mallocr.c ****             requirements
 876:../target/stm32/malloc/mallocr.c ****          3. It causes malloc performance to be more dependent on host
 877:../target/stm32/malloc/mallocr.c ****             system memory management support routines which may vary in
 878:../target/stm32/malloc/mallocr.c ****             implementation quality and may impose arbitrary
 879:../target/stm32/malloc/mallocr.c ****             limitations. Generally, servicing a request via normal
 880:../target/stm32/malloc/mallocr.c ****             malloc steps is faster than going through a system's mmap.
 881:../target/stm32/malloc/mallocr.c **** 
 882:../target/stm32/malloc/mallocr.c ****       All together, these considerations should lead you to use mmap
 883:../target/stm32/malloc/mallocr.c ****       only for relatively large requests.  
 884:../target/stm32/malloc/mallocr.c **** 
 885:../target/stm32/malloc/mallocr.c **** 
 886:../target/stm32/malloc/mallocr.c **** */
 887:../target/stm32/malloc/mallocr.c **** 
 888:../target/stm32/malloc/mallocr.c **** 
 889:../target/stm32/malloc/mallocr.c **** 
 890:../target/stm32/malloc/mallocr.c **** #ifndef DEFAULT_MMAP_MAX
 891:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
 892:../target/stm32/malloc/mallocr.c **** #define DEFAULT_MMAP_MAX       (64)
 893:../target/stm32/malloc/mallocr.c **** #else
 894:../target/stm32/malloc/mallocr.c **** #define DEFAULT_MMAP_MAX       (0)
 895:../target/stm32/malloc/mallocr.c **** #endif
 896:../target/stm32/malloc/mallocr.c **** #endif
 897:../target/stm32/malloc/mallocr.c **** 
 898:../target/stm32/malloc/mallocr.c **** /*
 899:../target/stm32/malloc/mallocr.c ****     M_MMAP_MAX is the maximum number of requests to simultaneously 
 900:../target/stm32/malloc/mallocr.c ****       service using mmap. This parameter exists because:
 901:../target/stm32/malloc/mallocr.c **** 
 902:../target/stm32/malloc/mallocr.c ****          1. Some systems have a limited number of internal tables for
 903:../target/stm32/malloc/mallocr.c ****             use by mmap.
 904:../target/stm32/malloc/mallocr.c ****          2. In most systems, overreliance on mmap can degrade overall
 905:../target/stm32/malloc/mallocr.c ****             performance.
 906:../target/stm32/malloc/mallocr.c ****          3. If a program allocates many large regions, it is probably
 907:../target/stm32/malloc/mallocr.c ****             better off using normal sbrk-based allocation routines that
 908:../target/stm32/malloc/mallocr.c ****             can reclaim and reallocate normal heap memory. Using a
 909:../target/stm32/malloc/mallocr.c ****             small value allows transition into this mode after the
 910:../target/stm32/malloc/mallocr.c ****             first few allocations.
 911:../target/stm32/malloc/mallocr.c **** 
 912:../target/stm32/malloc/mallocr.c ****       Setting to 0 disables all use of mmap.  If HAVE_MMAP is not set,
 913:../target/stm32/malloc/mallocr.c ****       the default value is 0, and attempts to set it to non-zero values
 914:../target/stm32/malloc/mallocr.c ****       in mallopt will fail.
 915:../target/stm32/malloc/mallocr.c **** */
 916:../target/stm32/malloc/mallocr.c **** 
 917:../target/stm32/malloc/mallocr.c **** 
 918:../target/stm32/malloc/mallocr.c **** 
 919:../target/stm32/malloc/mallocr.c **** 
 920:../target/stm32/malloc/mallocr.c **** /* 
 921:../target/stm32/malloc/mallocr.c **** 
 922:../target/stm32/malloc/mallocr.c ****   Special defines for linux libc
 923:../target/stm32/malloc/mallocr.c **** 
 924:../target/stm32/malloc/mallocr.c ****   Except when compiled using these special defines for Linux libc
 925:../target/stm32/malloc/mallocr.c ****   using weak aliases, this malloc is NOT designed to work in
 926:../target/stm32/malloc/mallocr.c ****   multithreaded applications.  No semaphores or other concurrency
 927:../target/stm32/malloc/mallocr.c ****   control are provided to ensure that multiple malloc or free calls
 928:../target/stm32/malloc/mallocr.c ****   don't run at the same time, which could be disasterous. A single
 929:../target/stm32/malloc/mallocr.c ****   semaphore could be used across malloc, realloc, and free (which is
 930:../target/stm32/malloc/mallocr.c ****   essentially the effect of the linux weak alias approach). It would
 931:../target/stm32/malloc/mallocr.c ****   be hard to obtain finer granularity.
 932:../target/stm32/malloc/mallocr.c **** 
 933:../target/stm32/malloc/mallocr.c **** */
 934:../target/stm32/malloc/mallocr.c **** 
 935:../target/stm32/malloc/mallocr.c **** 
 936:../target/stm32/malloc/mallocr.c **** #ifdef INTERNAL_LINUX_C_LIB
 937:../target/stm32/malloc/mallocr.c **** 
 938:../target/stm32/malloc/mallocr.c **** #if __STD_C
 939:../target/stm32/malloc/mallocr.c **** 
 940:../target/stm32/malloc/mallocr.c **** Void_t * __default_morecore_init (ptrdiff_t);
 941:../target/stm32/malloc/mallocr.c **** Void_t *(*__morecore)(ptrdiff_t) = __default_morecore_init;
 942:../target/stm32/malloc/mallocr.c **** 
 943:../target/stm32/malloc/mallocr.c **** #else
 944:../target/stm32/malloc/mallocr.c **** 
 945:../target/stm32/malloc/mallocr.c **** Void_t * __default_morecore_init ();
 946:../target/stm32/malloc/mallocr.c **** Void_t *(*__morecore)() = __default_morecore_init;
 947:../target/stm32/malloc/mallocr.c **** 
 948:../target/stm32/malloc/mallocr.c **** #endif
 949:../target/stm32/malloc/mallocr.c **** 
 950:../target/stm32/malloc/mallocr.c **** #define MORECORE (*__morecore)
 951:../target/stm32/malloc/mallocr.c **** #define MORECORE_FAILURE 0
 952:../target/stm32/malloc/mallocr.c **** #define MORECORE_CLEARS 1 
 953:../target/stm32/malloc/mallocr.c **** 
 954:../target/stm32/malloc/mallocr.c **** #else /* INTERNAL_LINUX_C_LIB */
 955:../target/stm32/malloc/mallocr.c **** 
 956:../target/stm32/malloc/mallocr.c **** #ifndef INTERNAL_NEWLIB
 957:../target/stm32/malloc/mallocr.c **** #if __STD_C
 958:../target/stm32/malloc/mallocr.c **** extern Void_t*     sbrk(ptrdiff_t);
 959:../target/stm32/malloc/mallocr.c **** #else
 960:../target/stm32/malloc/mallocr.c **** extern Void_t*     sbrk();
 961:../target/stm32/malloc/mallocr.c **** #endif
 962:../target/stm32/malloc/mallocr.c **** #endif
 963:../target/stm32/malloc/mallocr.c **** 
 964:../target/stm32/malloc/mallocr.c **** #ifndef MORECORE
 965:../target/stm32/malloc/mallocr.c **** #define MORECORE sbrk
 966:../target/stm32/malloc/mallocr.c **** #endif
 967:../target/stm32/malloc/mallocr.c **** 
 968:../target/stm32/malloc/mallocr.c **** #ifndef MORECORE_FAILURE
 969:../target/stm32/malloc/mallocr.c **** #define MORECORE_FAILURE -1
 970:../target/stm32/malloc/mallocr.c **** #endif
 971:../target/stm32/malloc/mallocr.c **** 
 972:../target/stm32/malloc/mallocr.c **** #ifndef MORECORE_CLEARS
 973:../target/stm32/malloc/mallocr.c **** #define MORECORE_CLEARS 1
 974:../target/stm32/malloc/mallocr.c **** #endif
 975:../target/stm32/malloc/mallocr.c **** 
 976:../target/stm32/malloc/mallocr.c **** #endif /* INTERNAL_LINUX_C_LIB */
 977:../target/stm32/malloc/mallocr.c **** 
 978:../target/stm32/malloc/mallocr.c **** #if defined(INTERNAL_LINUX_C_LIB) && defined(__ELF__)
 979:../target/stm32/malloc/mallocr.c **** 
 980:../target/stm32/malloc/mallocr.c **** #define cALLOc		__libc_calloc
 981:../target/stm32/malloc/mallocr.c **** #define fREe		__libc_free
 982:../target/stm32/malloc/mallocr.c **** #define mALLOc		__libc_malloc
 983:../target/stm32/malloc/mallocr.c **** #define mEMALIGn	__libc_memalign
 984:../target/stm32/malloc/mallocr.c **** #define rEALLOc		__libc_realloc
 985:../target/stm32/malloc/mallocr.c **** #define vALLOc		__libc_valloc
 986:../target/stm32/malloc/mallocr.c **** #define pvALLOc		__libc_pvalloc
 987:../target/stm32/malloc/mallocr.c **** #define mALLINFo	__libc_mallinfo
 988:../target/stm32/malloc/mallocr.c **** #define mALLOPt		__libc_mallopt
 989:../target/stm32/malloc/mallocr.c **** 
 990:../target/stm32/malloc/mallocr.c **** #pragma weak calloc = __libc_calloc
 991:../target/stm32/malloc/mallocr.c **** #pragma weak free = __libc_free
 992:../target/stm32/malloc/mallocr.c **** #pragma weak cfree = __libc_free
 993:../target/stm32/malloc/mallocr.c **** #pragma weak malloc = __libc_malloc
 994:../target/stm32/malloc/mallocr.c **** #pragma weak memalign = __libc_memalign
 995:../target/stm32/malloc/mallocr.c **** #pragma weak realloc = __libc_realloc
 996:../target/stm32/malloc/mallocr.c **** #pragma weak valloc = __libc_valloc
 997:../target/stm32/malloc/mallocr.c **** #pragma weak pvalloc = __libc_pvalloc
 998:../target/stm32/malloc/mallocr.c **** #pragma weak mallinfo = __libc_mallinfo
 999:../target/stm32/malloc/mallocr.c **** #pragma weak mallopt = __libc_mallopt
1000:../target/stm32/malloc/mallocr.c **** 
1001:../target/stm32/malloc/mallocr.c **** #else
1002:../target/stm32/malloc/mallocr.c **** 
1003:../target/stm32/malloc/mallocr.c **** #ifdef INTERNAL_NEWLIB
1004:../target/stm32/malloc/mallocr.c **** 
1005:../target/stm32/malloc/mallocr.c **** #define cALLOc		_calloc_r
1006:../target/stm32/malloc/mallocr.c **** #define fREe		_free_r
1007:../target/stm32/malloc/mallocr.c **** #define mALLOc		_malloc_r
1008:../target/stm32/malloc/mallocr.c **** #define mEMALIGn	_memalign_r
1009:../target/stm32/malloc/mallocr.c **** #define rEALLOc		_realloc_r
1010:../target/stm32/malloc/mallocr.c **** #define vALLOc		_valloc_r
1011:../target/stm32/malloc/mallocr.c **** #define pvALLOc		_pvalloc_r
1012:../target/stm32/malloc/mallocr.c **** #define mALLINFo	_mallinfo_r
1013:../target/stm32/malloc/mallocr.c **** #define mALLOPt		_mallopt_r
1014:../target/stm32/malloc/mallocr.c **** 
1015:../target/stm32/malloc/mallocr.c **** #define malloc_stats			_malloc_stats_r
1016:../target/stm32/malloc/mallocr.c **** #define malloc_trim			_malloc_trim_r
1017:../target/stm32/malloc/mallocr.c **** #define malloc_usable_size		_malloc_usable_size_r
1018:../target/stm32/malloc/mallocr.c **** 
1019:../target/stm32/malloc/mallocr.c **** #define malloc_update_mallinfo		__malloc_update_mallinfo
1020:../target/stm32/malloc/mallocr.c **** 
1021:../target/stm32/malloc/mallocr.c **** #define malloc_av_			__malloc_av_
1022:../target/stm32/malloc/mallocr.c **** #define malloc_current_mallinfo		__malloc_current_mallinfo
1023:../target/stm32/malloc/mallocr.c **** #define malloc_max_sbrked_mem		__malloc_max_sbrked_mem
1024:../target/stm32/malloc/mallocr.c **** #define malloc_max_total_mem		__malloc_max_total_mem
1025:../target/stm32/malloc/mallocr.c **** #define malloc_sbrk_base		__malloc_sbrk_base
1026:../target/stm32/malloc/mallocr.c **** #define malloc_top_pad			__malloc_top_pad
1027:../target/stm32/malloc/mallocr.c **** #define malloc_trim_threshold		__malloc_trim_threshold
1028:../target/stm32/malloc/mallocr.c **** 
1029:../target/stm32/malloc/mallocr.c **** #else /* ! INTERNAL_NEWLIB */
1030:../target/stm32/malloc/mallocr.c **** 
1031:../target/stm32/malloc/mallocr.c **** #define cALLOc		calloc
1032:../target/stm32/malloc/mallocr.c **** #define fREe		free
1033:../target/stm32/malloc/mallocr.c **** #define mALLOc		malloc
1034:../target/stm32/malloc/mallocr.c **** #define mEMALIGn	memalign
1035:../target/stm32/malloc/mallocr.c **** #define rEALLOc		realloc
1036:../target/stm32/malloc/mallocr.c **** #define vALLOc		valloc
1037:../target/stm32/malloc/mallocr.c **** #define pvALLOc		pvalloc
1038:../target/stm32/malloc/mallocr.c **** #define mALLINFo	mallinfo
1039:../target/stm32/malloc/mallocr.c **** #define mALLOPt		mallopt
1040:../target/stm32/malloc/mallocr.c **** 
1041:../target/stm32/malloc/mallocr.c **** #endif /* ! INTERNAL_NEWLIB */
1042:../target/stm32/malloc/mallocr.c **** #endif
1043:../target/stm32/malloc/mallocr.c **** 
1044:../target/stm32/malloc/mallocr.c **** /* Public routines */
1045:../target/stm32/malloc/mallocr.c **** 
1046:../target/stm32/malloc/mallocr.c **** #if __STD_C
1047:../target/stm32/malloc/mallocr.c **** 
1048:../target/stm32/malloc/mallocr.c **** Void_t* mALLOc(RARG size_t);
1049:../target/stm32/malloc/mallocr.c **** void    fREe(RARG Void_t*);
1050:../target/stm32/malloc/mallocr.c **** Void_t* rEALLOc(RARG Void_t*, size_t);
1051:../target/stm32/malloc/mallocr.c **** Void_t* mEMALIGn(RARG size_t, size_t);
1052:../target/stm32/malloc/mallocr.c **** Void_t* vALLOc(RARG size_t);
1053:../target/stm32/malloc/mallocr.c **** Void_t* pvALLOc(RARG size_t);
1054:../target/stm32/malloc/mallocr.c **** Void_t* cALLOc(RARG size_t, size_t);
1055:../target/stm32/malloc/mallocr.c **** void    cfree(Void_t*);
1056:../target/stm32/malloc/mallocr.c **** int     malloc_trim(RARG size_t);
1057:../target/stm32/malloc/mallocr.c **** size_t  malloc_usable_size(RARG Void_t*);
1058:../target/stm32/malloc/mallocr.c **** void    malloc_stats(RONEARG);
1059:../target/stm32/malloc/mallocr.c **** int     mALLOPt(RARG int, int);
1060:../target/stm32/malloc/mallocr.c **** struct mallinfo mALLINFo(RONEARG);
1061:../target/stm32/malloc/mallocr.c **** #else
1062:../target/stm32/malloc/mallocr.c **** Void_t* mALLOc();
1063:../target/stm32/malloc/mallocr.c **** void    fREe();
1064:../target/stm32/malloc/mallocr.c **** Void_t* rEALLOc();
1065:../target/stm32/malloc/mallocr.c **** Void_t* mEMALIGn();
1066:../target/stm32/malloc/mallocr.c **** Void_t* vALLOc();
1067:../target/stm32/malloc/mallocr.c **** Void_t* pvALLOc();
1068:../target/stm32/malloc/mallocr.c **** Void_t* cALLOc();
1069:../target/stm32/malloc/mallocr.c **** void    cfree();
1070:../target/stm32/malloc/mallocr.c **** int     malloc_trim();
1071:../target/stm32/malloc/mallocr.c **** size_t  malloc_usable_size();
1072:../target/stm32/malloc/mallocr.c **** void    malloc_stats();
1073:../target/stm32/malloc/mallocr.c **** int     mALLOPt();
1074:../target/stm32/malloc/mallocr.c **** struct mallinfo mALLINFo();
1075:../target/stm32/malloc/mallocr.c **** #endif
1076:../target/stm32/malloc/mallocr.c **** 
1077:../target/stm32/malloc/mallocr.c **** 
1078:../target/stm32/malloc/mallocr.c **** #ifdef __cplusplus
1079:../target/stm32/malloc/mallocr.c **** };  /* end of extern "C" */
1080:../target/stm32/malloc/mallocr.c **** #endif
1081:../target/stm32/malloc/mallocr.c **** 
1082:../target/stm32/malloc/mallocr.c **** /* ---------- To make a malloc.h, end cutting here ------------ */
1083:../target/stm32/malloc/mallocr.c **** 
1084:../target/stm32/malloc/mallocr.c **** 
1085:../target/stm32/malloc/mallocr.c **** /* 
1086:../target/stm32/malloc/mallocr.c ****   Emulation of sbrk for WIN32
1087:../target/stm32/malloc/mallocr.c ****   All code within the ifdef WIN32 is untested by me.
1088:../target/stm32/malloc/mallocr.c **** */
1089:../target/stm32/malloc/mallocr.c **** 
1090:../target/stm32/malloc/mallocr.c **** 
1091:../target/stm32/malloc/mallocr.c **** #ifdef WIN32
1092:../target/stm32/malloc/mallocr.c **** 
1093:../target/stm32/malloc/mallocr.c **** #define AlignPage(add) (((add) + (malloc_getpagesize-1)) & \
1094:../target/stm32/malloc/mallocr.c **** ~(malloc_getpagesize-1))
1095:../target/stm32/malloc/mallocr.c **** 
1096:../target/stm32/malloc/mallocr.c **** /* resrve 64MB to insure large contiguous space */ 
1097:../target/stm32/malloc/mallocr.c **** #define RESERVED_SIZE (1024*1024*64)
1098:../target/stm32/malloc/mallocr.c **** #define NEXT_SIZE (2048*1024)
1099:../target/stm32/malloc/mallocr.c **** #define TOP_MEMORY ((unsigned long)2*1024*1024*1024)
1100:../target/stm32/malloc/mallocr.c **** 
1101:../target/stm32/malloc/mallocr.c **** struct GmListElement;
1102:../target/stm32/malloc/mallocr.c **** typedef struct GmListElement GmListElement;
1103:../target/stm32/malloc/mallocr.c **** 
1104:../target/stm32/malloc/mallocr.c **** struct GmListElement 
1105:../target/stm32/malloc/mallocr.c **** {
1106:../target/stm32/malloc/mallocr.c **** 	GmListElement* next;
1107:../target/stm32/malloc/mallocr.c **** 	void* base;
1108:../target/stm32/malloc/mallocr.c **** };
1109:../target/stm32/malloc/mallocr.c **** 
1110:../target/stm32/malloc/mallocr.c **** static GmListElement* head = 0;
1111:../target/stm32/malloc/mallocr.c **** static unsigned int gNextAddress = 0;
1112:../target/stm32/malloc/mallocr.c **** static unsigned int gAddressBase = 0;
1113:../target/stm32/malloc/mallocr.c **** static unsigned int gAllocatedSize = 0;
1114:../target/stm32/malloc/mallocr.c **** 
1115:../target/stm32/malloc/mallocr.c **** static
1116:../target/stm32/malloc/mallocr.c **** GmListElement* makeGmListElement (void* bas)
1117:../target/stm32/malloc/mallocr.c **** {
1118:../target/stm32/malloc/mallocr.c **** 	GmListElement* this;
1119:../target/stm32/malloc/mallocr.c **** 	this = (GmListElement*)(void*)LocalAlloc (0, sizeof (GmListElement));
1120:../target/stm32/malloc/mallocr.c **** 	ASSERT (this);
1121:../target/stm32/malloc/mallocr.c **** 	if (this)
1122:../target/stm32/malloc/mallocr.c **** 	{
1123:../target/stm32/malloc/mallocr.c **** 		this->base = bas;
1124:../target/stm32/malloc/mallocr.c **** 		this->next = head;
1125:../target/stm32/malloc/mallocr.c **** 		head = this;
1126:../target/stm32/malloc/mallocr.c **** 	}
1127:../target/stm32/malloc/mallocr.c **** 	return this;
1128:../target/stm32/malloc/mallocr.c **** }
1129:../target/stm32/malloc/mallocr.c **** 
1130:../target/stm32/malloc/mallocr.c **** void gcleanup ()
1131:../target/stm32/malloc/mallocr.c **** {
1132:../target/stm32/malloc/mallocr.c **** 	BOOL rval;
1133:../target/stm32/malloc/mallocr.c **** 	ASSERT ( (head == NULL) || (head->base == (void*)gAddressBase));
1134:../target/stm32/malloc/mallocr.c **** 	if (gAddressBase && (gNextAddress - gAddressBase))
1135:../target/stm32/malloc/mallocr.c **** 	{
1136:../target/stm32/malloc/mallocr.c **** 		rval = VirtualFree ((void*)gAddressBase, 
1137:../target/stm32/malloc/mallocr.c **** 							gNextAddress - gAddressBase, 
1138:../target/stm32/malloc/mallocr.c **** 							MEM_DECOMMIT);
1139:../target/stm32/malloc/mallocr.c ****         ASSERT (rval);
1140:../target/stm32/malloc/mallocr.c **** 	}
1141:../target/stm32/malloc/mallocr.c **** 	while (head)
1142:../target/stm32/malloc/mallocr.c **** 	{
1143:../target/stm32/malloc/mallocr.c **** 		GmListElement* next = head->next;
1144:../target/stm32/malloc/mallocr.c **** 		rval = VirtualFree (head->base, 0, MEM_RELEASE);
1145:../target/stm32/malloc/mallocr.c **** 		ASSERT (rval);
1146:../target/stm32/malloc/mallocr.c **** 		LocalFree (head);
1147:../target/stm32/malloc/mallocr.c **** 		head = next;
1148:../target/stm32/malloc/mallocr.c **** 	}
1149:../target/stm32/malloc/mallocr.c **** }
1150:../target/stm32/malloc/mallocr.c **** 		
1151:../target/stm32/malloc/mallocr.c **** static
1152:../target/stm32/malloc/mallocr.c **** void* findRegion (void* start_address, unsigned long size)
1153:../target/stm32/malloc/mallocr.c **** {
1154:../target/stm32/malloc/mallocr.c **** 	MEMORY_BASIC_INFORMATION info;
1155:../target/stm32/malloc/mallocr.c **** 	while ((unsigned long)start_address < TOP_MEMORY)
1156:../target/stm32/malloc/mallocr.c **** 	{
1157:../target/stm32/malloc/mallocr.c **** 		VirtualQuery (start_address, &info, sizeof (info));
1158:../target/stm32/malloc/mallocr.c **** 		if (info.State != MEM_FREE)
1159:../target/stm32/malloc/mallocr.c **** 			start_address = (char*)info.BaseAddress + info.RegionSize;
1160:../target/stm32/malloc/mallocr.c **** 		else if (info.RegionSize >= size)
1161:../target/stm32/malloc/mallocr.c **** 			return start_address;
1162:../target/stm32/malloc/mallocr.c **** 		else
1163:../target/stm32/malloc/mallocr.c **** 			start_address = (char*)info.BaseAddress + info.RegionSize; 
1164:../target/stm32/malloc/mallocr.c **** 	}
1165:../target/stm32/malloc/mallocr.c **** 	return NULL;
1166:../target/stm32/malloc/mallocr.c **** 	
1167:../target/stm32/malloc/mallocr.c **** }
1168:../target/stm32/malloc/mallocr.c **** 
1169:../target/stm32/malloc/mallocr.c **** 
1170:../target/stm32/malloc/mallocr.c **** void* wsbrk (long size)
1171:../target/stm32/malloc/mallocr.c **** {
1172:../target/stm32/malloc/mallocr.c **** 	void* tmp;
1173:../target/stm32/malloc/mallocr.c **** 	if (size > 0)
1174:../target/stm32/malloc/mallocr.c **** 	{
1175:../target/stm32/malloc/mallocr.c **** 		if (gAddressBase == 0)
1176:../target/stm32/malloc/mallocr.c **** 		{
1177:../target/stm32/malloc/mallocr.c **** 			gAllocatedSize = max (RESERVED_SIZE, AlignPage (size));
1178:../target/stm32/malloc/mallocr.c **** 			gNextAddress = gAddressBase = 
1179:../target/stm32/malloc/mallocr.c **** 				(unsigned int)VirtualAlloc (NULL, gAllocatedSize, 
1180:../target/stm32/malloc/mallocr.c **** 											MEM_RESERVE, PAGE_NOACCESS);
1181:../target/stm32/malloc/mallocr.c **** 		} else if (AlignPage (gNextAddress + size) > (gAddressBase +
1182:../target/stm32/malloc/mallocr.c **** gAllocatedSize))
1183:../target/stm32/malloc/mallocr.c **** 		{
1184:../target/stm32/malloc/mallocr.c **** 			long new_size = max (NEXT_SIZE, AlignPage (size));
1185:../target/stm32/malloc/mallocr.c **** 			void* new_address = (void*)(gAddressBase+gAllocatedSize);
1186:../target/stm32/malloc/mallocr.c **** 			do 
1187:../target/stm32/malloc/mallocr.c **** 			{
1188:../target/stm32/malloc/mallocr.c **** 				new_address = findRegion (new_address, new_size);
1189:../target/stm32/malloc/mallocr.c **** 				
1190:../target/stm32/malloc/mallocr.c **** 				if (new_address == 0)
1191:../target/stm32/malloc/mallocr.c **** 					return (void*)-1;
1192:../target/stm32/malloc/mallocr.c **** 
1193:../target/stm32/malloc/mallocr.c **** 				gAddressBase = gNextAddress =
1194:../target/stm32/malloc/mallocr.c **** 					(unsigned int)VirtualAlloc (new_address, new_size,
1195:../target/stm32/malloc/mallocr.c **** 												MEM_RESERVE, PAGE_NOACCESS);
1196:../target/stm32/malloc/mallocr.c **** 				// repeat in case of race condition
1197:../target/stm32/malloc/mallocr.c **** 				// The region that we found has been snagged 
1198:../target/stm32/malloc/mallocr.c **** 				// by another thread
1199:../target/stm32/malloc/mallocr.c **** 			}
1200:../target/stm32/malloc/mallocr.c **** 			while (gAddressBase == 0);
1201:../target/stm32/malloc/mallocr.c **** 
1202:../target/stm32/malloc/mallocr.c **** 			ASSERT (new_address == (void*)gAddressBase);
1203:../target/stm32/malloc/mallocr.c **** 
1204:../target/stm32/malloc/mallocr.c **** 			gAllocatedSize = new_size;
1205:../target/stm32/malloc/mallocr.c **** 
1206:../target/stm32/malloc/mallocr.c **** 			if (!makeGmListElement ((void*)gAddressBase))
1207:../target/stm32/malloc/mallocr.c **** 				return (void*)-1;
1208:../target/stm32/malloc/mallocr.c **** 		}
1209:../target/stm32/malloc/mallocr.c **** 		if ((size + gNextAddress) > AlignPage (gNextAddress))
1210:../target/stm32/malloc/mallocr.c **** 		{
1211:../target/stm32/malloc/mallocr.c **** 			void* res;
1212:../target/stm32/malloc/mallocr.c **** 			res = VirtualAlloc ((void*)AlignPage (gNextAddress),
1213:../target/stm32/malloc/mallocr.c **** 								(size + gNextAddress - 
1214:../target/stm32/malloc/mallocr.c **** 								 AlignPage (gNextAddress)), 
1215:../target/stm32/malloc/mallocr.c **** 								MEM_COMMIT, PAGE_READWRITE);
1216:../target/stm32/malloc/mallocr.c **** 			if (res == 0)
1217:../target/stm32/malloc/mallocr.c **** 				return (void*)-1;
1218:../target/stm32/malloc/mallocr.c **** 		}
1219:../target/stm32/malloc/mallocr.c **** 		tmp = (void*)gNextAddress;
1220:../target/stm32/malloc/mallocr.c **** 		gNextAddress = (unsigned int)tmp + size;
1221:../target/stm32/malloc/mallocr.c **** 		return tmp;
1222:../target/stm32/malloc/mallocr.c **** 	}
1223:../target/stm32/malloc/mallocr.c **** 	else if (size < 0)
1224:../target/stm32/malloc/mallocr.c **** 	{
1225:../target/stm32/malloc/mallocr.c **** 		unsigned int alignedGoal = AlignPage (gNextAddress + size);
1226:../target/stm32/malloc/mallocr.c **** 		/* Trim by releasing the virtual memory */
1227:../target/stm32/malloc/mallocr.c **** 		if (alignedGoal >= gAddressBase)
1228:../target/stm32/malloc/mallocr.c **** 		{
1229:../target/stm32/malloc/mallocr.c **** 			VirtualFree ((void*)alignedGoal, gNextAddress - alignedGoal,  
1230:../target/stm32/malloc/mallocr.c **** 						 MEM_DECOMMIT);
1231:../target/stm32/malloc/mallocr.c **** 			gNextAddress = gNextAddress + size;
1232:../target/stm32/malloc/mallocr.c **** 			return (void*)gNextAddress;
1233:../target/stm32/malloc/mallocr.c **** 		}
1234:../target/stm32/malloc/mallocr.c **** 		else 
1235:../target/stm32/malloc/mallocr.c **** 		{
1236:../target/stm32/malloc/mallocr.c **** 			VirtualFree ((void*)gAddressBase, gNextAddress - gAddressBase,
1237:../target/stm32/malloc/mallocr.c **** 						 MEM_DECOMMIT);
1238:../target/stm32/malloc/mallocr.c **** 			gNextAddress = gAddressBase;
1239:../target/stm32/malloc/mallocr.c **** 			return (void*)-1;
1240:../target/stm32/malloc/mallocr.c **** 		}
1241:../target/stm32/malloc/mallocr.c **** 	}
1242:../target/stm32/malloc/mallocr.c **** 	else
1243:../target/stm32/malloc/mallocr.c **** 	{
1244:../target/stm32/malloc/mallocr.c **** 		return (void*)gNextAddress;
1245:../target/stm32/malloc/mallocr.c **** 	}
1246:../target/stm32/malloc/mallocr.c **** }
1247:../target/stm32/malloc/mallocr.c **** 
1248:../target/stm32/malloc/mallocr.c **** #endif
1249:../target/stm32/malloc/mallocr.c **** 
1250:../target/stm32/malloc/mallocr.c **** 
1251:../target/stm32/malloc/mallocr.c **** 
1252:../target/stm32/malloc/mallocr.c **** /*
1253:../target/stm32/malloc/mallocr.c ****   Type declarations
1254:../target/stm32/malloc/mallocr.c **** */
1255:../target/stm32/malloc/mallocr.c **** 
1256:../target/stm32/malloc/mallocr.c **** 
1257:../target/stm32/malloc/mallocr.c **** struct malloc_chunk
1258:../target/stm32/malloc/mallocr.c **** {
1259:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T prev_size; /* Size of previous chunk (if free). */
1260:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T size;      /* Size in bytes, including overhead. */
1261:../target/stm32/malloc/mallocr.c ****   struct malloc_chunk* fd;   /* double links -- used only if free. */
1262:../target/stm32/malloc/mallocr.c ****   struct malloc_chunk* bk;
1263:../target/stm32/malloc/mallocr.c **** };
1264:../target/stm32/malloc/mallocr.c **** 
1265:../target/stm32/malloc/mallocr.c **** typedef struct malloc_chunk* mchunkptr;
1266:../target/stm32/malloc/mallocr.c **** 
1267:../target/stm32/malloc/mallocr.c **** /*
1268:../target/stm32/malloc/mallocr.c **** 
1269:../target/stm32/malloc/mallocr.c ****    malloc_chunk details:
1270:../target/stm32/malloc/mallocr.c **** 
1271:../target/stm32/malloc/mallocr.c ****     (The following includes lightly edited explanations by Colin Plumb.)
1272:../target/stm32/malloc/mallocr.c **** 
1273:../target/stm32/malloc/mallocr.c ****     Chunks of memory are maintained using a `boundary tag' method as
1274:../target/stm32/malloc/mallocr.c ****     described in e.g., Knuth or Standish.  (See the paper by Paul
1275:../target/stm32/malloc/mallocr.c ****     Wilson ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps for a
1276:../target/stm32/malloc/mallocr.c ****     survey of such techniques.)  Sizes of free chunks are stored both
1277:../target/stm32/malloc/mallocr.c ****     in the front of each chunk and at the end.  This makes
1278:../target/stm32/malloc/mallocr.c ****     consolidating fragmented chunks into bigger chunks very fast.  The
1279:../target/stm32/malloc/mallocr.c ****     size fields also hold bits representing whether chunks are free or
1280:../target/stm32/malloc/mallocr.c ****     in use.
1281:../target/stm32/malloc/mallocr.c **** 
1282:../target/stm32/malloc/mallocr.c ****     An allocated chunk looks like this:  
1283:../target/stm32/malloc/mallocr.c **** 
1284:../target/stm32/malloc/mallocr.c **** 
1285:../target/stm32/malloc/mallocr.c ****     chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1286:../target/stm32/malloc/mallocr.c ****             |             Size of previous chunk, if allocated            | |
1287:../target/stm32/malloc/mallocr.c ****             +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1288:../target/stm32/malloc/mallocr.c ****             |             Size of chunk, in bytes                         |P|
1289:../target/stm32/malloc/mallocr.c ****       mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1290:../target/stm32/malloc/mallocr.c ****             |             User data starts here...                          .
1291:../target/stm32/malloc/mallocr.c ****             .                                                               .
1292:../target/stm32/malloc/mallocr.c ****             .             (malloc_usable_space() bytes)                     .
1293:../target/stm32/malloc/mallocr.c ****             .                                                               |
1294:../target/stm32/malloc/mallocr.c **** nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1295:../target/stm32/malloc/mallocr.c ****             |             Size of chunk                                     |
1296:../target/stm32/malloc/mallocr.c ****             +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1297:../target/stm32/malloc/mallocr.c **** 
1298:../target/stm32/malloc/mallocr.c **** 
1299:../target/stm32/malloc/mallocr.c ****     Where "chunk" is the front of the chunk for the purpose of most of
1300:../target/stm32/malloc/mallocr.c ****     the malloc code, but "mem" is the pointer that is returned to the
1301:../target/stm32/malloc/mallocr.c ****     user.  "Nextchunk" is the beginning of the next contiguous chunk.
1302:../target/stm32/malloc/mallocr.c **** 
1303:../target/stm32/malloc/mallocr.c ****     Chunks always begin on even word boundries, so the mem portion
1304:../target/stm32/malloc/mallocr.c ****     (which is returned to the user) is also on an even word boundary, and
1305:../target/stm32/malloc/mallocr.c ****     thus double-word aligned.
1306:../target/stm32/malloc/mallocr.c **** 
1307:../target/stm32/malloc/mallocr.c ****     Free chunks are stored in circular doubly-linked lists, and look like this:
1308:../target/stm32/malloc/mallocr.c **** 
1309:../target/stm32/malloc/mallocr.c ****     chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1310:../target/stm32/malloc/mallocr.c ****             |             Size of previous chunk                            |
1311:../target/stm32/malloc/mallocr.c ****             +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1312:../target/stm32/malloc/mallocr.c ****     `head:' |             Size of chunk, in bytes                         |P|
1313:../target/stm32/malloc/mallocr.c ****       mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1314:../target/stm32/malloc/mallocr.c ****             |             Forward pointer to next chunk in list             |
1315:../target/stm32/malloc/mallocr.c ****             +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1316:../target/stm32/malloc/mallocr.c ****             |             Back pointer to previous chunk in list            |
1317:../target/stm32/malloc/mallocr.c ****             +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1318:../target/stm32/malloc/mallocr.c ****             |             Unused space (may be 0 bytes long)                .
1319:../target/stm32/malloc/mallocr.c ****             .                                                               .
1320:../target/stm32/malloc/mallocr.c ****             .                                                               |
1321:../target/stm32/malloc/mallocr.c **** nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1322:../target/stm32/malloc/mallocr.c ****     `foot:' |             Size of chunk, in bytes                           |
1323:../target/stm32/malloc/mallocr.c ****             +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
1324:../target/stm32/malloc/mallocr.c **** 
1325:../target/stm32/malloc/mallocr.c ****     The P (PREV_INUSE) bit, stored in the unused low-order bit of the
1326:../target/stm32/malloc/mallocr.c ****     chunk size (which is always a multiple of two words), is an in-use
1327:../target/stm32/malloc/mallocr.c ****     bit for the *previous* chunk.  If that bit is *clear*, then the
1328:../target/stm32/malloc/mallocr.c ****     word before the current chunk size contains the previous chunk
1329:../target/stm32/malloc/mallocr.c ****     size, and can be used to find the front of the previous chunk.
1330:../target/stm32/malloc/mallocr.c ****     (The very first chunk allocated always has this bit set,
1331:../target/stm32/malloc/mallocr.c ****     preventing access to non-existent (or non-owned) memory.)
1332:../target/stm32/malloc/mallocr.c **** 
1333:../target/stm32/malloc/mallocr.c ****     Note that the `foot' of the current chunk is actually represented
1334:../target/stm32/malloc/mallocr.c ****     as the prev_size of the NEXT chunk. (This makes it easier to
1335:../target/stm32/malloc/mallocr.c ****     deal with alignments etc).
1336:../target/stm32/malloc/mallocr.c **** 
1337:../target/stm32/malloc/mallocr.c ****     The two exceptions to all this are 
1338:../target/stm32/malloc/mallocr.c **** 
1339:../target/stm32/malloc/mallocr.c ****      1. The special chunk `top', which doesn't bother using the 
1340:../target/stm32/malloc/mallocr.c ****         trailing size field since there is no
1341:../target/stm32/malloc/mallocr.c ****         next contiguous chunk that would have to index off it. (After
1342:../target/stm32/malloc/mallocr.c ****         initialization, `top' is forced to always exist.  If it would
1343:../target/stm32/malloc/mallocr.c ****         become less than MINSIZE bytes long, it is replenished via
1344:../target/stm32/malloc/mallocr.c ****         malloc_extend_top.)
1345:../target/stm32/malloc/mallocr.c **** 
1346:../target/stm32/malloc/mallocr.c ****      2. Chunks allocated via mmap, which have the second-lowest-order
1347:../target/stm32/malloc/mallocr.c ****         bit (IS_MMAPPED) set in their size fields.  Because they are
1348:../target/stm32/malloc/mallocr.c ****         never merged or traversed from any other chunk, they have no
1349:../target/stm32/malloc/mallocr.c ****         foot size or inuse information.
1350:../target/stm32/malloc/mallocr.c **** 
1351:../target/stm32/malloc/mallocr.c ****     Available chunks are kept in any of several places (all declared below):
1352:../target/stm32/malloc/mallocr.c **** 
1353:../target/stm32/malloc/mallocr.c ****     * `av': An array of chunks serving as bin headers for consolidated
1354:../target/stm32/malloc/mallocr.c ****        chunks. Each bin is doubly linked.  The bins are approximately
1355:../target/stm32/malloc/mallocr.c ****        proportionally (log) spaced.  There are a lot of these bins
1356:../target/stm32/malloc/mallocr.c ****        (128). This may look excessive, but works very well in
1357:../target/stm32/malloc/mallocr.c ****        practice.  All procedures maintain the invariant that no
1358:../target/stm32/malloc/mallocr.c ****        consolidated chunk physically borders another one. Chunks in
1359:../target/stm32/malloc/mallocr.c ****        bins are kept in size order, with ties going to the
1360:../target/stm32/malloc/mallocr.c ****        approximately least recently used chunk.
1361:../target/stm32/malloc/mallocr.c **** 
1362:../target/stm32/malloc/mallocr.c ****        The chunks in each bin are maintained in decreasing sorted order by
1363:../target/stm32/malloc/mallocr.c ****        size.  This is irrelevant for the small bins, which all contain
1364:../target/stm32/malloc/mallocr.c ****        the same-sized chunks, but facilitates best-fit allocation for
1365:../target/stm32/malloc/mallocr.c ****        larger chunks. (These lists are just sequential. Keeping them in
1366:../target/stm32/malloc/mallocr.c ****        order almost never requires enough traversal to warrant using
1367:../target/stm32/malloc/mallocr.c ****        fancier ordered data structures.)  Chunks of the same size are
1368:../target/stm32/malloc/mallocr.c ****        linked with the most recently freed at the front, and allocations
1369:../target/stm32/malloc/mallocr.c ****        are taken from the back.  This results in LRU or FIFO allocation
1370:../target/stm32/malloc/mallocr.c ****        order, which tends to give each chunk an equal opportunity to be
1371:../target/stm32/malloc/mallocr.c ****        consolidated with adjacent freed chunks, resulting in larger free
1372:../target/stm32/malloc/mallocr.c ****        chunks and less fragmentation. 
1373:../target/stm32/malloc/mallocr.c **** 
1374:../target/stm32/malloc/mallocr.c ****     * `top': The top-most available chunk (i.e., the one bordering the
1375:../target/stm32/malloc/mallocr.c ****        end of available memory) is treated specially. It is never
1376:../target/stm32/malloc/mallocr.c ****        included in any bin, is used only if no other chunk is
1377:../target/stm32/malloc/mallocr.c ****        available, and is released back to the system if it is very
1378:../target/stm32/malloc/mallocr.c ****        large (see M_TRIM_THRESHOLD).
1379:../target/stm32/malloc/mallocr.c **** 
1380:../target/stm32/malloc/mallocr.c ****     * `last_remainder': A bin holding only the remainder of the
1381:../target/stm32/malloc/mallocr.c ****        most recently split (non-top) chunk. This bin is checked
1382:../target/stm32/malloc/mallocr.c ****        before other non-fitting chunks, so as to provide better
1383:../target/stm32/malloc/mallocr.c ****        locality for runs of sequentially allocated chunks. 
1384:../target/stm32/malloc/mallocr.c **** 
1385:../target/stm32/malloc/mallocr.c ****     *  Implicitly, through the host system's memory mapping tables.
1386:../target/stm32/malloc/mallocr.c ****        If supported, requests greater than a threshold are usually 
1387:../target/stm32/malloc/mallocr.c ****        serviced via calls to mmap, and then later released via munmap.
1388:../target/stm32/malloc/mallocr.c **** 
1389:../target/stm32/malloc/mallocr.c **** */
1390:../target/stm32/malloc/mallocr.c **** 
1391:../target/stm32/malloc/mallocr.c **** 
1392:../target/stm32/malloc/mallocr.c **** 
1393:../target/stm32/malloc/mallocr.c **** 
1394:../target/stm32/malloc/mallocr.c **** 
1395:../target/stm32/malloc/mallocr.c **** 
1396:../target/stm32/malloc/mallocr.c **** /*  sizes, alignments */
1397:../target/stm32/malloc/mallocr.c **** 
1398:../target/stm32/malloc/mallocr.c **** #define SIZE_SZ                (sizeof(INTERNAL_SIZE_T))
1399:../target/stm32/malloc/mallocr.c **** #ifndef MALLOC_ALIGNMENT
1400:../target/stm32/malloc/mallocr.c **** #define MALLOC_ALIGN           8
1401:../target/stm32/malloc/mallocr.c **** #define MALLOC_ALIGNMENT       (SIZE_SZ < 4 ? 8 : (SIZE_SZ + SIZE_SZ))
1402:../target/stm32/malloc/mallocr.c **** #else
1403:../target/stm32/malloc/mallocr.c **** #define MALLOC_ALIGN           MALLOC_ALIGNMENT
1404:../target/stm32/malloc/mallocr.c **** #endif
1405:../target/stm32/malloc/mallocr.c **** #define MALLOC_ALIGN_MASK      (MALLOC_ALIGNMENT - 1)
1406:../target/stm32/malloc/mallocr.c **** #define MINSIZE                (sizeof(struct malloc_chunk))
1407:../target/stm32/malloc/mallocr.c **** 
1408:../target/stm32/malloc/mallocr.c **** /* conversion from malloc headers to user pointers, and back */
1409:../target/stm32/malloc/mallocr.c **** 
1410:../target/stm32/malloc/mallocr.c **** #define chunk2mem(p)   ((Void_t*)((char*)(p) + 2*SIZE_SZ))
1411:../target/stm32/malloc/mallocr.c **** #define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))
1412:../target/stm32/malloc/mallocr.c **** 
1413:../target/stm32/malloc/mallocr.c **** /* pad request bytes into a usable size */
1414:../target/stm32/malloc/mallocr.c **** 
1415:../target/stm32/malloc/mallocr.c **** #define request2size(req) \
1416:../target/stm32/malloc/mallocr.c ****  (((unsigned long)((req) + (SIZE_SZ + MALLOC_ALIGN_MASK)) < \
1417:../target/stm32/malloc/mallocr.c ****   (unsigned long)(MINSIZE + MALLOC_ALIGN_MASK)) ? ((MINSIZE + MALLOC_ALIGN_MASK) & ~(MALLOC_ALIGN_M
1418:../target/stm32/malloc/mallocr.c ****    (((req) + (SIZE_SZ + MALLOC_ALIGN_MASK)) & ~(MALLOC_ALIGN_MASK)))
1419:../target/stm32/malloc/mallocr.c **** 
1420:../target/stm32/malloc/mallocr.c **** /* Check if m has acceptable alignment */
1421:../target/stm32/malloc/mallocr.c **** 
1422:../target/stm32/malloc/mallocr.c **** #define aligned_OK(m)    (((unsigned long)((m)) & (MALLOC_ALIGN_MASK)) == 0)
1423:../target/stm32/malloc/mallocr.c **** 
1424:../target/stm32/malloc/mallocr.c **** 
1425:../target/stm32/malloc/mallocr.c **** 
1426:../target/stm32/malloc/mallocr.c **** 
1427:../target/stm32/malloc/mallocr.c **** /* 
1428:../target/stm32/malloc/mallocr.c ****   Physical chunk operations  
1429:../target/stm32/malloc/mallocr.c **** */
1430:../target/stm32/malloc/mallocr.c **** 
1431:../target/stm32/malloc/mallocr.c **** 
1432:../target/stm32/malloc/mallocr.c **** /* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
1433:../target/stm32/malloc/mallocr.c **** 
1434:../target/stm32/malloc/mallocr.c **** #define PREV_INUSE 0x1 
1435:../target/stm32/malloc/mallocr.c **** 
1436:../target/stm32/malloc/mallocr.c **** /* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
1437:../target/stm32/malloc/mallocr.c **** 
1438:../target/stm32/malloc/mallocr.c **** #define IS_MMAPPED 0x2
1439:../target/stm32/malloc/mallocr.c **** 
1440:../target/stm32/malloc/mallocr.c **** /* Bits to mask off when extracting size */
1441:../target/stm32/malloc/mallocr.c **** 
1442:../target/stm32/malloc/mallocr.c **** #define SIZE_BITS (PREV_INUSE|IS_MMAPPED)
1443:../target/stm32/malloc/mallocr.c **** 
1444:../target/stm32/malloc/mallocr.c **** 
1445:../target/stm32/malloc/mallocr.c **** /* Ptr to next physical malloc_chunk. */
1446:../target/stm32/malloc/mallocr.c **** 
1447:../target/stm32/malloc/mallocr.c **** #define next_chunk(p) ((mchunkptr)( ((char*)(p)) + ((p)->size & ~PREV_INUSE) ))
1448:../target/stm32/malloc/mallocr.c **** 
1449:../target/stm32/malloc/mallocr.c **** /* Ptr to previous physical malloc_chunk */
1450:../target/stm32/malloc/mallocr.c **** 
1451:../target/stm32/malloc/mallocr.c **** #define prev_chunk(p)\
1452:../target/stm32/malloc/mallocr.c ****    ((mchunkptr)( ((char*)(p)) - ((p)->prev_size) ))
1453:../target/stm32/malloc/mallocr.c **** 
1454:../target/stm32/malloc/mallocr.c **** 
1455:../target/stm32/malloc/mallocr.c **** /* Treat space at ptr + offset as a chunk */
1456:../target/stm32/malloc/mallocr.c **** 
1457:../target/stm32/malloc/mallocr.c **** #define chunk_at_offset(p, s)  ((mchunkptr)(((char*)(p)) + (s)))
1458:../target/stm32/malloc/mallocr.c **** 
1459:../target/stm32/malloc/mallocr.c **** 
1460:../target/stm32/malloc/mallocr.c **** 
1461:../target/stm32/malloc/mallocr.c **** 
1462:../target/stm32/malloc/mallocr.c **** /* 
1463:../target/stm32/malloc/mallocr.c ****   Dealing with use bits 
1464:../target/stm32/malloc/mallocr.c **** */
1465:../target/stm32/malloc/mallocr.c **** 
1466:../target/stm32/malloc/mallocr.c **** /* extract p's inuse bit */
1467:../target/stm32/malloc/mallocr.c **** 
1468:../target/stm32/malloc/mallocr.c **** #define inuse(p)\
1469:../target/stm32/malloc/mallocr.c **** ((((mchunkptr)(((char*)(p))+((p)->size & ~PREV_INUSE)))->size) & PREV_INUSE)
1470:../target/stm32/malloc/mallocr.c **** 
1471:../target/stm32/malloc/mallocr.c **** /* extract inuse bit of previous chunk */
1472:../target/stm32/malloc/mallocr.c **** 
1473:../target/stm32/malloc/mallocr.c **** #define prev_inuse(p)  ((p)->size & PREV_INUSE)
1474:../target/stm32/malloc/mallocr.c **** 
1475:../target/stm32/malloc/mallocr.c **** /* check for mmap()'ed chunk */
1476:../target/stm32/malloc/mallocr.c **** 
1477:../target/stm32/malloc/mallocr.c **** #define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)
1478:../target/stm32/malloc/mallocr.c **** 
1479:../target/stm32/malloc/mallocr.c **** /* set/clear chunk as in use without otherwise disturbing */
1480:../target/stm32/malloc/mallocr.c **** 
1481:../target/stm32/malloc/mallocr.c **** #define set_inuse(p)\
1482:../target/stm32/malloc/mallocr.c **** ((mchunkptr)(((char*)(p)) + ((p)->size & ~PREV_INUSE)))->size |= PREV_INUSE
1483:../target/stm32/malloc/mallocr.c **** 
1484:../target/stm32/malloc/mallocr.c **** #define clear_inuse(p)\
1485:../target/stm32/malloc/mallocr.c **** ((mchunkptr)(((char*)(p)) + ((p)->size & ~PREV_INUSE)))->size &= ~(PREV_INUSE)
1486:../target/stm32/malloc/mallocr.c **** 
1487:../target/stm32/malloc/mallocr.c **** /* check/set/clear inuse bits in known places */
1488:../target/stm32/malloc/mallocr.c **** 
1489:../target/stm32/malloc/mallocr.c **** #define inuse_bit_at_offset(p, s)\
1490:../target/stm32/malloc/mallocr.c ****  (((mchunkptr)(((char*)(p)) + (s)))->size & PREV_INUSE)
1491:../target/stm32/malloc/mallocr.c **** 
1492:../target/stm32/malloc/mallocr.c **** #define set_inuse_bit_at_offset(p, s)\
1493:../target/stm32/malloc/mallocr.c ****  (((mchunkptr)(((char*)(p)) + (s)))->size |= PREV_INUSE)
1494:../target/stm32/malloc/mallocr.c **** 
1495:../target/stm32/malloc/mallocr.c **** #define clear_inuse_bit_at_offset(p, s)\
1496:../target/stm32/malloc/mallocr.c ****  (((mchunkptr)(((char*)(p)) + (s)))->size &= ~(PREV_INUSE))
1497:../target/stm32/malloc/mallocr.c **** 
1498:../target/stm32/malloc/mallocr.c **** 
1499:../target/stm32/malloc/mallocr.c **** 
1500:../target/stm32/malloc/mallocr.c **** 
1501:../target/stm32/malloc/mallocr.c **** /* 
1502:../target/stm32/malloc/mallocr.c ****   Dealing with size fields 
1503:../target/stm32/malloc/mallocr.c **** */
1504:../target/stm32/malloc/mallocr.c **** 
1505:../target/stm32/malloc/mallocr.c **** /* Get size, ignoring use bits */
1506:../target/stm32/malloc/mallocr.c **** 
1507:../target/stm32/malloc/mallocr.c **** #define chunksize(p)          ((p)->size & ~(SIZE_BITS))
1508:../target/stm32/malloc/mallocr.c **** 
1509:../target/stm32/malloc/mallocr.c **** /* Set size at head, without disturbing its use bit */
1510:../target/stm32/malloc/mallocr.c **** 
1511:../target/stm32/malloc/mallocr.c **** #define set_head_size(p, s)   ((p)->size = (((p)->size & PREV_INUSE) | (s)))
1512:../target/stm32/malloc/mallocr.c **** 
1513:../target/stm32/malloc/mallocr.c **** /* Set size/use ignoring previous bits in header */
1514:../target/stm32/malloc/mallocr.c **** 
1515:../target/stm32/malloc/mallocr.c **** #define set_head(p, s)        ((p)->size = (s))
1516:../target/stm32/malloc/mallocr.c **** 
1517:../target/stm32/malloc/mallocr.c **** /* Set size at footer (only when chunk is not in use) */
1518:../target/stm32/malloc/mallocr.c **** 
1519:../target/stm32/malloc/mallocr.c **** #define set_foot(p, s)   (((mchunkptr)((char*)(p) + (s)))->prev_size = (s))
1520:../target/stm32/malloc/mallocr.c **** 
1521:../target/stm32/malloc/mallocr.c **** 
1522:../target/stm32/malloc/mallocr.c **** 
1523:../target/stm32/malloc/mallocr.c **** 
1524:../target/stm32/malloc/mallocr.c **** 
1525:../target/stm32/malloc/mallocr.c **** /*
1526:../target/stm32/malloc/mallocr.c ****    Bins
1527:../target/stm32/malloc/mallocr.c **** 
1528:../target/stm32/malloc/mallocr.c ****     The bins, `av_' are an array of pairs of pointers serving as the
1529:../target/stm32/malloc/mallocr.c ****     heads of (initially empty) doubly-linked lists of chunks, laid out
1530:../target/stm32/malloc/mallocr.c ****     in a way so that each pair can be treated as if it were in a
1531:../target/stm32/malloc/mallocr.c ****     malloc_chunk. (This way, the fd/bk offsets for linking bin heads
1532:../target/stm32/malloc/mallocr.c ****     and chunks are the same).
1533:../target/stm32/malloc/mallocr.c **** 
1534:../target/stm32/malloc/mallocr.c ****     Bins for sizes < 512 bytes contain chunks of all the same size, spaced
1535:../target/stm32/malloc/mallocr.c ****     8 bytes apart. Larger bins are approximately logarithmically
1536:../target/stm32/malloc/mallocr.c ****     spaced. (See the table below.) The `av_' array is never mentioned
1537:../target/stm32/malloc/mallocr.c ****     directly in the code, but instead via bin access macros.
1538:../target/stm32/malloc/mallocr.c **** 
1539:../target/stm32/malloc/mallocr.c ****     Bin layout:
1540:../target/stm32/malloc/mallocr.c **** 
1541:../target/stm32/malloc/mallocr.c ****     64 bins of size       8
1542:../target/stm32/malloc/mallocr.c ****     32 bins of size      64
1543:../target/stm32/malloc/mallocr.c ****     16 bins of size     512
1544:../target/stm32/malloc/mallocr.c ****      8 bins of size    4096
1545:../target/stm32/malloc/mallocr.c ****      4 bins of size   32768
1546:../target/stm32/malloc/mallocr.c ****      2 bins of size  262144
1547:../target/stm32/malloc/mallocr.c ****      1 bin  of size what's left
1548:../target/stm32/malloc/mallocr.c **** 
1549:../target/stm32/malloc/mallocr.c ****     There is actually a little bit of slop in the numbers in bin_index
1550:../target/stm32/malloc/mallocr.c ****     for the sake of speed. This makes no difference elsewhere.
1551:../target/stm32/malloc/mallocr.c **** 
1552:../target/stm32/malloc/mallocr.c ****     The special chunks `top' and `last_remainder' get their own bins,
1553:../target/stm32/malloc/mallocr.c ****     (this is implemented via yet more trickery with the av_ array),
1554:../target/stm32/malloc/mallocr.c ****     although `top' is never properly linked to its bin since it is
1555:../target/stm32/malloc/mallocr.c ****     always handled specially.
1556:../target/stm32/malloc/mallocr.c **** 
1557:../target/stm32/malloc/mallocr.c **** */
1558:../target/stm32/malloc/mallocr.c **** 
1559:../target/stm32/malloc/mallocr.c **** #ifdef SEPARATE_OBJECTS
1560:../target/stm32/malloc/mallocr.c **** #define av_ malloc_av_
1561:../target/stm32/malloc/mallocr.c **** #endif
1562:../target/stm32/malloc/mallocr.c **** 
1563:../target/stm32/malloc/mallocr.c **** #define NAV             128   /* number of bins */
1564:../target/stm32/malloc/mallocr.c **** 
1565:../target/stm32/malloc/mallocr.c **** typedef struct malloc_chunk* mbinptr;
1566:../target/stm32/malloc/mallocr.c **** 
1567:../target/stm32/malloc/mallocr.c **** /* access macros */
1568:../target/stm32/malloc/mallocr.c **** 
1569:../target/stm32/malloc/mallocr.c **** #define bin_at(i)      ((mbinptr)((char*)&(av_[2*(i) + 2]) - 2*SIZE_SZ))
1570:../target/stm32/malloc/mallocr.c **** #define next_bin(b)    ((mbinptr)((char*)(b) + 2 * sizeof(mbinptr)))
1571:../target/stm32/malloc/mallocr.c **** #define prev_bin(b)    ((mbinptr)((char*)(b) - 2 * sizeof(mbinptr)))
1572:../target/stm32/malloc/mallocr.c **** 
1573:../target/stm32/malloc/mallocr.c **** /*
1574:../target/stm32/malloc/mallocr.c ****    The first 2 bins are never indexed. The corresponding av_ cells are instead
1575:../target/stm32/malloc/mallocr.c ****    used for bookkeeping. This is not to save space, but to simplify
1576:../target/stm32/malloc/mallocr.c ****    indexing, maintain locality, and avoid some initialization tests.
1577:../target/stm32/malloc/mallocr.c **** */
1578:../target/stm32/malloc/mallocr.c **** 
1579:../target/stm32/malloc/mallocr.c **** #define top            (bin_at(0)->fd)   /* The topmost chunk */
1580:../target/stm32/malloc/mallocr.c **** #define last_remainder (bin_at(1))       /* remainder from last split */
1581:../target/stm32/malloc/mallocr.c **** 
1582:../target/stm32/malloc/mallocr.c **** 
1583:../target/stm32/malloc/mallocr.c **** /*
1584:../target/stm32/malloc/mallocr.c ****    Because top initially points to its own bin with initial
1585:../target/stm32/malloc/mallocr.c ****    zero size, thus forcing extension on the first malloc request, 
1586:../target/stm32/malloc/mallocr.c ****    we avoid having any special code in malloc to check whether 
1587:../target/stm32/malloc/mallocr.c ****    it even exists yet. But we still need to in malloc_extend_top.
1588:../target/stm32/malloc/mallocr.c **** */
1589:../target/stm32/malloc/mallocr.c **** 
1590:../target/stm32/malloc/mallocr.c **** #define initial_top    ((mchunkptr)(bin_at(0)))
1591:../target/stm32/malloc/mallocr.c **** 
1592:../target/stm32/malloc/mallocr.c **** /* Helper macro to initialize bins */
1593:../target/stm32/malloc/mallocr.c **** 
1594:../target/stm32/malloc/mallocr.c **** #define IAV(i)  bin_at(i), bin_at(i)
1595:../target/stm32/malloc/mallocr.c **** 
1596:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_MALLOC
1597:../target/stm32/malloc/mallocr.c **** STATIC mbinptr av_[NAV * 2 + 2] = {
1598:../target/stm32/malloc/mallocr.c ****  0, 0,
1599:../target/stm32/malloc/mallocr.c ****  IAV(0),   IAV(1),   IAV(2),   IAV(3),   IAV(4),   IAV(5),   IAV(6),   IAV(7),
1600:../target/stm32/malloc/mallocr.c ****  IAV(8),   IAV(9),   IAV(10),  IAV(11),  IAV(12),  IAV(13),  IAV(14),  IAV(15),
1601:../target/stm32/malloc/mallocr.c ****  IAV(16),  IAV(17),  IAV(18),  IAV(19),  IAV(20),  IAV(21),  IAV(22),  IAV(23),
1602:../target/stm32/malloc/mallocr.c ****  IAV(24),  IAV(25),  IAV(26),  IAV(27),  IAV(28),  IAV(29),  IAV(30),  IAV(31),
1603:../target/stm32/malloc/mallocr.c ****  IAV(32),  IAV(33),  IAV(34),  IAV(35),  IAV(36),  IAV(37),  IAV(38),  IAV(39),
1604:../target/stm32/malloc/mallocr.c ****  IAV(40),  IAV(41),  IAV(42),  IAV(43),  IAV(44),  IAV(45),  IAV(46),  IAV(47),
1605:../target/stm32/malloc/mallocr.c ****  IAV(48),  IAV(49),  IAV(50),  IAV(51),  IAV(52),  IAV(53),  IAV(54),  IAV(55),
1606:../target/stm32/malloc/mallocr.c ****  IAV(56),  IAV(57),  IAV(58),  IAV(59),  IAV(60),  IAV(61),  IAV(62),  IAV(63),
1607:../target/stm32/malloc/mallocr.c ****  IAV(64),  IAV(65),  IAV(66),  IAV(67),  IAV(68),  IAV(69),  IAV(70),  IAV(71),
1608:../target/stm32/malloc/mallocr.c ****  IAV(72),  IAV(73),  IAV(74),  IAV(75),  IAV(76),  IAV(77),  IAV(78),  IAV(79),
1609:../target/stm32/malloc/mallocr.c ****  IAV(80),  IAV(81),  IAV(82),  IAV(83),  IAV(84),  IAV(85),  IAV(86),  IAV(87),
1610:../target/stm32/malloc/mallocr.c ****  IAV(88),  IAV(89),  IAV(90),  IAV(91),  IAV(92),  IAV(93),  IAV(94),  IAV(95),
1611:../target/stm32/malloc/mallocr.c ****  IAV(96),  IAV(97),  IAV(98),  IAV(99),  IAV(100), IAV(101), IAV(102), IAV(103),
1612:../target/stm32/malloc/mallocr.c ****  IAV(104), IAV(105), IAV(106), IAV(107), IAV(108), IAV(109), IAV(110), IAV(111),
1613:../target/stm32/malloc/mallocr.c ****  IAV(112), IAV(113), IAV(114), IAV(115), IAV(116), IAV(117), IAV(118), IAV(119),
1614:../target/stm32/malloc/mallocr.c ****  IAV(120), IAV(121), IAV(122), IAV(123), IAV(124), IAV(125), IAV(126), IAV(127)
1615:../target/stm32/malloc/mallocr.c **** };
1616:../target/stm32/malloc/mallocr.c **** #else
1617:../target/stm32/malloc/mallocr.c **** extern mbinptr av_[NAV * 2 + 2];
1618:../target/stm32/malloc/mallocr.c **** #endif
1619:../target/stm32/malloc/mallocr.c **** 
1620:../target/stm32/malloc/mallocr.c **** 
1621:../target/stm32/malloc/mallocr.c **** 
1622:../target/stm32/malloc/mallocr.c **** /* field-extraction macros */
1623:../target/stm32/malloc/mallocr.c **** 
1624:../target/stm32/malloc/mallocr.c **** #define first(b) ((b)->fd)
1625:../target/stm32/malloc/mallocr.c **** #define last(b)  ((b)->bk)
1626:../target/stm32/malloc/mallocr.c **** 
1627:../target/stm32/malloc/mallocr.c **** /* 
1628:../target/stm32/malloc/mallocr.c ****   Indexing into bins
1629:../target/stm32/malloc/mallocr.c **** */
1630:../target/stm32/malloc/mallocr.c **** 
1631:../target/stm32/malloc/mallocr.c **** #define bin_index(sz)                                                          \
1632:../target/stm32/malloc/mallocr.c **** (((((unsigned long)(sz)) >> 9) ==    0) ?       (((unsigned long)(sz)) >>  3): \
1633:../target/stm32/malloc/mallocr.c ****  ((((unsigned long)(sz)) >> 9) <=    4) ?  56 + (((unsigned long)(sz)) >>  6): \
1634:../target/stm32/malloc/mallocr.c ****  ((((unsigned long)(sz)) >> 9) <=   20) ?  91 + (((unsigned long)(sz)) >>  9): \
1635:../target/stm32/malloc/mallocr.c ****  ((((unsigned long)(sz)) >> 9) <=   84) ? 110 + (((unsigned long)(sz)) >> 12): \
1636:../target/stm32/malloc/mallocr.c ****  ((((unsigned long)(sz)) >> 9) <=  340) ? 119 + (((unsigned long)(sz)) >> 15): \
1637:../target/stm32/malloc/mallocr.c ****  ((((unsigned long)(sz)) >> 9) <= 1364) ? 124 + (((unsigned long)(sz)) >> 18): \
1638:../target/stm32/malloc/mallocr.c ****                                           126)                     
1639:../target/stm32/malloc/mallocr.c **** /* 
1640:../target/stm32/malloc/mallocr.c ****   bins for chunks < 512 are all spaced SMALLBIN_WIDTH bytes apart, and hold
1641:../target/stm32/malloc/mallocr.c ****   identically sized chunks. This is exploited in malloc.
1642:../target/stm32/malloc/mallocr.c **** */
1643:../target/stm32/malloc/mallocr.c **** 
1644:../target/stm32/malloc/mallocr.c **** #define MAX_SMALLBIN_SIZE   512
1645:../target/stm32/malloc/mallocr.c **** #define SMALLBIN_WIDTH        8
1646:../target/stm32/malloc/mallocr.c **** #define SMALLBIN_WIDTH_BITS   3
1647:../target/stm32/malloc/mallocr.c **** #define MAX_SMALLBIN        (MAX_SMALLBIN_SIZE / SMALLBIN_WIDTH) - 1
1648:../target/stm32/malloc/mallocr.c **** 
1649:../target/stm32/malloc/mallocr.c **** #define smallbin_index(sz)  (((unsigned long)(sz)) >> SMALLBIN_WIDTH_BITS)
1650:../target/stm32/malloc/mallocr.c **** 
1651:../target/stm32/malloc/mallocr.c **** /* 
1652:../target/stm32/malloc/mallocr.c ****    Requests are `small' if both the corresponding and the next bin are small
1653:../target/stm32/malloc/mallocr.c **** */
1654:../target/stm32/malloc/mallocr.c **** 
1655:../target/stm32/malloc/mallocr.c **** #define is_small_request(nb) (nb < MAX_SMALLBIN_SIZE - SMALLBIN_WIDTH)
1656:../target/stm32/malloc/mallocr.c **** 
1657:../target/stm32/malloc/mallocr.c **** 
1658:../target/stm32/malloc/mallocr.c **** 
1659:../target/stm32/malloc/mallocr.c **** /*
1660:../target/stm32/malloc/mallocr.c ****     To help compensate for the large number of bins, a one-level index
1661:../target/stm32/malloc/mallocr.c ****     structure is used for bin-by-bin searching.  `binblocks' is a
1662:../target/stm32/malloc/mallocr.c ****     one-word bitvector recording whether groups of BINBLOCKWIDTH bins
1663:../target/stm32/malloc/mallocr.c ****     have any (possibly) non-empty bins, so they can be skipped over
1664:../target/stm32/malloc/mallocr.c ****     all at once during during traversals. The bits are NOT always
1665:../target/stm32/malloc/mallocr.c ****     cleared as soon as all bins in a block are empty, but instead only
1666:../target/stm32/malloc/mallocr.c ****     when all are noticed to be empty during traversal in malloc.
1667:../target/stm32/malloc/mallocr.c **** */
1668:../target/stm32/malloc/mallocr.c **** 
1669:../target/stm32/malloc/mallocr.c **** #define BINBLOCKWIDTH     4   /* bins per block */
1670:../target/stm32/malloc/mallocr.c **** 
1671:../target/stm32/malloc/mallocr.c **** #define binblocks      (bin_at(0)->size) /* bitvector of nonempty blocks */
1672:../target/stm32/malloc/mallocr.c **** 
1673:../target/stm32/malloc/mallocr.c **** /* bin<->block macros */
1674:../target/stm32/malloc/mallocr.c **** 
1675:../target/stm32/malloc/mallocr.c **** #define idx2binblock(ix)    ((unsigned long)1 << (ix / BINBLOCKWIDTH))
1676:../target/stm32/malloc/mallocr.c **** #define mark_binblock(ii)   (binblocks |= idx2binblock(ii))
1677:../target/stm32/malloc/mallocr.c **** #define clear_binblock(ii)  (binblocks &= ~(idx2binblock(ii)))
1678:../target/stm32/malloc/mallocr.c **** 
1679:../target/stm32/malloc/mallocr.c **** 
1680:../target/stm32/malloc/mallocr.c **** 
1681:../target/stm32/malloc/mallocr.c **** 
1682:../target/stm32/malloc/mallocr.c **** 
1683:../target/stm32/malloc/mallocr.c **** /*  Other static bookkeeping data */
1684:../target/stm32/malloc/mallocr.c **** 
1685:../target/stm32/malloc/mallocr.c **** #ifdef SEPARATE_OBJECTS
1686:../target/stm32/malloc/mallocr.c **** #define trim_threshold		malloc_trim_threshold
1687:../target/stm32/malloc/mallocr.c **** #define top_pad			malloc_top_pad
1688:../target/stm32/malloc/mallocr.c **** #define n_mmaps_max		malloc_n_mmaps_max
1689:../target/stm32/malloc/mallocr.c **** #define mmap_threshold		malloc_mmap_threshold
1690:../target/stm32/malloc/mallocr.c **** #define sbrk_base		malloc_sbrk_base
1691:../target/stm32/malloc/mallocr.c **** #define max_sbrked_mem		malloc_max_sbrked_mem
1692:../target/stm32/malloc/mallocr.c **** #define max_total_mem		malloc_max_total_mem
1693:../target/stm32/malloc/mallocr.c **** #define current_mallinfo	malloc_current_mallinfo
1694:../target/stm32/malloc/mallocr.c **** #define n_mmaps			malloc_n_mmaps
1695:../target/stm32/malloc/mallocr.c **** #define max_n_mmaps		malloc_max_n_mmaps
1696:../target/stm32/malloc/mallocr.c **** #define mmapped_mem		malloc_mmapped_mem
1697:../target/stm32/malloc/mallocr.c **** #define max_mmapped_mem		malloc_max_mmapped_mem
1698:../target/stm32/malloc/mallocr.c **** #endif
1699:../target/stm32/malloc/mallocr.c **** 
1700:../target/stm32/malloc/mallocr.c **** /* variables holding tunable values */
1701:../target/stm32/malloc/mallocr.c **** 
1702:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_MALLOC
1703:../target/stm32/malloc/mallocr.c **** 
1704:../target/stm32/malloc/mallocr.c **** STATIC unsigned long trim_threshold   = DEFAULT_TRIM_THRESHOLD;
1705:../target/stm32/malloc/mallocr.c **** STATIC unsigned long top_pad          = DEFAULT_TOP_PAD;
1706:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
1707:../target/stm32/malloc/mallocr.c **** STATIC unsigned int  n_mmaps_max      = DEFAULT_MMAP_MAX;
1708:../target/stm32/malloc/mallocr.c **** STATIC unsigned long mmap_threshold   = DEFAULT_MMAP_THRESHOLD;
1709:../target/stm32/malloc/mallocr.c **** #endif
1710:../target/stm32/malloc/mallocr.c **** 
1711:../target/stm32/malloc/mallocr.c **** /* The first value returned from sbrk */
1712:../target/stm32/malloc/mallocr.c **** STATIC char* sbrk_base = (char*)(-1);
1713:../target/stm32/malloc/mallocr.c **** 
1714:../target/stm32/malloc/mallocr.c **** /* The maximum memory obtained from system via sbrk */
1715:../target/stm32/malloc/mallocr.c **** STATIC unsigned long max_sbrked_mem = 0; 
1716:../target/stm32/malloc/mallocr.c **** 
1717:../target/stm32/malloc/mallocr.c **** /* The maximum via either sbrk or mmap */
1718:../target/stm32/malloc/mallocr.c **** STATIC unsigned long max_total_mem = 0; 
1719:../target/stm32/malloc/mallocr.c **** 
1720:../target/stm32/malloc/mallocr.c **** /* internal working copy of mallinfo */
1721:../target/stm32/malloc/mallocr.c **** STATIC struct mallinfo current_mallinfo = {  0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
1722:../target/stm32/malloc/mallocr.c **** 
1723:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
1724:../target/stm32/malloc/mallocr.c **** 
1725:../target/stm32/malloc/mallocr.c **** /* Tracking mmaps */
1726:../target/stm32/malloc/mallocr.c **** 
1727:../target/stm32/malloc/mallocr.c **** STATIC unsigned int n_mmaps = 0;
1728:../target/stm32/malloc/mallocr.c **** STATIC unsigned int max_n_mmaps = 0;
1729:../target/stm32/malloc/mallocr.c **** STATIC unsigned long mmapped_mem = 0;
1730:../target/stm32/malloc/mallocr.c **** STATIC unsigned long max_mmapped_mem = 0;
1731:../target/stm32/malloc/mallocr.c **** 
1732:../target/stm32/malloc/mallocr.c **** #endif
1733:../target/stm32/malloc/mallocr.c **** 
1734:../target/stm32/malloc/mallocr.c **** #else /* ! DEFINE_MALLOC */
1735:../target/stm32/malloc/mallocr.c **** 
1736:../target/stm32/malloc/mallocr.c **** extern unsigned long trim_threshold;
1737:../target/stm32/malloc/mallocr.c **** extern unsigned long top_pad;
1738:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
1739:../target/stm32/malloc/mallocr.c **** extern unsigned int  n_mmaps_max;
1740:../target/stm32/malloc/mallocr.c **** extern unsigned long mmap_threshold;
1741:../target/stm32/malloc/mallocr.c **** #endif
1742:../target/stm32/malloc/mallocr.c **** extern char* sbrk_base;
1743:../target/stm32/malloc/mallocr.c **** extern unsigned long max_sbrked_mem;
1744:../target/stm32/malloc/mallocr.c **** extern unsigned long max_total_mem;
1745:../target/stm32/malloc/mallocr.c **** extern struct mallinfo current_mallinfo;
1746:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
1747:../target/stm32/malloc/mallocr.c **** extern unsigned int n_mmaps;
1748:../target/stm32/malloc/mallocr.c **** extern unsigned int max_n_mmaps;
1749:../target/stm32/malloc/mallocr.c **** extern unsigned long mmapped_mem;
1750:../target/stm32/malloc/mallocr.c **** extern unsigned long max_mmapped_mem;
1751:../target/stm32/malloc/mallocr.c **** #endif
1752:../target/stm32/malloc/mallocr.c **** 
1753:../target/stm32/malloc/mallocr.c **** #endif /* ! DEFINE_MALLOC */
1754:../target/stm32/malloc/mallocr.c **** 
1755:../target/stm32/malloc/mallocr.c **** /* The total memory obtained from system via sbrk */
1756:../target/stm32/malloc/mallocr.c **** #define sbrked_mem  (current_mallinfo.arena)
1757:../target/stm32/malloc/mallocr.c **** 
1758:../target/stm32/malloc/mallocr.c **** 
1759:../target/stm32/malloc/mallocr.c **** 
1760:../target/stm32/malloc/mallocr.c **** /* 
1761:../target/stm32/malloc/mallocr.c ****   Debugging support 
1762:../target/stm32/malloc/mallocr.c **** */
1763:../target/stm32/malloc/mallocr.c **** 
1764:../target/stm32/malloc/mallocr.c **** #if DEBUG
1765:../target/stm32/malloc/mallocr.c **** 
1766:../target/stm32/malloc/mallocr.c **** 
1767:../target/stm32/malloc/mallocr.c **** /*
1768:../target/stm32/malloc/mallocr.c ****   These routines make a number of assertions about the states
1769:../target/stm32/malloc/mallocr.c ****   of data structures that should be true at all times. If any
1770:../target/stm32/malloc/mallocr.c ****   are not true, it's very likely that a user program has somehow
1771:../target/stm32/malloc/mallocr.c ****   trashed memory. (It's also possible that there is a coding error
1772:../target/stm32/malloc/mallocr.c ****   in malloc. In which case, please report it!)
1773:../target/stm32/malloc/mallocr.c **** */
1774:../target/stm32/malloc/mallocr.c **** 
1775:../target/stm32/malloc/mallocr.c **** #if __STD_C
1776:../target/stm32/malloc/mallocr.c **** static void do_check_chunk(mchunkptr p) 
1777:../target/stm32/malloc/mallocr.c **** #else
1778:../target/stm32/malloc/mallocr.c **** static void do_check_chunk(p) mchunkptr p;
1779:../target/stm32/malloc/mallocr.c **** #endif
1780:../target/stm32/malloc/mallocr.c **** { 
1781:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T sz = p->size & ~PREV_INUSE;
1782:../target/stm32/malloc/mallocr.c **** 
1783:../target/stm32/malloc/mallocr.c ****   /* No checkable chunk is mmapped */
1784:../target/stm32/malloc/mallocr.c ****   assert(!chunk_is_mmapped(p));
1785:../target/stm32/malloc/mallocr.c **** 
1786:../target/stm32/malloc/mallocr.c ****   /* Check for legal address ... */
1787:../target/stm32/malloc/mallocr.c ****   assert((char*)p >= sbrk_base);
1788:../target/stm32/malloc/mallocr.c ****   if (p != top) 
1789:../target/stm32/malloc/mallocr.c ****     assert((char*)p + sz <= (char*)top);
1790:../target/stm32/malloc/mallocr.c ****   else
1791:../target/stm32/malloc/mallocr.c ****     assert((char*)p + sz <= sbrk_base + sbrked_mem);
1792:../target/stm32/malloc/mallocr.c **** 
1793:../target/stm32/malloc/mallocr.c **** }
1794:../target/stm32/malloc/mallocr.c **** 
1795:../target/stm32/malloc/mallocr.c **** 
1796:../target/stm32/malloc/mallocr.c **** #if __STD_C
1797:../target/stm32/malloc/mallocr.c **** static void do_check_free_chunk(mchunkptr p) 
1798:../target/stm32/malloc/mallocr.c **** #else
1799:../target/stm32/malloc/mallocr.c **** static void do_check_free_chunk(p) mchunkptr p;
1800:../target/stm32/malloc/mallocr.c **** #endif
1801:../target/stm32/malloc/mallocr.c **** { 
1802:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T sz = p->size & ~PREV_INUSE;
1803:../target/stm32/malloc/mallocr.c ****   mchunkptr next = chunk_at_offset(p, sz);
1804:../target/stm32/malloc/mallocr.c **** 
1805:../target/stm32/malloc/mallocr.c ****   do_check_chunk(p);
1806:../target/stm32/malloc/mallocr.c **** 
1807:../target/stm32/malloc/mallocr.c ****   /* Check whether it claims to be free ... */
1808:../target/stm32/malloc/mallocr.c ****   assert(!inuse(p));
1809:../target/stm32/malloc/mallocr.c **** 
1810:../target/stm32/malloc/mallocr.c ****   /* Unless a special marker, must have OK fields */
1811:../target/stm32/malloc/mallocr.c ****   if ((long)sz >= (long)MINSIZE)
1812:../target/stm32/malloc/mallocr.c ****   {
1813:../target/stm32/malloc/mallocr.c ****     assert((sz & MALLOC_ALIGN_MASK) == 0);
1814:../target/stm32/malloc/mallocr.c ****     assert(aligned_OK(chunk2mem(p)));
1815:../target/stm32/malloc/mallocr.c ****     /* ... matching footer field */
1816:../target/stm32/malloc/mallocr.c ****     assert(next->prev_size == sz);
1817:../target/stm32/malloc/mallocr.c ****     /* ... and is fully consolidated */
1818:../target/stm32/malloc/mallocr.c ****     assert(prev_inuse(p));
1819:../target/stm32/malloc/mallocr.c ****     assert (next == top || inuse(next));
1820:../target/stm32/malloc/mallocr.c ****     
1821:../target/stm32/malloc/mallocr.c ****     /* ... and has minimally sane links */
1822:../target/stm32/malloc/mallocr.c ****     assert(p->fd->bk == p);
1823:../target/stm32/malloc/mallocr.c ****     assert(p->bk->fd == p);
1824:../target/stm32/malloc/mallocr.c ****   }
1825:../target/stm32/malloc/mallocr.c ****   else /* markers are always of size SIZE_SZ */
1826:../target/stm32/malloc/mallocr.c ****     assert(sz == SIZE_SZ); 
1827:../target/stm32/malloc/mallocr.c **** }
1828:../target/stm32/malloc/mallocr.c **** 
1829:../target/stm32/malloc/mallocr.c **** #if __STD_C
1830:../target/stm32/malloc/mallocr.c **** static void do_check_inuse_chunk(mchunkptr p) 
1831:../target/stm32/malloc/mallocr.c **** #else
1832:../target/stm32/malloc/mallocr.c **** static void do_check_inuse_chunk(p) mchunkptr p;
1833:../target/stm32/malloc/mallocr.c **** #endif
1834:../target/stm32/malloc/mallocr.c **** { 
1835:../target/stm32/malloc/mallocr.c ****   mchunkptr next = next_chunk(p);
1836:../target/stm32/malloc/mallocr.c ****   do_check_chunk(p);
1837:../target/stm32/malloc/mallocr.c **** 
1838:../target/stm32/malloc/mallocr.c ****   /* Check whether it claims to be in use ... */
1839:../target/stm32/malloc/mallocr.c ****   assert(inuse(p));
1840:../target/stm32/malloc/mallocr.c **** 
1841:../target/stm32/malloc/mallocr.c ****   /* ... and is surrounded by OK chunks.
1842:../target/stm32/malloc/mallocr.c ****     Since more things can be checked with free chunks than inuse ones,
1843:../target/stm32/malloc/mallocr.c ****     if an inuse chunk borders them and debug is on, it's worth doing them.
1844:../target/stm32/malloc/mallocr.c ****   */
1845:../target/stm32/malloc/mallocr.c ****   if (!prev_inuse(p)) 
1846:../target/stm32/malloc/mallocr.c ****   {
1847:../target/stm32/malloc/mallocr.c ****     mchunkptr prv = prev_chunk(p);
1848:../target/stm32/malloc/mallocr.c ****     assert(next_chunk(prv) == p);
1849:../target/stm32/malloc/mallocr.c ****     do_check_free_chunk(prv);
1850:../target/stm32/malloc/mallocr.c ****   }
1851:../target/stm32/malloc/mallocr.c ****   if (next == top)
1852:../target/stm32/malloc/mallocr.c ****   {
1853:../target/stm32/malloc/mallocr.c ****     assert(prev_inuse(next));
1854:../target/stm32/malloc/mallocr.c ****     assert(chunksize(next) >= MINSIZE);
1855:../target/stm32/malloc/mallocr.c ****   }
1856:../target/stm32/malloc/mallocr.c ****   else if (!inuse(next))
1857:../target/stm32/malloc/mallocr.c ****     do_check_free_chunk(next);
1858:../target/stm32/malloc/mallocr.c **** 
1859:../target/stm32/malloc/mallocr.c **** }
1860:../target/stm32/malloc/mallocr.c **** 
1861:../target/stm32/malloc/mallocr.c **** #if __STD_C
1862:../target/stm32/malloc/mallocr.c **** static void do_check_malloced_chunk(mchunkptr p, INTERNAL_SIZE_T s) 
1863:../target/stm32/malloc/mallocr.c **** #else
1864:../target/stm32/malloc/mallocr.c **** static void do_check_malloced_chunk(p, s) mchunkptr p; INTERNAL_SIZE_T s;
1865:../target/stm32/malloc/mallocr.c **** #endif
1866:../target/stm32/malloc/mallocr.c **** {
1867:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T sz = p->size & ~PREV_INUSE;
1868:../target/stm32/malloc/mallocr.c ****   long room = long_sub_size_t(sz, s);
1869:../target/stm32/malloc/mallocr.c **** 
1870:../target/stm32/malloc/mallocr.c ****   do_check_inuse_chunk(p);
1871:../target/stm32/malloc/mallocr.c **** 
1872:../target/stm32/malloc/mallocr.c ****   /* Legal size ... */
1873:../target/stm32/malloc/mallocr.c ****   assert((long)sz >= (long)MINSIZE);
1874:../target/stm32/malloc/mallocr.c ****   assert((sz & MALLOC_ALIGN_MASK) == 0);
1875:../target/stm32/malloc/mallocr.c ****   assert(room >= 0);
1876:../target/stm32/malloc/mallocr.c ****   assert(room < (long)MINSIZE);
1877:../target/stm32/malloc/mallocr.c **** 
1878:../target/stm32/malloc/mallocr.c ****   /* ... and alignment */
1879:../target/stm32/malloc/mallocr.c ****   assert(aligned_OK(chunk2mem(p)));
1880:../target/stm32/malloc/mallocr.c **** 
1881:../target/stm32/malloc/mallocr.c **** 
1882:../target/stm32/malloc/mallocr.c ****   /* ... and was allocated at front of an available chunk */
1883:../target/stm32/malloc/mallocr.c ****   assert(prev_inuse(p));
1884:../target/stm32/malloc/mallocr.c **** 
1885:../target/stm32/malloc/mallocr.c **** }
1886:../target/stm32/malloc/mallocr.c **** 
1887:../target/stm32/malloc/mallocr.c **** 
1888:../target/stm32/malloc/mallocr.c **** #define check_free_chunk(P)  do_check_free_chunk(P)
1889:../target/stm32/malloc/mallocr.c **** #define check_inuse_chunk(P) do_check_inuse_chunk(P)
1890:../target/stm32/malloc/mallocr.c **** #define check_chunk(P) do_check_chunk(P)
1891:../target/stm32/malloc/mallocr.c **** #define check_malloced_chunk(P,N) do_check_malloced_chunk(P,N)
1892:../target/stm32/malloc/mallocr.c **** #else
1893:../target/stm32/malloc/mallocr.c **** #define check_free_chunk(P) 
1894:../target/stm32/malloc/mallocr.c **** #define check_inuse_chunk(P)
1895:../target/stm32/malloc/mallocr.c **** #define check_chunk(P)
1896:../target/stm32/malloc/mallocr.c **** #define check_malloced_chunk(P,N)
1897:../target/stm32/malloc/mallocr.c **** #endif
1898:../target/stm32/malloc/mallocr.c **** 
1899:../target/stm32/malloc/mallocr.c **** 
1900:../target/stm32/malloc/mallocr.c **** 
1901:../target/stm32/malloc/mallocr.c **** /* 
1902:../target/stm32/malloc/mallocr.c ****   Macro-based internal utilities
1903:../target/stm32/malloc/mallocr.c **** */
1904:../target/stm32/malloc/mallocr.c **** 
1905:../target/stm32/malloc/mallocr.c **** 
1906:../target/stm32/malloc/mallocr.c **** /*  
1907:../target/stm32/malloc/mallocr.c ****   Linking chunks in bin lists.
1908:../target/stm32/malloc/mallocr.c ****   Call these only with variables, not arbitrary expressions, as arguments.
1909:../target/stm32/malloc/mallocr.c **** */
1910:../target/stm32/malloc/mallocr.c **** 
1911:../target/stm32/malloc/mallocr.c **** /* 
1912:../target/stm32/malloc/mallocr.c ****   Place chunk p of size s in its bin, in size order,
1913:../target/stm32/malloc/mallocr.c ****   putting it ahead of others of same size.
1914:../target/stm32/malloc/mallocr.c **** */
1915:../target/stm32/malloc/mallocr.c **** 
1916:../target/stm32/malloc/mallocr.c **** 
1917:../target/stm32/malloc/mallocr.c **** #define frontlink(P, S, IDX, BK, FD)                                          \
1918:../target/stm32/malloc/mallocr.c **** {                                                                             \
1919:../target/stm32/malloc/mallocr.c ****   if (S < MAX_SMALLBIN_SIZE)                                                  \
1920:../target/stm32/malloc/mallocr.c ****   {                                                                           \
1921:../target/stm32/malloc/mallocr.c ****     IDX = smallbin_index(S);                                                  \
1922:../target/stm32/malloc/mallocr.c ****     mark_binblock(IDX);                                                       \
1923:../target/stm32/malloc/mallocr.c ****     BK = bin_at(IDX);                                                         \
1924:../target/stm32/malloc/mallocr.c ****     FD = BK->fd;                                                              \
1925:../target/stm32/malloc/mallocr.c ****     P->bk = BK;                                                               \
1926:../target/stm32/malloc/mallocr.c ****     P->fd = FD;                                                               \
1927:../target/stm32/malloc/mallocr.c ****     FD->bk = BK->fd = P;                                                      \
1928:../target/stm32/malloc/mallocr.c ****   }                                                                           \
1929:../target/stm32/malloc/mallocr.c ****   else                                                                        \
1930:../target/stm32/malloc/mallocr.c ****   {                                                                           \
1931:../target/stm32/malloc/mallocr.c ****     IDX = bin_index(S);                                                       \
1932:../target/stm32/malloc/mallocr.c ****     BK = bin_at(IDX);                                                         \
1933:../target/stm32/malloc/mallocr.c ****     FD = BK->fd;                                                              \
1934:../target/stm32/malloc/mallocr.c ****     if (FD == BK) mark_binblock(IDX);                                         \
1935:../target/stm32/malloc/mallocr.c ****     else                                                                      \
1936:../target/stm32/malloc/mallocr.c ****     {                                                                         \
1937:../target/stm32/malloc/mallocr.c ****       while (FD != BK && S < chunksize(FD)) FD = FD->fd;                      \
1938:../target/stm32/malloc/mallocr.c ****       BK = FD->bk;                                                            \
1939:../target/stm32/malloc/mallocr.c ****     }                                                                         \
1940:../target/stm32/malloc/mallocr.c ****     P->bk = BK;                                                               \
1941:../target/stm32/malloc/mallocr.c ****     P->fd = FD;                                                               \
1942:../target/stm32/malloc/mallocr.c ****     FD->bk = BK->fd = P;                                                      \
1943:../target/stm32/malloc/mallocr.c ****   }                                                                           \
1944:../target/stm32/malloc/mallocr.c **** }
1945:../target/stm32/malloc/mallocr.c **** 
1946:../target/stm32/malloc/mallocr.c **** 
1947:../target/stm32/malloc/mallocr.c **** /* take a chunk off a list */
1948:../target/stm32/malloc/mallocr.c **** 
1949:../target/stm32/malloc/mallocr.c **** #define unlink(P, BK, FD)                                                     \
1950:../target/stm32/malloc/mallocr.c **** {                                                                             \
1951:../target/stm32/malloc/mallocr.c ****   BK = P->bk;                                                                 \
1952:../target/stm32/malloc/mallocr.c ****   FD = P->fd;                                                                 \
1953:../target/stm32/malloc/mallocr.c ****   FD->bk = BK;                                                        \
1954:../target/stm32/malloc/mallocr.c ****   BK->fd = FD;                                                        \
1955:../target/stm32/malloc/mallocr.c **** }                                                                             \
1956:../target/stm32/malloc/mallocr.c **** 
1957:../target/stm32/malloc/mallocr.c **** /* Place p as the last remainder */
1958:../target/stm32/malloc/mallocr.c **** 
1959:../target/stm32/malloc/mallocr.c **** #define link_last_remainder(P)                                                \
1960:../target/stm32/malloc/mallocr.c **** {                                                                             \
1961:../target/stm32/malloc/mallocr.c ****   last_remainder->fd = last_remainder->bk =  P;                               \
1962:../target/stm32/malloc/mallocr.c ****   P->fd = P->bk = last_remainder;                                             \
1963:../target/stm32/malloc/mallocr.c **** }
1964:../target/stm32/malloc/mallocr.c **** 
1965:../target/stm32/malloc/mallocr.c **** /* Clear the last_remainder bin */
1966:../target/stm32/malloc/mallocr.c **** 
1967:../target/stm32/malloc/mallocr.c **** #define clear_last_remainder \
1968:../target/stm32/malloc/mallocr.c ****   (last_remainder->fd = last_remainder->bk = last_remainder)
1969:../target/stm32/malloc/mallocr.c **** 
1970:../target/stm32/malloc/mallocr.c **** 
1971:../target/stm32/malloc/mallocr.c **** 
1972:../target/stm32/malloc/mallocr.c **** 
1973:../target/stm32/malloc/mallocr.c **** 
1974:../target/stm32/malloc/mallocr.c **** 
1975:../target/stm32/malloc/mallocr.c **** /* Routines dealing with mmap(). */
1976:../target/stm32/malloc/mallocr.c **** 
1977:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
1978:../target/stm32/malloc/mallocr.c **** 
1979:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_MALLOC
1980:../target/stm32/malloc/mallocr.c **** 
1981:../target/stm32/malloc/mallocr.c **** #if __STD_C
1982:../target/stm32/malloc/mallocr.c **** static mchunkptr mmap_chunk(size_t size)
1983:../target/stm32/malloc/mallocr.c **** #else
1984:../target/stm32/malloc/mallocr.c **** static mchunkptr mmap_chunk(size) size_t size;
1985:../target/stm32/malloc/mallocr.c **** #endif
1986:../target/stm32/malloc/mallocr.c **** {
1987:../target/stm32/malloc/mallocr.c ****   size_t page_mask = malloc_getpagesize - 1;
1988:../target/stm32/malloc/mallocr.c ****   mchunkptr p;
1989:../target/stm32/malloc/mallocr.c **** 
1990:../target/stm32/malloc/mallocr.c **** #ifndef MAP_ANONYMOUS
1991:../target/stm32/malloc/mallocr.c ****   static int fd = -1;
1992:../target/stm32/malloc/mallocr.c **** #endif
1993:../target/stm32/malloc/mallocr.c **** 
1994:../target/stm32/malloc/mallocr.c ****   if(n_mmaps >= n_mmaps_max) return 0; /* too many regions */
1995:../target/stm32/malloc/mallocr.c **** 
1996:../target/stm32/malloc/mallocr.c ****   /* For mmapped chunks, the overhead is one SIZE_SZ unit larger, because
1997:../target/stm32/malloc/mallocr.c ****    * there is no following chunk whose prev_size field could be used.
1998:../target/stm32/malloc/mallocr.c ****    */
1999:../target/stm32/malloc/mallocr.c ****   size = (size + SIZE_SZ + page_mask) & ~page_mask;
2000:../target/stm32/malloc/mallocr.c **** 
2001:../target/stm32/malloc/mallocr.c **** #ifdef MAP_ANONYMOUS
2002:../target/stm32/malloc/mallocr.c ****   p = (mchunkptr)mmap(0, size, PROT_READ|PROT_WRITE,
2003:../target/stm32/malloc/mallocr.c **** 		      MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
2004:../target/stm32/malloc/mallocr.c **** #else /* !MAP_ANONYMOUS */
2005:../target/stm32/malloc/mallocr.c ****   if (fd < 0) 
2006:../target/stm32/malloc/mallocr.c ****   {
2007:../target/stm32/malloc/mallocr.c ****     fd = open("/dev/zero", O_RDWR);
2008:../target/stm32/malloc/mallocr.c ****     if(fd < 0) return 0;
2009:../target/stm32/malloc/mallocr.c ****   }
2010:../target/stm32/malloc/mallocr.c ****   p = (mchunkptr)mmap(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE, fd, 0);
2011:../target/stm32/malloc/mallocr.c **** #endif
2012:../target/stm32/malloc/mallocr.c **** 
2013:../target/stm32/malloc/mallocr.c ****   if(p == (mchunkptr)-1) return 0;
2014:../target/stm32/malloc/mallocr.c **** 
2015:../target/stm32/malloc/mallocr.c ****   n_mmaps++;
2016:../target/stm32/malloc/mallocr.c ****   if (n_mmaps > max_n_mmaps) max_n_mmaps = n_mmaps;
2017:../target/stm32/malloc/mallocr.c ****   
2018:../target/stm32/malloc/mallocr.c ****   /* We demand that eight bytes into a page must be 8-byte aligned. */
2019:../target/stm32/malloc/mallocr.c ****   assert(aligned_OK(chunk2mem(p)));
2020:../target/stm32/malloc/mallocr.c **** 
2021:../target/stm32/malloc/mallocr.c ****   /* The offset to the start of the mmapped region is stored
2022:../target/stm32/malloc/mallocr.c ****    * in the prev_size field of the chunk; normally it is zero,
2023:../target/stm32/malloc/mallocr.c ****    * but that can be changed in memalign().
2024:../target/stm32/malloc/mallocr.c ****    */
2025:../target/stm32/malloc/mallocr.c ****   p->prev_size = 0;
2026:../target/stm32/malloc/mallocr.c ****   set_head(p, size|IS_MMAPPED);
2027:../target/stm32/malloc/mallocr.c ****   
2028:../target/stm32/malloc/mallocr.c ****   mmapped_mem += size;
2029:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)mmapped_mem > (unsigned long)max_mmapped_mem) 
2030:../target/stm32/malloc/mallocr.c ****     max_mmapped_mem = mmapped_mem;
2031:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)(mmapped_mem + sbrked_mem) > (unsigned long)max_total_mem) 
2032:../target/stm32/malloc/mallocr.c ****     max_total_mem = mmapped_mem + sbrked_mem;
2033:../target/stm32/malloc/mallocr.c ****   return p;
2034:../target/stm32/malloc/mallocr.c **** }
2035:../target/stm32/malloc/mallocr.c **** 
2036:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_MALLOC */
2037:../target/stm32/malloc/mallocr.c **** 
2038:../target/stm32/malloc/mallocr.c **** #ifdef SEPARATE_OBJECTS
2039:../target/stm32/malloc/mallocr.c **** #define munmap_chunk malloc_munmap_chunk
2040:../target/stm32/malloc/mallocr.c **** #endif
2041:../target/stm32/malloc/mallocr.c **** 
2042:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_FREE
2043:../target/stm32/malloc/mallocr.c **** 
2044:../target/stm32/malloc/mallocr.c **** #if __STD_C
2045:../target/stm32/malloc/mallocr.c **** STATIC void munmap_chunk(mchunkptr p)
2046:../target/stm32/malloc/mallocr.c **** #else
2047:../target/stm32/malloc/mallocr.c **** STATIC void munmap_chunk(p) mchunkptr p;
2048:../target/stm32/malloc/mallocr.c **** #endif
2049:../target/stm32/malloc/mallocr.c **** {
2050:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T size = chunksize(p);
2051:../target/stm32/malloc/mallocr.c ****   int ret;
2052:../target/stm32/malloc/mallocr.c **** 
2053:../target/stm32/malloc/mallocr.c ****   assert (chunk_is_mmapped(p));
2054:../target/stm32/malloc/mallocr.c ****   assert(! ((char*)p >= sbrk_base && (char*)p < sbrk_base + sbrked_mem));
2055:../target/stm32/malloc/mallocr.c ****   assert((n_mmaps > 0));
2056:../target/stm32/malloc/mallocr.c ****   assert(((p->prev_size + size) & (malloc_getpagesize-1)) == 0);
2057:../target/stm32/malloc/mallocr.c **** 
2058:../target/stm32/malloc/mallocr.c ****   n_mmaps--;
2059:../target/stm32/malloc/mallocr.c ****   mmapped_mem -= (size + p->prev_size);
2060:../target/stm32/malloc/mallocr.c **** 
2061:../target/stm32/malloc/mallocr.c ****   ret = munmap((char *)p - p->prev_size, size + p->prev_size);
2062:../target/stm32/malloc/mallocr.c **** 
2063:../target/stm32/malloc/mallocr.c ****   /* munmap returns non-zero on failure */
2064:../target/stm32/malloc/mallocr.c ****   assert(ret == 0);
2065:../target/stm32/malloc/mallocr.c **** }
2066:../target/stm32/malloc/mallocr.c **** 
2067:../target/stm32/malloc/mallocr.c **** #else /* ! DEFINE_FREE */
2068:../target/stm32/malloc/mallocr.c **** 
2069:../target/stm32/malloc/mallocr.c **** #if __STD_C
2070:../target/stm32/malloc/mallocr.c **** extern void munmap_chunk(mchunkptr);
2071:../target/stm32/malloc/mallocr.c **** #else
2072:../target/stm32/malloc/mallocr.c **** extern void munmap_chunk();
2073:../target/stm32/malloc/mallocr.c **** #endif
2074:../target/stm32/malloc/mallocr.c **** 
2075:../target/stm32/malloc/mallocr.c **** #endif /* ! DEFINE_FREE */
2076:../target/stm32/malloc/mallocr.c **** 
2077:../target/stm32/malloc/mallocr.c **** #if HAVE_MREMAP
2078:../target/stm32/malloc/mallocr.c **** 
2079:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_REALLOC
2080:../target/stm32/malloc/mallocr.c **** 
2081:../target/stm32/malloc/mallocr.c **** #if __STD_C
2082:../target/stm32/malloc/mallocr.c **** static mchunkptr mremap_chunk(mchunkptr p, size_t new_size)
2083:../target/stm32/malloc/mallocr.c **** #else
2084:../target/stm32/malloc/mallocr.c **** static mchunkptr mremap_chunk(p, new_size) mchunkptr p; size_t new_size;
2085:../target/stm32/malloc/mallocr.c **** #endif
2086:../target/stm32/malloc/mallocr.c **** {
2087:../target/stm32/malloc/mallocr.c ****   size_t page_mask = malloc_getpagesize - 1;
2088:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T offset = p->prev_size;
2089:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T size = chunksize(p);
2090:../target/stm32/malloc/mallocr.c ****   char *cp;
2091:../target/stm32/malloc/mallocr.c **** 
2092:../target/stm32/malloc/mallocr.c ****   assert (chunk_is_mmapped(p));
2093:../target/stm32/malloc/mallocr.c ****   assert(! ((char*)p >= sbrk_base && (char*)p < sbrk_base + sbrked_mem));
2094:../target/stm32/malloc/mallocr.c ****   assert((n_mmaps > 0));
2095:../target/stm32/malloc/mallocr.c ****   assert(((size + offset) & (malloc_getpagesize-1)) == 0);
2096:../target/stm32/malloc/mallocr.c **** 
2097:../target/stm32/malloc/mallocr.c ****   /* Note the extra SIZE_SZ overhead as in mmap_chunk(). */
2098:../target/stm32/malloc/mallocr.c ****   new_size = (new_size + offset + SIZE_SZ + page_mask) & ~page_mask;
2099:../target/stm32/malloc/mallocr.c **** 
2100:../target/stm32/malloc/mallocr.c ****   cp = (char *)mremap((char *)p - offset, size + offset, new_size, 1);
2101:../target/stm32/malloc/mallocr.c **** 
2102:../target/stm32/malloc/mallocr.c ****   if (cp == (char *)-1) return 0;
2103:../target/stm32/malloc/mallocr.c **** 
2104:../target/stm32/malloc/mallocr.c ****   p = (mchunkptr)(cp + offset);
2105:../target/stm32/malloc/mallocr.c **** 
2106:../target/stm32/malloc/mallocr.c ****   assert(aligned_OK(chunk2mem(p)));
2107:../target/stm32/malloc/mallocr.c **** 
2108:../target/stm32/malloc/mallocr.c ****   assert((p->prev_size == offset));
2109:../target/stm32/malloc/mallocr.c ****   set_head(p, (new_size - offset)|IS_MMAPPED);
2110:../target/stm32/malloc/mallocr.c **** 
2111:../target/stm32/malloc/mallocr.c ****   mmapped_mem -= size + offset;
2112:../target/stm32/malloc/mallocr.c ****   mmapped_mem += new_size;
2113:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)mmapped_mem > (unsigned long)max_mmapped_mem) 
2114:../target/stm32/malloc/mallocr.c ****     max_mmapped_mem = mmapped_mem;
2115:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)(mmapped_mem + sbrked_mem) > (unsigned long)max_total_mem)
2116:../target/stm32/malloc/mallocr.c ****     max_total_mem = mmapped_mem + sbrked_mem;
2117:../target/stm32/malloc/mallocr.c ****   return p;
2118:../target/stm32/malloc/mallocr.c **** }
2119:../target/stm32/malloc/mallocr.c **** 
2120:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_REALLOC */
2121:../target/stm32/malloc/mallocr.c **** 
2122:../target/stm32/malloc/mallocr.c **** #endif /* HAVE_MREMAP */
2123:../target/stm32/malloc/mallocr.c **** 
2124:../target/stm32/malloc/mallocr.c **** #endif /* HAVE_MMAP */
2125:../target/stm32/malloc/mallocr.c **** 
2126:../target/stm32/malloc/mallocr.c **** 
2127:../target/stm32/malloc/mallocr.c **** 
2128:../target/stm32/malloc/mallocr.c **** 
2129:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_MALLOC
2130:../target/stm32/malloc/mallocr.c **** 
2131:../target/stm32/malloc/mallocr.c **** /* 
2132:../target/stm32/malloc/mallocr.c ****   Extend the top-most chunk by obtaining memory from system.
2133:../target/stm32/malloc/mallocr.c ****   Main interface to sbrk (but see also malloc_trim).
2134:../target/stm32/malloc/mallocr.c **** */
2135:../target/stm32/malloc/mallocr.c **** 
2136:../target/stm32/malloc/mallocr.c **** #if __STD_C
2137:../target/stm32/malloc/mallocr.c **** static void malloc_extend_top(RARG INTERNAL_SIZE_T nb)
2138:../target/stm32/malloc/mallocr.c **** #else
2139:../target/stm32/malloc/mallocr.c **** static void malloc_extend_top(RARG nb) RDECL INTERNAL_SIZE_T nb;
2140:../target/stm32/malloc/mallocr.c **** #endif
2141:../target/stm32/malloc/mallocr.c **** {
 3432              		.loc 1 2141 0
 3433              		.cfi_startproc
 3434              		@ args = 0, pretend = 0, frame = 56
 3435              		@ frame_needed = 1, uses_anonymous_args = 0
 3436 0000 80B5     		push	{r7, lr}
 3437              	.LCFI0:
 3438              		.cfi_def_cfa_offset 8
 3439 0002 8EB0     		sub	sp, sp, #56
 3440              	.LCFI1:
 3441              		.cfi_def_cfa_offset 64
 3442 0004 00AF     		add	r7, sp, #0
 3443              		.cfi_offset 14, -4
 3444              		.cfi_offset 7, -8
 3445              	.LCFI2:
 3446              		.cfi_def_cfa_register 7
 3447 0006 7860     		str	r0, [r7, #4]
 3448 0008 3960     		str	r1, [r7, #0]
2142:../target/stm32/malloc/mallocr.c ****   char*     brk;                  /* return value from sbrk */
2143:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of sbrked space */
2144:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T correction;     /* bytes for 2nd sbrk call */
2145:../target/stm32/malloc/mallocr.c ****   int correction_failed = 0;      /* whether we should relax the assertion */
 3449              		.loc 1 2145 0
 3450 000a 4FF00003 		mov	r3, #0
 3451 000e 7B62     		str	r3, [r7, #36]
2146:../target/stm32/malloc/mallocr.c ****   char*     new_brk;              /* return of 2nd sbrk call */
2147:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T top_size;       /* new size of top chunk */
2148:../target/stm32/malloc/mallocr.c **** 
2149:../target/stm32/malloc/mallocr.c ****   mchunkptr old_top     = top;  /* Record state of old top */
 3452              		.loc 1 2149 0
 3453 0010 40F20003 		movw	r3, #:lower16:__malloc_av_
 3454 0014 C0F20003 		movt	r3, #:upper16:__malloc_av_
 3455 0018 9B68     		ldr	r3, [r3, #8]
 3456 001a 3B62     		str	r3, [r7, #32]
2150:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T old_top_size = chunksize(old_top);
 3457              		.loc 1 2150 0
 3458 001c 3B6A     		ldr	r3, [r7, #32]
 3459 001e 5B68     		ldr	r3, [r3, #4]
 3460 0020 23F00303 		bic	r3, r3, #3
 3461 0024 FB61     		str	r3, [r7, #28]
2151:../target/stm32/malloc/mallocr.c ****   char*     old_end      = (char*)(chunk_at_offset(old_top, old_top_size));
 3462              		.loc 1 2151 0
 3463 0026 3A6A     		ldr	r2, [r7, #32]
 3464 0028 FB69     		ldr	r3, [r7, #28]
 3465 002a D318     		adds	r3, r2, r3
 3466 002c BB61     		str	r3, [r7, #24]
2152:../target/stm32/malloc/mallocr.c **** 
2153:../target/stm32/malloc/mallocr.c ****   /* Pad request with top_pad plus minimal overhead */
2154:../target/stm32/malloc/mallocr.c ****   
2155:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T    sbrk_size     = nb + top_pad + MINSIZE;
 3467              		.loc 1 2155 0
 3468 002e 40F20003 		movw	r3, #:lower16:__malloc_top_pad
 3469 0032 C0F20003 		movt	r3, #:upper16:__malloc_top_pad
 3470 0036 1A68     		ldr	r2, [r3, #0]
 3471 0038 3B68     		ldr	r3, [r7, #0]
 3472 003a D318     		adds	r3, r2, r3
 3473 003c 03F11003 		add	r3, r3, #16
 3474 0040 BB62     		str	r3, [r7, #40]
2156:../target/stm32/malloc/mallocr.c ****   unsigned long pagesz    = malloc_getpagesize;
 3475              		.loc 1 2156 0
 3476 0042 4FF48053 		mov	r3, #4096
 3477 0046 7B61     		str	r3, [r7, #20]
2157:../target/stm32/malloc/mallocr.c **** 
2158:../target/stm32/malloc/mallocr.c ****   /* If not the first time through, round to preserve page boundary */
2159:../target/stm32/malloc/mallocr.c ****   /* Otherwise, we need to correct to a page size below anyway. */
2160:../target/stm32/malloc/mallocr.c ****   /* (We also correct below if an intervening foreign sbrk call.) */
2161:../target/stm32/malloc/mallocr.c **** 
2162:../target/stm32/malloc/mallocr.c ****   if (sbrk_base != (char*)(-1))
 3478              		.loc 1 2162 0
 3479 0048 40F20003 		movw	r3, #:lower16:__malloc_sbrk_base
 3480 004c C0F20003 		movt	r3, #:upper16:__malloc_sbrk_base
 3481 0050 1B68     		ldr	r3, [r3, #0]
 3482 0052 B3F1FF3F 		cmp	r3, #-1
 3483 0056 0AD0     		beq	.L2
2163:../target/stm32/malloc/mallocr.c ****     sbrk_size = (sbrk_size + (pagesz - 1)) & ~(pagesz - 1);
 3484              		.loc 1 2163 0
 3485 0058 7A69     		ldr	r2, [r7, #20]
 3486 005a BB6A     		ldr	r3, [r7, #40]
 3487 005c D318     		adds	r3, r2, r3
 3488 005e 03F1FF32 		add	r2, r3, #-1
 3489 0062 7B69     		ldr	r3, [r7, #20]
 3490 0064 C3F10003 		rsb	r3, r3, #0
 3491 0068 02EA0303 		and	r3, r2, r3
 3492 006c BB62     		str	r3, [r7, #40]
 3493              	.L2:
2164:../target/stm32/malloc/mallocr.c **** 
2165:../target/stm32/malloc/mallocr.c ****   brk = (char*)(MORECORE (sbrk_size));
 3494              		.loc 1 2165 0
 3495 006e BB6A     		ldr	r3, [r7, #40]
 3496 0070 7868     		ldr	r0, [r7, #4]
 3497 0072 1946     		mov	r1, r3
 3498 0074 FFF7FEFF 		bl	_sbrk_r
 3499 0078 0346     		mov	r3, r0
 3500 007a 7B63     		str	r3, [r7, #52]
2166:../target/stm32/malloc/mallocr.c **** 
2167:../target/stm32/malloc/mallocr.c ****   /* Fail if sbrk failed or if a foreign sbrk call killed our space */
2168:../target/stm32/malloc/mallocr.c ****   if (brk == (char*)(MORECORE_FAILURE) || 
 3501              		.loc 1 2168 0
 3502 007c 7B6B     		ldr	r3, [r7, #52]
 3503 007e B3F1FF3F 		cmp	r3, #-1
 3504 0082 00F01581 		beq	.L15
 3505              		.loc 1 2168 0 is_stmt 0 discriminator 1
 3506 0086 7A6B     		ldr	r2, [r7, #52]
 3507 0088 BB69     		ldr	r3, [r7, #24]
 3508 008a 9A42     		cmp	r2, r3
 3509 008c 07D2     		bcs	.L4
2169:../target/stm32/malloc/mallocr.c ****       (brk < old_end && old_top != initial_top))
 3510              		.loc 1 2169 0 is_stmt 1
 3511 008e 40F20003 		movw	r3, #:lower16:__malloc_av_
 3512 0092 C0F20003 		movt	r3, #:upper16:__malloc_av_
 3513 0096 3A6A     		ldr	r2, [r7, #32]
 3514 0098 9A42     		cmp	r2, r3
 3515 009a 40F00B81 		bne	.L16
 3516              	.L4:
2170:../target/stm32/malloc/mallocr.c ****     return;
2171:../target/stm32/malloc/mallocr.c **** 
2172:../target/stm32/malloc/mallocr.c ****   sbrked_mem += sbrk_size;
 3517              		.loc 1 2172 0
 3518 009e 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3519 00a2 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3520 00a6 1B68     		ldr	r3, [r3, #0]
 3521 00a8 1A46     		mov	r2, r3
 3522 00aa BB6A     		ldr	r3, [r7, #40]
 3523 00ac D318     		adds	r3, r2, r3
 3524 00ae 1A46     		mov	r2, r3
 3525 00b0 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3526 00b4 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3527 00b8 1A60     		str	r2, [r3, #0]
2173:../target/stm32/malloc/mallocr.c **** 
2174:../target/stm32/malloc/mallocr.c ****   if (brk == old_end /* can just add bytes to current top, unless
 3528              		.loc 1 2174 0
 3529 00ba 7A6B     		ldr	r2, [r7, #52]
 3530 00bc BB69     		ldr	r3, [r7, #24]
 3531 00be 9A42     		cmp	r2, r3
 3532 00c0 15D1     		bne	.L6
2175:../target/stm32/malloc/mallocr.c **** 			previous correction failed */
2176:../target/stm32/malloc/mallocr.c ****       && ((POINTER_UINT)old_end & (pagesz - 1)) == 0)
 3533              		.loc 1 2176 0
 3534 00c2 7B69     		ldr	r3, [r7, #20]
 3535 00c4 03F1FF32 		add	r2, r3, #-1
 3536 00c8 BB69     		ldr	r3, [r7, #24]
 3537 00ca 02EA0303 		and	r3, r2, r3
 3538 00ce 002B     		cmp	r3, #0
 3539 00d0 0DD1     		bne	.L6
2177:../target/stm32/malloc/mallocr.c ****   {
2178:../target/stm32/malloc/mallocr.c ****     top_size = sbrk_size + old_top_size;
 3540              		.loc 1 2178 0
 3541 00d2 BA6A     		ldr	r2, [r7, #40]
 3542 00d4 FB69     		ldr	r3, [r7, #28]
 3543 00d6 D318     		adds	r3, r2, r3
 3544 00d8 3B61     		str	r3, [r7, #16]
2179:../target/stm32/malloc/mallocr.c ****     set_head(top, top_size | PREV_INUSE);
 3545              		.loc 1 2179 0
 3546 00da 40F20003 		movw	r3, #:lower16:__malloc_av_
 3547 00de C0F20003 		movt	r3, #:upper16:__malloc_av_
 3548 00e2 9B68     		ldr	r3, [r3, #8]
 3549 00e4 3A69     		ldr	r2, [r7, #16]
 3550 00e6 42F00102 		orr	r2, r2, #1
 3551 00ea 5A60     		str	r2, [r3, #4]
 3552 00ec AFE0     		b	.L7
 3553              	.L6:
2180:../target/stm32/malloc/mallocr.c ****   }
2181:../target/stm32/malloc/mallocr.c ****   else
2182:../target/stm32/malloc/mallocr.c ****   {
2183:../target/stm32/malloc/mallocr.c ****     if (sbrk_base == (char*)(-1))  /* First time through. Record base */
 3554              		.loc 1 2183 0
 3555 00ee 40F20003 		movw	r3, #:lower16:__malloc_sbrk_base
 3556 00f2 C0F20003 		movt	r3, #:upper16:__malloc_sbrk_base
 3557 00f6 1B68     		ldr	r3, [r3, #0]
 3558 00f8 B3F1FF3F 		cmp	r3, #-1
 3559 00fc 06D1     		bne	.L8
2184:../target/stm32/malloc/mallocr.c ****       sbrk_base = brk;
 3560              		.loc 1 2184 0
 3561 00fe 40F20003 		movw	r3, #:lower16:__malloc_sbrk_base
 3562 0102 C0F20003 		movt	r3, #:upper16:__malloc_sbrk_base
 3563 0106 7A6B     		ldr	r2, [r7, #52]
 3564 0108 1A60     		str	r2, [r3, #0]
 3565 010a 0DE0     		b	.L9
 3566              	.L8:
2185:../target/stm32/malloc/mallocr.c ****     else  /* Someone else called sbrk().  Count those bytes as sbrked_mem. */
2186:../target/stm32/malloc/mallocr.c ****       sbrked_mem += brk - (char*)old_end;
 3567              		.loc 1 2186 0
 3568 010c 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3569 0110 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3570 0114 1A68     		ldr	r2, [r3, #0]
 3571 0116 796B     		ldr	r1, [r7, #52]
 3572 0118 BB69     		ldr	r3, [r7, #24]
 3573 011a CB1A     		subs	r3, r1, r3
 3574 011c D218     		adds	r2, r2, r3
 3575 011e 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3576 0122 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3577 0126 1A60     		str	r2, [r3, #0]
 3578              	.L9:
2187:../target/stm32/malloc/mallocr.c **** 
2188:../target/stm32/malloc/mallocr.c ****     /* Guarantee alignment of first new chunk made from this space */
2189:../target/stm32/malloc/mallocr.c ****     front_misalign = (POINTER_UINT)chunk2mem(brk) & MALLOC_ALIGN_MASK;
 3579              		.loc 1 2189 0
 3580 0128 7B6B     		ldr	r3, [r7, #52]
 3581 012a 03F10803 		add	r3, r3, #8
 3582 012e 03F00703 		and	r3, r3, #7
 3583 0132 FB60     		str	r3, [r7, #12]
2190:../target/stm32/malloc/mallocr.c ****     if (front_misalign > 0) 
 3584              		.loc 1 2190 0
 3585 0134 FB68     		ldr	r3, [r7, #12]
 3586 0136 002B     		cmp	r3, #0
 3587 0138 08D0     		beq	.L10
2191:../target/stm32/malloc/mallocr.c ****     {
2192:../target/stm32/malloc/mallocr.c ****       correction = (MALLOC_ALIGNMENT) - front_misalign;
 3588              		.loc 1 2192 0
 3589 013a FB68     		ldr	r3, [r7, #12]
 3590 013c C3F10803 		rsb	r3, r3, #8
 3591 0140 3B63     		str	r3, [r7, #48]
2193:../target/stm32/malloc/mallocr.c ****       brk += correction;
 3592              		.loc 1 2193 0
 3593 0142 7A6B     		ldr	r2, [r7, #52]
 3594 0144 3B6B     		ldr	r3, [r7, #48]
 3595 0146 D318     		adds	r3, r2, r3
 3596 0148 7B63     		str	r3, [r7, #52]
 3597 014a 02E0     		b	.L11
 3598              	.L10:
2194:../target/stm32/malloc/mallocr.c ****     }
2195:../target/stm32/malloc/mallocr.c ****     else
2196:../target/stm32/malloc/mallocr.c ****       correction = 0;
 3599              		.loc 1 2196 0
 3600 014c 4FF00003 		mov	r3, #0
 3601 0150 3B63     		str	r3, [r7, #48]
 3602              	.L11:
2197:../target/stm32/malloc/mallocr.c **** 
2198:../target/stm32/malloc/mallocr.c ****     /* Guarantee the next brk will be at a page boundary */
2199:../target/stm32/malloc/mallocr.c ****     correction += pagesz - ((POINTER_UINT)(brk + sbrk_size) & (pagesz - 1));
 3603              		.loc 1 2199 0
 3604 0152 7A6B     		ldr	r2, [r7, #52]
 3605 0154 BB6A     		ldr	r3, [r7, #40]
 3606 0156 D318     		adds	r3, r2, r3
 3607 0158 1A46     		mov	r2, r3
 3608 015a 7B69     		ldr	r3, [r7, #20]
 3609 015c 03F1FF33 		add	r3, r3, #-1
 3610 0160 02EA0303 		and	r3, r2, r3
 3611 0164 7A69     		ldr	r2, [r7, #20]
 3612 0166 D31A     		subs	r3, r2, r3
 3613 0168 3A6B     		ldr	r2, [r7, #48]
 3614 016a D318     		adds	r3, r2, r3
 3615 016c 3B63     		str	r3, [r7, #48]
2200:../target/stm32/malloc/mallocr.c **** 
2201:../target/stm32/malloc/mallocr.c ****     /* Allocate correction */
2202:../target/stm32/malloc/mallocr.c ****     new_brk = (char*)(MORECORE (correction));
 3616              		.loc 1 2202 0
 3617 016e 3B6B     		ldr	r3, [r7, #48]
 3618 0170 7868     		ldr	r0, [r7, #4]
 3619 0172 1946     		mov	r1, r3
 3620 0174 FFF7FEFF 		bl	_sbrk_r
 3621 0178 0346     		mov	r3, r0
 3622 017a FB62     		str	r3, [r7, #44]
2203:../target/stm32/malloc/mallocr.c ****     if (new_brk == (char*)(MORECORE_FAILURE))
 3623              		.loc 1 2203 0
 3624 017c FB6A     		ldr	r3, [r7, #44]
 3625 017e B3F1FF3F 		cmp	r3, #-1
 3626 0182 07D1     		bne	.L12
2204:../target/stm32/malloc/mallocr.c ****       {
2205:../target/stm32/malloc/mallocr.c **** 	correction = 0;
 3627              		.loc 1 2205 0
 3628 0184 4FF00003 		mov	r3, #0
 3629 0188 3B63     		str	r3, [r7, #48]
2206:../target/stm32/malloc/mallocr.c **** 	correction_failed = 1;
 3630              		.loc 1 2206 0
 3631 018a 4FF00103 		mov	r3, #1
 3632 018e 7B62     		str	r3, [r7, #36]
2207:../target/stm32/malloc/mallocr.c **** 	new_brk = brk;
 3633              		.loc 1 2207 0
 3634 0190 7B6B     		ldr	r3, [r7, #52]
 3635 0192 FB62     		str	r3, [r7, #44]
 3636              	.L12:
2208:../target/stm32/malloc/mallocr.c ****       }
2209:../target/stm32/malloc/mallocr.c **** 
2210:../target/stm32/malloc/mallocr.c ****     sbrked_mem += correction;
 3637              		.loc 1 2210 0
 3638 0194 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3639 0198 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3640 019c 1B68     		ldr	r3, [r3, #0]
 3641 019e 1A46     		mov	r2, r3
 3642 01a0 3B6B     		ldr	r3, [r7, #48]
 3643 01a2 D318     		adds	r3, r2, r3
 3644 01a4 1A46     		mov	r2, r3
 3645 01a6 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3646 01aa C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3647 01ae 1A60     		str	r2, [r3, #0]
2211:../target/stm32/malloc/mallocr.c **** 
2212:../target/stm32/malloc/mallocr.c ****     top = (mchunkptr)brk;
 3648              		.loc 1 2212 0
 3649 01b0 40F20003 		movw	r3, #:lower16:__malloc_av_
 3650 01b4 C0F20003 		movt	r3, #:upper16:__malloc_av_
 3651 01b8 7A6B     		ldr	r2, [r7, #52]
 3652 01ba 9A60     		str	r2, [r3, #8]
2213:../target/stm32/malloc/mallocr.c ****     top_size = new_brk - brk + correction;
 3653              		.loc 1 2213 0
 3654 01bc FA6A     		ldr	r2, [r7, #44]
 3655 01be 7B6B     		ldr	r3, [r7, #52]
 3656 01c0 D31A     		subs	r3, r2, r3
 3657 01c2 1A46     		mov	r2, r3
 3658 01c4 3B6B     		ldr	r3, [r7, #48]
 3659 01c6 D318     		adds	r3, r2, r3
 3660 01c8 3B61     		str	r3, [r7, #16]
2214:../target/stm32/malloc/mallocr.c ****     set_head(top, top_size | PREV_INUSE);
 3661              		.loc 1 2214 0
 3662 01ca 40F20003 		movw	r3, #:lower16:__malloc_av_
 3663 01ce C0F20003 		movt	r3, #:upper16:__malloc_av_
 3664 01d2 9B68     		ldr	r3, [r3, #8]
 3665 01d4 3A69     		ldr	r2, [r7, #16]
 3666 01d6 42F00102 		orr	r2, r2, #1
 3667 01da 5A60     		str	r2, [r3, #4]
2215:../target/stm32/malloc/mallocr.c **** 
2216:../target/stm32/malloc/mallocr.c ****     if (old_top != initial_top)
 3668              		.loc 1 2216 0
 3669 01dc 40F20003 		movw	r3, #:lower16:__malloc_av_
 3670 01e0 C0F20003 		movt	r3, #:upper16:__malloc_av_
 3671 01e4 3A6A     		ldr	r2, [r7, #32]
 3672 01e6 9A42     		cmp	r2, r3
 3673 01e8 31D0     		beq	.L7
2217:../target/stm32/malloc/mallocr.c ****     {
2218:../target/stm32/malloc/mallocr.c **** 
2219:../target/stm32/malloc/mallocr.c ****       /* There must have been an intervening foreign sbrk call. */
2220:../target/stm32/malloc/mallocr.c ****       /* A double fencepost is necessary to prevent consolidation */
2221:../target/stm32/malloc/mallocr.c **** 
2222:../target/stm32/malloc/mallocr.c ****       /* If not enough space to do this, then user did something very wrong */
2223:../target/stm32/malloc/mallocr.c ****       if (old_top_size < MINSIZE) 
 3674              		.loc 1 2223 0
 3675 01ea FB69     		ldr	r3, [r7, #28]
 3676 01ec 0F2B     		cmp	r3, #15
 3677 01ee 08D8     		bhi	.L13
2224:../target/stm32/malloc/mallocr.c ****       {
2225:../target/stm32/malloc/mallocr.c ****         set_head(top, PREV_INUSE); /* will force null return from malloc */
 3678              		.loc 1 2225 0
 3679 01f0 40F20003 		movw	r3, #:lower16:__malloc_av_
 3680 01f4 C0F20003 		movt	r3, #:upper16:__malloc_av_
 3681 01f8 9B68     		ldr	r3, [r3, #8]
 3682 01fa 4FF00102 		mov	r2, #1
 3683 01fe 5A60     		str	r2, [r3, #4]
2226:../target/stm32/malloc/mallocr.c ****         return;
 3684              		.loc 1 2226 0
 3685 0200 59E0     		b	.L1
 3686              	.L13:
2227:../target/stm32/malloc/mallocr.c ****       }
2228:../target/stm32/malloc/mallocr.c **** 
2229:../target/stm32/malloc/mallocr.c ****       /* Also keep size a multiple of MALLOC_ALIGNMENT */
2230:../target/stm32/malloc/mallocr.c ****       old_top_size = (old_top_size - 3*SIZE_SZ) & ~MALLOC_ALIGN_MASK;
 3687              		.loc 1 2230 0
 3688 0202 FB69     		ldr	r3, [r7, #28]
 3689 0204 A3F10C03 		sub	r3, r3, #12
 3690 0208 23F00703 		bic	r3, r3, #7
 3691 020c FB61     		str	r3, [r7, #28]
2231:../target/stm32/malloc/mallocr.c ****       set_head_size(old_top, old_top_size);
 3692              		.loc 1 2231 0
 3693 020e 3B6A     		ldr	r3, [r7, #32]
 3694 0210 5B68     		ldr	r3, [r3, #4]
 3695 0212 03F00102 		and	r2, r3, #1
 3696 0216 FB69     		ldr	r3, [r7, #28]
 3697 0218 1A43     		orrs	r2, r2, r3
 3698 021a 3B6A     		ldr	r3, [r7, #32]
 3699 021c 5A60     		str	r2, [r3, #4]
2232:../target/stm32/malloc/mallocr.c ****       chunk_at_offset(old_top, old_top_size          )->size =
 3700              		.loc 1 2232 0
 3701 021e 3A6A     		ldr	r2, [r7, #32]
 3702 0220 FB69     		ldr	r3, [r7, #28]
 3703 0222 D318     		adds	r3, r2, r3
 3704 0224 4FF00502 		mov	r2, #5
 3705 0228 5A60     		str	r2, [r3, #4]
2233:../target/stm32/malloc/mallocr.c ****         SIZE_SZ|PREV_INUSE;
2234:../target/stm32/malloc/mallocr.c ****       chunk_at_offset(old_top, old_top_size + SIZE_SZ)->size =
 3706              		.loc 1 2234 0
 3707 022a FB69     		ldr	r3, [r7, #28]
 3708 022c 03F10403 		add	r3, r3, #4
 3709 0230 3A6A     		ldr	r2, [r7, #32]
 3710 0232 D318     		adds	r3, r2, r3
 3711 0234 4FF00502 		mov	r2, #5
 3712 0238 5A60     		str	r2, [r3, #4]
2235:../target/stm32/malloc/mallocr.c ****         SIZE_SZ|PREV_INUSE;
2236:../target/stm32/malloc/mallocr.c ****       /* If possible, release the rest. */
2237:../target/stm32/malloc/mallocr.c ****       if (old_top_size >= MINSIZE) 
 3713              		.loc 1 2237 0
 3714 023a FB69     		ldr	r3, [r7, #28]
 3715 023c 0F2B     		cmp	r3, #15
 3716 023e 06D9     		bls	.L7
2238:../target/stm32/malloc/mallocr.c ****         fREe(RCALL chunk2mem(old_top));
 3717              		.loc 1 2238 0
 3718 0240 3B6A     		ldr	r3, [r7, #32]
 3719 0242 03F10803 		add	r3, r3, #8
 3720 0246 7868     		ldr	r0, [r7, #4]
 3721 0248 1946     		mov	r1, r3
 3722 024a FFF7FEFF 		bl	_free_r
 3723              	.L7:
2239:../target/stm32/malloc/mallocr.c ****     }
2240:../target/stm32/malloc/mallocr.c ****   }
2241:../target/stm32/malloc/mallocr.c **** 
2242:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)sbrked_mem > (unsigned long)max_sbrked_mem) 
 3724              		.loc 1 2242 0
 3725 024e 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3726 0252 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3727 0256 1B68     		ldr	r3, [r3, #0]
 3728 0258 1A46     		mov	r2, r3
 3729 025a 40F20003 		movw	r3, #:lower16:__malloc_max_sbrked_mem
 3730 025e C0F20003 		movt	r3, #:upper16:__malloc_max_sbrked_mem
 3731 0262 1B68     		ldr	r3, [r3, #0]
 3732 0264 9A42     		cmp	r2, r3
 3733 0266 0AD9     		bls	.L14
2243:../target/stm32/malloc/mallocr.c ****     max_sbrked_mem = sbrked_mem;
 3734              		.loc 1 2243 0
 3735 0268 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3736 026c C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3737 0270 1B68     		ldr	r3, [r3, #0]
 3738 0272 1A46     		mov	r2, r3
 3739 0274 40F20003 		movw	r3, #:lower16:__malloc_max_sbrked_mem
 3740 0278 C0F20003 		movt	r3, #:upper16:__malloc_max_sbrked_mem
 3741 027c 1A60     		str	r2, [r3, #0]
 3742              	.L14:
2244:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
2245:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)(mmapped_mem + sbrked_mem) > (unsigned long)max_total_mem) 
2246:../target/stm32/malloc/mallocr.c ****     max_total_mem = mmapped_mem + sbrked_mem;
2247:../target/stm32/malloc/mallocr.c **** #else
2248:../target/stm32/malloc/mallocr.c ****   if ((unsigned long)(sbrked_mem) > (unsigned long)max_total_mem) 
 3743              		.loc 1 2248 0
 3744 027e 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3745 0282 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3746 0286 1B68     		ldr	r3, [r3, #0]
 3747 0288 1A46     		mov	r2, r3
 3748 028a 40F20003 		movw	r3, #:lower16:__malloc_max_total_mem
 3749 028e C0F20003 		movt	r3, #:upper16:__malloc_max_total_mem
 3750 0292 1B68     		ldr	r3, [r3, #0]
 3751 0294 9A42     		cmp	r2, r3
 3752 0296 0ED9     		bls	.L1
2249:../target/stm32/malloc/mallocr.c ****     max_total_mem = sbrked_mem;
 3753              		.loc 1 2249 0
 3754 0298 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 3755 029c C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 3756 02a0 1B68     		ldr	r3, [r3, #0]
 3757 02a2 1A46     		mov	r2, r3
 3758 02a4 40F20003 		movw	r3, #:lower16:__malloc_max_total_mem
 3759 02a8 C0F20003 		movt	r3, #:upper16:__malloc_max_total_mem
 3760 02ac 1A60     		str	r2, [r3, #0]
 3761 02ae 02E0     		b	.L1
 3762              	.L15:
2170:../target/stm32/malloc/mallocr.c ****     return;
 3763              		.loc 1 2170 0
 3764 02b0 00BF     		nop
 3765 02b2 00E0     		b	.L1
 3766              	.L16:
 3767 02b4 00BF     		nop
 3768              	.L1:
2250:../target/stm32/malloc/mallocr.c **** #endif
2251:../target/stm32/malloc/mallocr.c **** 
2252:../target/stm32/malloc/mallocr.c ****   /* We always land on a page boundary */
2253:../target/stm32/malloc/mallocr.c ****   assert(((unsigned long)((char*)top + top_size) & (pagesz - 1)) == 0
2254:../target/stm32/malloc/mallocr.c **** 	 || correction_failed);
2255:../target/stm32/malloc/mallocr.c **** }
 3769              		.loc 1 2255 0
 3770 02b6 07F13807 		add	r7, r7, #56
 3771 02ba BD46     		mov	sp, r7
 3772 02bc 80BD     		pop	{r7, pc}
 3773              		.cfi_endproc
 3774              	.LFE0:
 3776 02be 00BF     		.section	.text._malloc_r,"ax",%progbits
 3777              		.align	2
 3778              		.global	_malloc_r
 3779              		.thumb
 3780              		.thumb_func
 3782              	_malloc_r:
 3783              	.LFB1:
2256:../target/stm32/malloc/mallocr.c **** 
2257:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_MALLOC */
2258:../target/stm32/malloc/mallocr.c **** 
2259:../target/stm32/malloc/mallocr.c **** 
2260:../target/stm32/malloc/mallocr.c **** /* Main public routines */
2261:../target/stm32/malloc/mallocr.c **** 
2262:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_MALLOC
2263:../target/stm32/malloc/mallocr.c **** 
2264:../target/stm32/malloc/mallocr.c **** /*
2265:../target/stm32/malloc/mallocr.c ****   Malloc Algorthim:
2266:../target/stm32/malloc/mallocr.c **** 
2267:../target/stm32/malloc/mallocr.c ****     The requested size is first converted into a usable form, `nb'.
2268:../target/stm32/malloc/mallocr.c ****     This currently means to add 4 bytes overhead plus possibly more to
2269:../target/stm32/malloc/mallocr.c ****     obtain 8-byte alignment and/or to obtain a size of at least
2270:../target/stm32/malloc/mallocr.c ****     MINSIZE (currently 16 bytes), the smallest allocatable size.
2271:../target/stm32/malloc/mallocr.c ****     (All fits are considered `exact' if they are within MINSIZE bytes.)
2272:../target/stm32/malloc/mallocr.c **** 
2273:../target/stm32/malloc/mallocr.c ****     From there, the first successful of the following steps is taken:
2274:../target/stm32/malloc/mallocr.c **** 
2275:../target/stm32/malloc/mallocr.c ****       1. The bin corresponding to the request size is scanned, and if
2276:../target/stm32/malloc/mallocr.c ****          a chunk of exactly the right size is found, it is taken.
2277:../target/stm32/malloc/mallocr.c **** 
2278:../target/stm32/malloc/mallocr.c ****       2. The most recently remaindered chunk is used if it is big
2279:../target/stm32/malloc/mallocr.c ****          enough.  This is a form of (roving) first fit, used only in
2280:../target/stm32/malloc/mallocr.c ****          the absence of exact fits. Runs of consecutive requests use
2281:../target/stm32/malloc/mallocr.c ****          the remainder of the chunk used for the previous such request
2282:../target/stm32/malloc/mallocr.c ****          whenever possible. This limited use of a first-fit style
2283:../target/stm32/malloc/mallocr.c ****          allocation strategy tends to give contiguous chunks
2284:../target/stm32/malloc/mallocr.c ****          coextensive lifetimes, which improves locality and can reduce
2285:../target/stm32/malloc/mallocr.c ****          fragmentation in the long run.
2286:../target/stm32/malloc/mallocr.c **** 
2287:../target/stm32/malloc/mallocr.c ****       3. Other bins are scanned in increasing size order, using a
2288:../target/stm32/malloc/mallocr.c ****          chunk big enough to fulfill the request, and splitting off
2289:../target/stm32/malloc/mallocr.c ****          any remainder.  This search is strictly by best-fit; i.e.,
2290:../target/stm32/malloc/mallocr.c ****          the smallest (with ties going to approximately the least
2291:../target/stm32/malloc/mallocr.c ****          recently used) chunk that fits is selected.
2292:../target/stm32/malloc/mallocr.c **** 
2293:../target/stm32/malloc/mallocr.c ****       4. If large enough, the chunk bordering the end of memory
2294:../target/stm32/malloc/mallocr.c ****          (`top') is split off. (This use of `top' is in accord with
2295:../target/stm32/malloc/mallocr.c ****          the best-fit search rule.  In effect, `top' is treated as
2296:../target/stm32/malloc/mallocr.c ****          larger (and thus less well fitting) than any other available
2297:../target/stm32/malloc/mallocr.c ****          chunk since it can be extended to be as large as necessary
2298:../target/stm32/malloc/mallocr.c ****          (up to system limitations).
2299:../target/stm32/malloc/mallocr.c **** 
2300:../target/stm32/malloc/mallocr.c ****       5. If the request size meets the mmap threshold and the
2301:../target/stm32/malloc/mallocr.c ****          system supports mmap, and there are few enough currently
2302:../target/stm32/malloc/mallocr.c ****          allocated mmapped regions, and a call to mmap succeeds,
2303:../target/stm32/malloc/mallocr.c ****          the request is allocated via direct memory mapping.
2304:../target/stm32/malloc/mallocr.c **** 
2305:../target/stm32/malloc/mallocr.c ****       6. Otherwise, the top of memory is extended by
2306:../target/stm32/malloc/mallocr.c ****          obtaining more space from the system (normally using sbrk,
2307:../target/stm32/malloc/mallocr.c ****          but definable to anything else via the MORECORE macro).
2308:../target/stm32/malloc/mallocr.c ****          Memory is gathered from the system (in system page-sized
2309:../target/stm32/malloc/mallocr.c ****          units) in a way that allows chunks obtained across different
2310:../target/stm32/malloc/mallocr.c ****          sbrk calls to be consolidated, but does not require
2311:../target/stm32/malloc/mallocr.c ****          contiguous memory. Thus, it should be safe to intersperse
2312:../target/stm32/malloc/mallocr.c ****          mallocs with other sbrk calls.
2313:../target/stm32/malloc/mallocr.c **** 
2314:../target/stm32/malloc/mallocr.c **** 
2315:../target/stm32/malloc/mallocr.c ****       All allocations are made from the the `lowest' part of any found
2316:../target/stm32/malloc/mallocr.c ****       chunk. (The implementation invariant is that prev_inuse is
2317:../target/stm32/malloc/mallocr.c ****       always true of any allocated chunk; i.e., that each allocated
2318:../target/stm32/malloc/mallocr.c ****       chunk borders either a previously allocated and still in-use chunk,
2319:../target/stm32/malloc/mallocr.c ****       or the base of its memory arena.)
2320:../target/stm32/malloc/mallocr.c **** 
2321:../target/stm32/malloc/mallocr.c **** */
2322:../target/stm32/malloc/mallocr.c **** 
2323:../target/stm32/malloc/mallocr.c **** #if __STD_C
2324:../target/stm32/malloc/mallocr.c **** Void_t* mALLOc(RARG size_t bytes)
2325:../target/stm32/malloc/mallocr.c **** #else
2326:../target/stm32/malloc/mallocr.c **** Void_t* mALLOc(RARG bytes) RDECL size_t bytes;
2327:../target/stm32/malloc/mallocr.c **** #endif
2328:../target/stm32/malloc/mallocr.c **** {
 3784              		.loc 1 2328 0
 3785              		.cfi_startproc
 3786              		@ args = 0, pretend = 0, frame = 64
 3787              		@ frame_needed = 1, uses_anonymous_args = 0
 3788 0000 80B5     		push	{r7, lr}
 3789              	.LCFI3:
 3790              		.cfi_def_cfa_offset 8
 3791 0002 90B0     		sub	sp, sp, #64
 3792              	.LCFI4:
 3793              		.cfi_def_cfa_offset 72
 3794 0004 00AF     		add	r7, sp, #0
 3795              		.cfi_offset 14, -4
 3796              		.cfi_offset 7, -8
 3797              	.LCFI5:
 3798              		.cfi_def_cfa_register 7
 3799 0006 7860     		str	r0, [r7, #4]
 3800 0008 3960     		str	r1, [r7, #0]
2329:../target/stm32/malloc/mallocr.c **** #ifdef MALLOC_PROVIDED
2330:../target/stm32/malloc/mallocr.c **** 
2331:../target/stm32/malloc/mallocr.c ****   return malloc (bytes); // Make sure that the pointer returned by malloc is returned back.
2332:../target/stm32/malloc/mallocr.c **** 
2333:../target/stm32/malloc/mallocr.c **** #else
2334:../target/stm32/malloc/mallocr.c **** 
2335:../target/stm32/malloc/mallocr.c ****   mchunkptr victim;                  /* inspected/selected chunk */
2336:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T victim_size;       /* its size */
2337:../target/stm32/malloc/mallocr.c ****   int       idx;                     /* index for bin traversal */
2338:../target/stm32/malloc/mallocr.c ****   mbinptr   bin;                     /* associated bin */
2339:../target/stm32/malloc/mallocr.c ****   mchunkptr remainder;               /* remainder from a split */
2340:../target/stm32/malloc/mallocr.c ****   long      remainder_size;          /* its size */
2341:../target/stm32/malloc/mallocr.c ****   int       remainder_index;         /* its bin index */
2342:../target/stm32/malloc/mallocr.c ****   unsigned long block;               /* block traverser bit */
2343:../target/stm32/malloc/mallocr.c ****   int       startidx;                /* first bin of a traversed block */
2344:../target/stm32/malloc/mallocr.c ****   mchunkptr fwd;                     /* misc temp for linking */
2345:../target/stm32/malloc/mallocr.c ****   mchunkptr bck;                     /* misc temp for linking */
2346:../target/stm32/malloc/mallocr.c ****   mbinptr q;                         /* misc temp */
2347:../target/stm32/malloc/mallocr.c **** 
2348:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T nb  = request2size(bytes);  /* padded request size; */
 3801              		.loc 1 2348 0
 3802 000a 3B68     		ldr	r3, [r7, #0]
 3803 000c 03F10B03 		add	r3, r3, #11
 3804 0010 162B     		cmp	r3, #22
 3805 0012 05D9     		bls	.L18
 3806              		.loc 1 2348 0 is_stmt 0 discriminator 1
 3807 0014 3B68     		ldr	r3, [r7, #0]
 3808 0016 03F10B03 		add	r3, r3, #11
 3809 001a 23F00703 		bic	r3, r3, #7
 3810 001e 01E0     		b	.L19
 3811              	.L18:
 3812              		.loc 1 2348 0 discriminator 2
 3813 0020 4FF01003 		mov	r3, #16
 3814              	.L19:
 3815              		.loc 1 2348 0 discriminator 3
 3816 0024 BB61     		str	r3, [r7, #24]
2349:../target/stm32/malloc/mallocr.c **** 
2350:../target/stm32/malloc/mallocr.c ****   /* Check for overflow and just fail, if so. */
2351:../target/stm32/malloc/mallocr.c ****   if (nb > INT_MAX || nb < bytes)
 3817              		.loc 1 2351 0 is_stmt 1 discriminator 3
 3818 0026 BB69     		ldr	r3, [r7, #24]
 3819 0028 002B     		cmp	r3, #0
 3820 002a 03DB     		blt	.L20
 3821              		.loc 1 2351 0 is_stmt 0 discriminator 1
 3822 002c BA69     		ldr	r2, [r7, #24]
 3823 002e 3B68     		ldr	r3, [r7, #0]
 3824 0030 9A42     		cmp	r2, r3
 3825 0032 06D2     		bcs	.L21
 3826              	.L20:
2352:../target/stm32/malloc/mallocr.c ****   {
2353:../target/stm32/malloc/mallocr.c ****     RERRNO = ENOMEM;
 3827              		.loc 1 2353 0 is_stmt 1
 3828 0034 7B68     		ldr	r3, [r7, #4]
 3829 0036 4FF00C02 		mov	r2, #12
 3830 003a 1A60     		str	r2, [r3, #0]
2354:../target/stm32/malloc/mallocr.c ****     return 0;
 3831              		.loc 1 2354 0
 3832 003c 4FF00003 		mov	r3, #0
 3833 0040 97E3     		b	.L22
 3834              	.L21:
2355:../target/stm32/malloc/mallocr.c ****   }
2356:../target/stm32/malloc/mallocr.c **** 
2357:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK;
 3835              		.loc 1 2357 0
 3836 0042 7868     		ldr	r0, [r7, #4]
 3837 0044 FFF7FEFF 		bl	__malloc_lock
2358:../target/stm32/malloc/mallocr.c **** 
2359:../target/stm32/malloc/mallocr.c ****   /* Check for exact match in a bin */
2360:../target/stm32/malloc/mallocr.c **** 
2361:../target/stm32/malloc/mallocr.c ****   if (is_small_request(nb))  /* Faster version for small requests */
 3838              		.loc 1 2361 0
 3839 0048 BA69     		ldr	r2, [r7, #24]
 3840 004a 40F2F713 		movw	r3, #503
 3841 004e 9A42     		cmp	r2, r3
 3842 0050 45D8     		bhi	.L23
2362:../target/stm32/malloc/mallocr.c ****   {
2363:../target/stm32/malloc/mallocr.c ****     idx = smallbin_index(nb); 
 3843              		.loc 1 2363 0
 3844 0052 BB69     		ldr	r3, [r7, #24]
 3845 0054 4FEAD303 		lsr	r3, r3, #3
 3846 0058 BB63     		str	r3, [r7, #56]
2364:../target/stm32/malloc/mallocr.c **** 
2365:../target/stm32/malloc/mallocr.c ****     /* No traversal or size check necessary for small bins.  */
2366:../target/stm32/malloc/mallocr.c **** 
2367:../target/stm32/malloc/mallocr.c ****     q = bin_at(idx);
 3847              		.loc 1 2367 0
 3848 005a 40F20003 		movw	r3, #:lower16:__malloc_av_
 3849 005e C0F20003 		movt	r3, #:upper16:__malloc_av_
 3850 0062 BA6B     		ldr	r2, [r7, #56]
 3851 0064 4FEAC202 		lsl	r2, r2, #3
 3852 0068 9B18     		adds	r3, r3, r2
 3853 006a FB61     		str	r3, [r7, #28]
2368:../target/stm32/malloc/mallocr.c ****     victim = last(q);
 3854              		.loc 1 2368 0
 3855 006c FB69     		ldr	r3, [r7, #28]
 3856 006e DB68     		ldr	r3, [r3, #12]
 3857 0070 FB63     		str	r3, [r7, #60]
2369:../target/stm32/malloc/mallocr.c **** 
2370:../target/stm32/malloc/mallocr.c **** #if MALLOC_ALIGN != 16
2371:../target/stm32/malloc/mallocr.c ****     /* Also scan the next one, since it would have a remainder < MINSIZE */
2372:../target/stm32/malloc/mallocr.c ****     if (victim == q)
 3858              		.loc 1 2372 0
 3859 0072 FA6B     		ldr	r2, [r7, #60]
 3860 0074 FB69     		ldr	r3, [r7, #28]
 3861 0076 9A42     		cmp	r2, r3
 3862 0078 06D1     		bne	.L24
2373:../target/stm32/malloc/mallocr.c ****     {
2374:../target/stm32/malloc/mallocr.c ****       q = next_bin(q);
 3863              		.loc 1 2374 0
 3864 007a FB69     		ldr	r3, [r7, #28]
 3865 007c 03F10803 		add	r3, r3, #8
 3866 0080 FB61     		str	r3, [r7, #28]
2375:../target/stm32/malloc/mallocr.c ****       victim = last(q);
 3867              		.loc 1 2375 0
 3868 0082 FB69     		ldr	r3, [r7, #28]
 3869 0084 DB68     		ldr	r3, [r3, #12]
 3870 0086 FB63     		str	r3, [r7, #60]
 3871              	.L24:
2376:../target/stm32/malloc/mallocr.c ****     }
2377:../target/stm32/malloc/mallocr.c **** #endif
2378:../target/stm32/malloc/mallocr.c ****     if (victim != q)
 3872              		.loc 1 2378 0
 3873 0088 FA6B     		ldr	r2, [r7, #60]
 3874 008a FB69     		ldr	r3, [r7, #28]
 3875 008c 9A42     		cmp	r2, r3
 3876 008e 21D0     		beq	.L25
2379:../target/stm32/malloc/mallocr.c ****     {
2380:../target/stm32/malloc/mallocr.c ****       victim_size = chunksize(victim);
 3877              		.loc 1 2380 0
 3878 0090 FB6B     		ldr	r3, [r7, #60]
 3879 0092 5B68     		ldr	r3, [r3, #4]
 3880 0094 23F00303 		bic	r3, r3, #3
 3881 0098 7B61     		str	r3, [r7, #20]
2381:../target/stm32/malloc/mallocr.c ****       unlink(victim, bck, fwd);
 3882              		.loc 1 2381 0
 3883 009a FB6B     		ldr	r3, [r7, #60]
 3884 009c DB68     		ldr	r3, [r3, #12]
 3885 009e 3B62     		str	r3, [r7, #32]
 3886 00a0 FB6B     		ldr	r3, [r7, #60]
 3887 00a2 9B68     		ldr	r3, [r3, #8]
 3888 00a4 7B62     		str	r3, [r7, #36]
 3889 00a6 7B6A     		ldr	r3, [r7, #36]
 3890 00a8 3A6A     		ldr	r2, [r7, #32]
 3891 00aa DA60     		str	r2, [r3, #12]
 3892 00ac 3B6A     		ldr	r3, [r7, #32]
 3893 00ae 7A6A     		ldr	r2, [r7, #36]
 3894 00b0 9A60     		str	r2, [r3, #8]
2382:../target/stm32/malloc/mallocr.c ****       set_inuse_bit_at_offset(victim, victim_size);
 3895              		.loc 1 2382 0
 3896 00b2 FA6B     		ldr	r2, [r7, #60]
 3897 00b4 7B69     		ldr	r3, [r7, #20]
 3898 00b6 D318     		adds	r3, r2, r3
 3899 00b8 F96B     		ldr	r1, [r7, #60]
 3900 00ba 7A69     		ldr	r2, [r7, #20]
 3901 00bc 8A18     		adds	r2, r1, r2
 3902 00be 5268     		ldr	r2, [r2, #4]
 3903 00c0 42F00102 		orr	r2, r2, #1
 3904 00c4 5A60     		str	r2, [r3, #4]
2383:../target/stm32/malloc/mallocr.c ****       check_malloced_chunk(victim, nb);
2384:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
 3905              		.loc 1 2384 0
 3906 00c6 7868     		ldr	r0, [r7, #4]
 3907 00c8 FFF7FEFF 		bl	__malloc_unlock
2385:../target/stm32/malloc/mallocr.c ****       return chunk2mem(victim);
 3908              		.loc 1 2385 0
 3909 00cc FB6B     		ldr	r3, [r7, #60]
 3910 00ce 03F10803 		add	r3, r3, #8
 3911 00d2 4EE3     		b	.L22
 3912              	.L25:
2386:../target/stm32/malloc/mallocr.c ****     }
2387:../target/stm32/malloc/mallocr.c **** 
2388:../target/stm32/malloc/mallocr.c ****     idx += 2; /* Set for bin scan below. We've already scanned 2 bins. */
 3913              		.loc 1 2388 0
 3914 00d4 BB6B     		ldr	r3, [r7, #56]
 3915 00d6 03F10203 		add	r3, r3, #2
 3916 00da BB63     		str	r3, [r7, #56]
 3917 00dc 8EE0     		b	.L26
 3918              	.L23:
2389:../target/stm32/malloc/mallocr.c **** 
2390:../target/stm32/malloc/mallocr.c ****   }
2391:../target/stm32/malloc/mallocr.c ****   else
2392:../target/stm32/malloc/mallocr.c ****   {
2393:../target/stm32/malloc/mallocr.c ****     idx = bin_index(nb);
 3919              		.loc 1 2393 0
 3920 00de BB69     		ldr	r3, [r7, #24]
 3921 00e0 4FEA5323 		lsr	r3, r3, #9
 3922 00e4 002B     		cmp	r3, #0
 3923 00e6 03D1     		bne	.L27
 3924              		.loc 1 2393 0 is_stmt 0 discriminator 1
 3925 00e8 BB69     		ldr	r3, [r7, #24]
 3926 00ea 4FEAD303 		lsr	r3, r3, #3
 3927 00ee 3BE0     		b	.L28
 3928              	.L27:
 3929              		.loc 1 2393 0 discriminator 2
 3930 00f0 BB69     		ldr	r3, [r7, #24]
 3931 00f2 4FEA5323 		lsr	r3, r3, #9
 3932 00f6 042B     		cmp	r3, #4
 3933 00f8 05D8     		bhi	.L29
 3934              		.loc 1 2393 0 discriminator 3
 3935 00fa BB69     		ldr	r3, [r7, #24]
 3936 00fc 4FEA9313 		lsr	r3, r3, #6
 3937 0100 03F13803 		add	r3, r3, #56
 3938 0104 30E0     		b	.L30
 3939              	.L29:
 3940              		.loc 1 2393 0 discriminator 4
 3941 0106 BB69     		ldr	r3, [r7, #24]
 3942 0108 4FEA5323 		lsr	r3, r3, #9
 3943 010c 142B     		cmp	r3, #20
 3944 010e 05D8     		bhi	.L31
 3945              		.loc 1 2393 0 discriminator 5
 3946 0110 BB69     		ldr	r3, [r7, #24]
 3947 0112 4FEA5323 		lsr	r3, r3, #9
 3948 0116 03F15B03 		add	r3, r3, #91
 3949 011a 25E0     		b	.L32
 3950              	.L31:
 3951              		.loc 1 2393 0 discriminator 6
 3952 011c BB69     		ldr	r3, [r7, #24]
 3953 011e 4FEA5323 		lsr	r3, r3, #9
 3954 0122 542B     		cmp	r3, #84
 3955 0124 05D8     		bhi	.L33
 3956              		.loc 1 2393 0 discriminator 7
 3957 0126 BB69     		ldr	r3, [r7, #24]
 3958 0128 4FEA1333 		lsr	r3, r3, #12
 3959 012c 03F16E03 		add	r3, r3, #110
 3960 0130 1AE0     		b	.L34
 3961              	.L33:
 3962              		.loc 1 2393 0 discriminator 8
 3963 0132 BB69     		ldr	r3, [r7, #24]
 3964 0134 4FEA5323 		lsr	r3, r3, #9
 3965 0138 B3F5AA7F 		cmp	r3, #340
 3966 013c 05D8     		bhi	.L35
 3967              		.loc 1 2393 0 discriminator 9
 3968 013e BB69     		ldr	r3, [r7, #24]
 3969 0140 4FEAD333 		lsr	r3, r3, #15
 3970 0144 03F17703 		add	r3, r3, #119
 3971 0148 0EE0     		b	.L36
 3972              	.L35:
 3973              		.loc 1 2393 0 discriminator 10
 3974 014a BB69     		ldr	r3, [r7, #24]
 3975 014c 4FEA5322 		lsr	r2, r3, #9
 3976 0150 40F25453 		movw	r3, #1364
 3977 0154 9A42     		cmp	r2, r3
 3978 0156 05D8     		bhi	.L37
 3979              		.loc 1 2393 0 discriminator 11
 3980 0158 BB69     		ldr	r3, [r7, #24]
 3981 015a 4FEA9343 		lsr	r3, r3, #18
 3982 015e 03F17C03 		add	r3, r3, #124
 3983 0162 01E0     		b	.L38
 3984              	.L37:
 3985              		.loc 1 2393 0 discriminator 12
 3986 0164 4FF07E03 		mov	r3, #126
 3987              	.L38:
 3988              	.L36:
 3989              	.L34:
 3990              	.L32:
 3991              	.L30:
 3992              	.L28:
 3993              		.loc 1 2393 0 discriminator 18
 3994 0168 BB63     		str	r3, [r7, #56]
2394:../target/stm32/malloc/mallocr.c ****     bin = bin_at(idx);
 3995              		.loc 1 2394 0 is_stmt 1 discriminator 18
 3996 016a 40F20003 		movw	r3, #:lower16:__malloc_av_
 3997 016e C0F20003 		movt	r3, #:upper16:__malloc_av_
 3998 0172 BA6B     		ldr	r2, [r7, #56]
 3999 0174 4FEAC202 		lsl	r2, r2, #3
 4000 0178 9B18     		adds	r3, r3, r2
 4001 017a 7B63     		str	r3, [r7, #52]
2395:../target/stm32/malloc/mallocr.c **** 
2396:../target/stm32/malloc/mallocr.c ****     for (victim = last(bin); victim != bin; victim = victim->bk)
 4002              		.loc 1 2396 0 discriminator 18
 4003 017c 7B6B     		ldr	r3, [r7, #52]
 4004 017e DB68     		ldr	r3, [r3, #12]
 4005 0180 FB63     		str	r3, [r7, #60]
 4006 0182 33E0     		b	.L39
 4007              	.L43:
2397:../target/stm32/malloc/mallocr.c ****     {
2398:../target/stm32/malloc/mallocr.c ****       victim_size = chunksize(victim);
 4008              		.loc 1 2398 0
 4009 0184 FB6B     		ldr	r3, [r7, #60]
 4010 0186 5B68     		ldr	r3, [r3, #4]
 4011 0188 23F00303 		bic	r3, r3, #3
 4012 018c 7B61     		str	r3, [r7, #20]
2399:../target/stm32/malloc/mallocr.c ****       remainder_size = long_sub_size_t(victim_size, nb);
 4013              		.loc 1 2399 0
 4014 018e 7A69     		ldr	r2, [r7, #20]
 4015 0190 BB69     		ldr	r3, [r7, #24]
 4016 0192 D31A     		subs	r3, r2, r3
 4017 0194 3B63     		str	r3, [r7, #48]
2400:../target/stm32/malloc/mallocr.c ****       
2401:../target/stm32/malloc/mallocr.c ****       if (remainder_size >= (long)MINSIZE) /* too big */
 4018              		.loc 1 2401 0
 4019 0196 3B6B     		ldr	r3, [r7, #48]
 4020 0198 0F2B     		cmp	r3, #15
 4021 019a 04DD     		ble	.L40
2402:../target/stm32/malloc/mallocr.c ****       {
2403:../target/stm32/malloc/mallocr.c ****         --idx; /* adjust to rescan below after checking last remainder */
 4022              		.loc 1 2403 0
 4023 019c BB6B     		ldr	r3, [r7, #56]
 4024 019e 03F1FF33 		add	r3, r3, #-1
 4025 01a2 BB63     		str	r3, [r7, #56]
2404:../target/stm32/malloc/mallocr.c ****         break;   
 4026              		.loc 1 2404 0
 4027 01a4 26E0     		b	.L41
 4028              	.L40:
2405:../target/stm32/malloc/mallocr.c ****       }
2406:../target/stm32/malloc/mallocr.c **** 
2407:../target/stm32/malloc/mallocr.c ****       else if (remainder_size >= 0) /* exact fit */
 4029              		.loc 1 2407 0
 4030 01a6 3B6B     		ldr	r3, [r7, #48]
 4031 01a8 002B     		cmp	r3, #0
 4032 01aa 1CDB     		blt	.L42
2408:../target/stm32/malloc/mallocr.c ****       {
2409:../target/stm32/malloc/mallocr.c ****         unlink(victim, bck, fwd);
 4033              		.loc 1 2409 0
 4034 01ac FB6B     		ldr	r3, [r7, #60]
 4035 01ae DB68     		ldr	r3, [r3, #12]
 4036 01b0 3B62     		str	r3, [r7, #32]
 4037 01b2 FB6B     		ldr	r3, [r7, #60]
 4038 01b4 9B68     		ldr	r3, [r3, #8]
 4039 01b6 7B62     		str	r3, [r7, #36]
 4040 01b8 7B6A     		ldr	r3, [r7, #36]
 4041 01ba 3A6A     		ldr	r2, [r7, #32]
 4042 01bc DA60     		str	r2, [r3, #12]
 4043 01be 3B6A     		ldr	r3, [r7, #32]
 4044 01c0 7A6A     		ldr	r2, [r7, #36]
 4045 01c2 9A60     		str	r2, [r3, #8]
2410:../target/stm32/malloc/mallocr.c ****         set_inuse_bit_at_offset(victim, victim_size);
 4046              		.loc 1 2410 0
 4047 01c4 FA6B     		ldr	r2, [r7, #60]
 4048 01c6 7B69     		ldr	r3, [r7, #20]
 4049 01c8 D318     		adds	r3, r2, r3
 4050 01ca F96B     		ldr	r1, [r7, #60]
 4051 01cc 7A69     		ldr	r2, [r7, #20]
 4052 01ce 8A18     		adds	r2, r1, r2
 4053 01d0 5268     		ldr	r2, [r2, #4]
 4054 01d2 42F00102 		orr	r2, r2, #1
 4055 01d6 5A60     		str	r2, [r3, #4]
2411:../target/stm32/malloc/mallocr.c ****         check_malloced_chunk(victim, nb);
2412:../target/stm32/malloc/mallocr.c **** 	MALLOC_UNLOCK;
 4056              		.loc 1 2412 0
 4057 01d8 7868     		ldr	r0, [r7, #4]
 4058 01da FFF7FEFF 		bl	__malloc_unlock
2413:../target/stm32/malloc/mallocr.c ****         return chunk2mem(victim);
 4059              		.loc 1 2413 0
 4060 01de FB6B     		ldr	r3, [r7, #60]
 4061 01e0 03F10803 		add	r3, r3, #8
 4062 01e4 C5E2     		b	.L22
 4063              	.L42:
2396:../target/stm32/malloc/mallocr.c ****     for (victim = last(bin); victim != bin; victim = victim->bk)
 4064              		.loc 1 2396 0
 4065 01e6 FB6B     		ldr	r3, [r7, #60]
 4066 01e8 DB68     		ldr	r3, [r3, #12]
 4067 01ea FB63     		str	r3, [r7, #60]
 4068              	.L39:
2396:../target/stm32/malloc/mallocr.c ****     for (victim = last(bin); victim != bin; victim = victim->bk)
 4069              		.loc 1 2396 0 is_stmt 0 discriminator 1
 4070 01ec FA6B     		ldr	r2, [r7, #60]
 4071 01ee 7B6B     		ldr	r3, [r7, #52]
 4072 01f0 9A42     		cmp	r2, r3
 4073 01f2 C7D1     		bne	.L43
 4074              	.L41:
2414:../target/stm32/malloc/mallocr.c ****       }
2415:../target/stm32/malloc/mallocr.c ****     }
2416:../target/stm32/malloc/mallocr.c **** 
2417:../target/stm32/malloc/mallocr.c ****     ++idx; 
 4075              		.loc 1 2417 0 is_stmt 1
 4076 01f4 BB6B     		ldr	r3, [r7, #56]
 4077 01f6 03F10103 		add	r3, r3, #1
 4078 01fa BB63     		str	r3, [r7, #56]
 4079              	.L26:
2418:../target/stm32/malloc/mallocr.c **** 
2419:../target/stm32/malloc/mallocr.c ****   }
2420:../target/stm32/malloc/mallocr.c **** 
2421:../target/stm32/malloc/mallocr.c ****   /* Try to use the last split-off remainder */
2422:../target/stm32/malloc/mallocr.c **** 
2423:../target/stm32/malloc/mallocr.c ****   if ( (victim = last_remainder->fd) != last_remainder)
 4080              		.loc 1 2423 0
 4081 01fc 40F20003 		movw	r3, #:lower16:__malloc_av_
 4082 0200 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4083 0204 03F10803 		add	r3, r3, #8
 4084 0208 9B68     		ldr	r3, [r3, #8]
 4085 020a FB63     		str	r3, [r7, #60]
 4086 020c 40F20003 		movw	r3, #:lower16:__malloc_av_
 4087 0210 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4088 0214 03F10803 		add	r3, r3, #8
 4089 0218 FA6B     		ldr	r2, [r7, #60]
 4090 021a 9A42     		cmp	r2, r3
 4091 021c 00F03481 		beq	.L44
2424:../target/stm32/malloc/mallocr.c ****   {
2425:../target/stm32/malloc/mallocr.c ****     victim_size = chunksize(victim);
 4092              		.loc 1 2425 0
 4093 0220 FB6B     		ldr	r3, [r7, #60]
 4094 0222 5B68     		ldr	r3, [r3, #4]
 4095 0224 23F00303 		bic	r3, r3, #3
 4096 0228 7B61     		str	r3, [r7, #20]
2426:../target/stm32/malloc/mallocr.c ****     remainder_size = long_sub_size_t(victim_size, nb);
 4097              		.loc 1 2426 0
 4098 022a 7A69     		ldr	r2, [r7, #20]
 4099 022c BB69     		ldr	r3, [r7, #24]
 4100 022e D31A     		subs	r3, r2, r3
 4101 0230 3B63     		str	r3, [r7, #48]
2427:../target/stm32/malloc/mallocr.c **** 
2428:../target/stm32/malloc/mallocr.c ****     if (remainder_size >= (long)MINSIZE) /* re-split */
 4102              		.loc 1 2428 0
 4103 0232 3B6B     		ldr	r3, [r7, #48]
 4104 0234 0F2B     		cmp	r3, #15
 4105 0236 36DD     		ble	.L45
2429:../target/stm32/malloc/mallocr.c ****     {
2430:../target/stm32/malloc/mallocr.c ****       remainder = chunk_at_offset(victim, nb);
 4106              		.loc 1 2430 0
 4107 0238 FA6B     		ldr	r2, [r7, #60]
 4108 023a BB69     		ldr	r3, [r7, #24]
 4109 023c D318     		adds	r3, r2, r3
 4110 023e 3B61     		str	r3, [r7, #16]
2431:../target/stm32/malloc/mallocr.c ****       set_head(victim, nb | PREV_INUSE);
 4111              		.loc 1 2431 0
 4112 0240 BB69     		ldr	r3, [r7, #24]
 4113 0242 43F00102 		orr	r2, r3, #1
 4114 0246 FB6B     		ldr	r3, [r7, #60]
 4115 0248 5A60     		str	r2, [r3, #4]
2432:../target/stm32/malloc/mallocr.c ****       link_last_remainder(remainder);
 4116              		.loc 1 2432 0
 4117 024a 40F20003 		movw	r3, #:lower16:__malloc_av_
 4118 024e C0F20003 		movt	r3, #:upper16:__malloc_av_
 4119 0252 03F10802 		add	r2, r3, #8
 4120 0256 40F20003 		movw	r3, #:lower16:__malloc_av_
 4121 025a C0F20003 		movt	r3, #:upper16:__malloc_av_
 4122 025e 03F10803 		add	r3, r3, #8
 4123 0262 3969     		ldr	r1, [r7, #16]
 4124 0264 D960     		str	r1, [r3, #12]
 4125 0266 DB68     		ldr	r3, [r3, #12]
 4126 0268 9360     		str	r3, [r2, #8]
 4127 026a 40F20003 		movw	r3, #:lower16:__malloc_av_
 4128 026e C0F20003 		movt	r3, #:upper16:__malloc_av_
 4129 0272 03F10802 		add	r2, r3, #8
 4130 0276 3B69     		ldr	r3, [r7, #16]
 4131 0278 DA60     		str	r2, [r3, #12]
 4132 027a 3B69     		ldr	r3, [r7, #16]
 4133 027c DA68     		ldr	r2, [r3, #12]
 4134 027e 3B69     		ldr	r3, [r7, #16]
 4135 0280 9A60     		str	r2, [r3, #8]
2433:../target/stm32/malloc/mallocr.c ****       set_head(remainder, remainder_size | PREV_INUSE);
 4136              		.loc 1 2433 0
 4137 0282 3B6B     		ldr	r3, [r7, #48]
 4138 0284 43F00103 		orr	r3, r3, #1
 4139 0288 1A46     		mov	r2, r3
 4140 028a 3B69     		ldr	r3, [r7, #16]
 4141 028c 5A60     		str	r2, [r3, #4]
2434:../target/stm32/malloc/mallocr.c ****       set_foot(remainder, remainder_size);
 4142              		.loc 1 2434 0
 4143 028e 3B6B     		ldr	r3, [r7, #48]
 4144 0290 3A69     		ldr	r2, [r7, #16]
 4145 0292 D318     		adds	r3, r2, r3
 4146 0294 3A6B     		ldr	r2, [r7, #48]
 4147 0296 1A60     		str	r2, [r3, #0]
2435:../target/stm32/malloc/mallocr.c ****       check_malloced_chunk(victim, nb);
2436:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
 4148              		.loc 1 2436 0
 4149 0298 7868     		ldr	r0, [r7, #4]
 4150 029a FFF7FEFF 		bl	__malloc_unlock
2437:../target/stm32/malloc/mallocr.c ****       return chunk2mem(victim);
 4151              		.loc 1 2437 0
 4152 029e FB6B     		ldr	r3, [r7, #60]
 4153 02a0 03F10803 		add	r3, r3, #8
 4154 02a4 65E2     		b	.L22
 4155              	.L45:
2438:../target/stm32/malloc/mallocr.c ****     }
2439:../target/stm32/malloc/mallocr.c **** 
2440:../target/stm32/malloc/mallocr.c ****     clear_last_remainder;
 4156              		.loc 1 2440 0
 4157 02a6 40F20003 		movw	r3, #:lower16:__malloc_av_
 4158 02aa C0F20003 		movt	r3, #:upper16:__malloc_av_
 4159 02ae 03F10801 		add	r1, r3, #8
 4160 02b2 40F20003 		movw	r3, #:lower16:__malloc_av_
 4161 02b6 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4162 02ba 03F10802 		add	r2, r3, #8
 4163 02be 40F20003 		movw	r3, #:lower16:__malloc_av_
 4164 02c2 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4165 02c6 03F10803 		add	r3, r3, #8
 4166 02ca D360     		str	r3, [r2, #12]
 4167 02cc D368     		ldr	r3, [r2, #12]
 4168 02ce 8B60     		str	r3, [r1, #8]
2441:../target/stm32/malloc/mallocr.c **** 
2442:../target/stm32/malloc/mallocr.c ****     if (remainder_size >= 0)  /* exhaust */
 4169              		.loc 1 2442 0
 4170 02d0 3B6B     		ldr	r3, [r7, #48]
 4171 02d2 002B     		cmp	r3, #0
 4172 02d4 10DB     		blt	.L46
2443:../target/stm32/malloc/mallocr.c ****     {
2444:../target/stm32/malloc/mallocr.c ****       set_inuse_bit_at_offset(victim, victim_size);
 4173              		.loc 1 2444 0
 4174 02d6 FA6B     		ldr	r2, [r7, #60]
 4175 02d8 7B69     		ldr	r3, [r7, #20]
 4176 02da D318     		adds	r3, r2, r3
 4177 02dc F96B     		ldr	r1, [r7, #60]
 4178 02de 7A69     		ldr	r2, [r7, #20]
 4179 02e0 8A18     		adds	r2, r1, r2
 4180 02e2 5268     		ldr	r2, [r2, #4]
 4181 02e4 42F00102 		orr	r2, r2, #1
 4182 02e8 5A60     		str	r2, [r3, #4]
2445:../target/stm32/malloc/mallocr.c ****       check_malloced_chunk(victim, nb);
2446:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
 4183              		.loc 1 2446 0
 4184 02ea 7868     		ldr	r0, [r7, #4]
 4185 02ec FFF7FEFF 		bl	__malloc_unlock
2447:../target/stm32/malloc/mallocr.c ****       return chunk2mem(victim);
 4186              		.loc 1 2447 0
 4187 02f0 FB6B     		ldr	r3, [r7, #60]
 4188 02f2 03F10803 		add	r3, r3, #8
 4189 02f6 3CE2     		b	.L22
 4190              	.L46:
2448:../target/stm32/malloc/mallocr.c ****     }
2449:../target/stm32/malloc/mallocr.c **** 
2450:../target/stm32/malloc/mallocr.c ****     /* Else place in bin */
2451:../target/stm32/malloc/mallocr.c **** 
2452:../target/stm32/malloc/mallocr.c ****     frontlink(victim, victim_size, remainder_index, bck, fwd);
 4191              		.loc 1 2452 0
 4192 02f8 7A69     		ldr	r2, [r7, #20]
 4193 02fa 40F2FF13 		movw	r3, #511
 4194 02fe 9A42     		cmp	r2, r3
 4195 0300 35D8     		bhi	.L47
 4196              		.loc 1 2452 0 is_stmt 0 discriminator 1
 4197 0302 7B69     		ldr	r3, [r7, #20]
 4198 0304 4FEAD303 		lsr	r3, r3, #3
 4199 0308 FB60     		str	r3, [r7, #12]
 4200 030a 40F20002 		movw	r2, #:lower16:__malloc_av_
 4201 030e C0F20002 		movt	r2, #:upper16:__malloc_av_
 4202 0312 40F20003 		movw	r3, #:lower16:__malloc_av_
 4203 0316 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4204 031a 5968     		ldr	r1, [r3, #4]
 4205 031c FB68     		ldr	r3, [r7, #12]
 4206 031e 03F10300 		add	r0, r3, #3
 4207 0322 002B     		cmp	r3, #0
 4208 0324 B8BF     		it	lt
 4209 0326 0346     		movlt	r3, r0
 4210 0328 4FEAA303 		asr	r3, r3, #2
 4211 032c 4FF00100 		mov	r0, #1
 4212 0330 00FA03F3 		lsl	r3, r0, r3
 4213 0334 41EA0303 		orr	r3, r1, r3
 4214 0338 5360     		str	r3, [r2, #4]
 4215 033a 40F20003 		movw	r3, #:lower16:__malloc_av_
 4216 033e C0F20003 		movt	r3, #:upper16:__malloc_av_
 4217 0342 FA68     		ldr	r2, [r7, #12]
 4218 0344 4FEAC202 		lsl	r2, r2, #3
 4219 0348 9B18     		adds	r3, r3, r2
 4220 034a 3B62     		str	r3, [r7, #32]
 4221 034c 3B6A     		ldr	r3, [r7, #32]
 4222 034e 9B68     		ldr	r3, [r3, #8]
 4223 0350 7B62     		str	r3, [r7, #36]
 4224 0352 FB6B     		ldr	r3, [r7, #60]
 4225 0354 3A6A     		ldr	r2, [r7, #32]
 4226 0356 DA60     		str	r2, [r3, #12]
 4227 0358 FB6B     		ldr	r3, [r7, #60]
 4228 035a 7A6A     		ldr	r2, [r7, #36]
 4229 035c 9A60     		str	r2, [r3, #8]
 4230 035e 3B6A     		ldr	r3, [r7, #32]
 4231 0360 FA6B     		ldr	r2, [r7, #60]
 4232 0362 9A60     		str	r2, [r3, #8]
 4233 0364 3B6A     		ldr	r3, [r7, #32]
 4234 0366 9A68     		ldr	r2, [r3, #8]
 4235 0368 7B6A     		ldr	r3, [r7, #36]
 4236 036a DA60     		str	r2, [r3, #12]
 4237 036c 8CE0     		b	.L44
 4238              	.L47:
 4239              		.loc 1 2452 0 discriminator 2
 4240 036e 7B69     		ldr	r3, [r7, #20]
 4241 0370 4FEA5323 		lsr	r3, r3, #9
 4242 0374 002B     		cmp	r3, #0
 4243 0376 03D1     		bne	.L48
 4244              		.loc 1 2452 0 discriminator 3
 4245 0378 7B69     		ldr	r3, [r7, #20]
 4246 037a 4FEAD303 		lsr	r3, r3, #3
 4247 037e 3BE0     		b	.L49
 4248              	.L48:
 4249              		.loc 1 2452 0 discriminator 4
 4250 0380 7B69     		ldr	r3, [r7, #20]
 4251 0382 4FEA5323 		lsr	r3, r3, #9
 4252 0386 042B     		cmp	r3, #4
 4253 0388 05D8     		bhi	.L50
 4254              		.loc 1 2452 0 discriminator 5
 4255 038a 7B69     		ldr	r3, [r7, #20]
 4256 038c 4FEA9313 		lsr	r3, r3, #6
 4257 0390 03F13803 		add	r3, r3, #56
 4258 0394 30E0     		b	.L51
 4259              	.L50:
 4260              		.loc 1 2452 0 discriminator 6
 4261 0396 7B69     		ldr	r3, [r7, #20]
 4262 0398 4FEA5323 		lsr	r3, r3, #9
 4263 039c 142B     		cmp	r3, #20
 4264 039e 05D8     		bhi	.L52
 4265              		.loc 1 2452 0 discriminator 7
 4266 03a0 7B69     		ldr	r3, [r7, #20]
 4267 03a2 4FEA5323 		lsr	r3, r3, #9
 4268 03a6 03F15B03 		add	r3, r3, #91
 4269 03aa 25E0     		b	.L53
 4270              	.L52:
 4271              		.loc 1 2452 0 discriminator 8
 4272 03ac 7B69     		ldr	r3, [r7, #20]
 4273 03ae 4FEA5323 		lsr	r3, r3, #9
 4274 03b2 542B     		cmp	r3, #84
 4275 03b4 05D8     		bhi	.L54
 4276              		.loc 1 2452 0 discriminator 9
 4277 03b6 7B69     		ldr	r3, [r7, #20]
 4278 03b8 4FEA1333 		lsr	r3, r3, #12
 4279 03bc 03F16E03 		add	r3, r3, #110
 4280 03c0 1AE0     		b	.L55
 4281              	.L54:
 4282              		.loc 1 2452 0 discriminator 10
 4283 03c2 7B69     		ldr	r3, [r7, #20]
 4284 03c4 4FEA5323 		lsr	r3, r3, #9
 4285 03c8 B3F5AA7F 		cmp	r3, #340
 4286 03cc 05D8     		bhi	.L56
 4287              		.loc 1 2452 0 discriminator 11
 4288 03ce 7B69     		ldr	r3, [r7, #20]
 4289 03d0 4FEAD333 		lsr	r3, r3, #15
 4290 03d4 03F17703 		add	r3, r3, #119
 4291 03d8 0EE0     		b	.L57
 4292              	.L56:
 4293              		.loc 1 2452 0 discriminator 12
 4294 03da 7B69     		ldr	r3, [r7, #20]
 4295 03dc 4FEA5322 		lsr	r2, r3, #9
 4296 03e0 40F25453 		movw	r3, #1364
 4297 03e4 9A42     		cmp	r2, r3
 4298 03e6 05D8     		bhi	.L58
 4299              		.loc 1 2452 0 discriminator 13
 4300 03e8 7B69     		ldr	r3, [r7, #20]
 4301 03ea 4FEA9343 		lsr	r3, r3, #18
 4302 03ee 03F17C03 		add	r3, r3, #124
 4303 03f2 01E0     		b	.L59
 4304              	.L58:
 4305              		.loc 1 2452 0 discriminator 14
 4306 03f4 4FF07E03 		mov	r3, #126
 4307              	.L59:
 4308              	.L57:
 4309              	.L55:
 4310              	.L53:
 4311              	.L51:
 4312              	.L49:
 4313              		.loc 1 2452 0 discriminator 20
 4314 03f8 FB60     		str	r3, [r7, #12]
 4315 03fa 40F20003 		movw	r3, #:lower16:__malloc_av_
 4316 03fe C0F20003 		movt	r3, #:upper16:__malloc_av_
 4317 0402 FA68     		ldr	r2, [r7, #12]
 4318 0404 4FEAC202 		lsl	r2, r2, #3
 4319 0408 9B18     		adds	r3, r3, r2
 4320 040a 3B62     		str	r3, [r7, #32]
 4321 040c 3B6A     		ldr	r3, [r7, #32]
 4322 040e 9B68     		ldr	r3, [r3, #8]
 4323 0410 7B62     		str	r3, [r7, #36]
 4324 0412 7A6A     		ldr	r2, [r7, #36]
 4325 0414 3B6A     		ldr	r3, [r7, #32]
 4326 0416 9A42     		cmp	r2, r3
 4327 0418 1BD1     		bne	.L62
 4328              		.loc 1 2452 0 discriminator 21
 4329 041a 40F20002 		movw	r2, #:lower16:__malloc_av_
 4330 041e C0F20002 		movt	r2, #:upper16:__malloc_av_
 4331 0422 40F20003 		movw	r3, #:lower16:__malloc_av_
 4332 0426 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4333 042a 5968     		ldr	r1, [r3, #4]
 4334 042c FB68     		ldr	r3, [r7, #12]
 4335 042e 03F10300 		add	r0, r3, #3
 4336 0432 002B     		cmp	r3, #0
 4337 0434 B8BF     		it	lt
 4338 0436 0346     		movlt	r3, r0
 4339 0438 4FEAA303 		asr	r3, r3, #2
 4340 043c 4FF00100 		mov	r0, #1
 4341 0440 00FA03F3 		lsl	r3, r0, r3
 4342 0444 41EA0303 		orr	r3, r1, r3
 4343 0448 5360     		str	r3, [r2, #4]
 4344 044a 10E0     		b	.L61
 4345              	.L64:
 4346              		.loc 1 2452 0 discriminator 26
 4347 044c 7B6A     		ldr	r3, [r7, #36]
 4348 044e 9B68     		ldr	r3, [r3, #8]
 4349 0450 7B62     		str	r3, [r7, #36]
 4350              	.L62:
 4351              		.loc 1 2452 0 discriminator 23
 4352 0452 7A6A     		ldr	r2, [r7, #36]
 4353 0454 3B6A     		ldr	r3, [r7, #32]
 4354 0456 9A42     		cmp	r2, r3
 4355 0458 06D0     		beq	.L63
 4356              		.loc 1 2452 0 discriminator 24
 4357 045a 7B6A     		ldr	r3, [r7, #36]
 4358 045c 5B68     		ldr	r3, [r3, #4]
 4359 045e 23F00302 		bic	r2, r3, #3
 4360 0462 7B69     		ldr	r3, [r7, #20]
 4361 0464 9A42     		cmp	r2, r3
 4362 0466 F1D8     		bhi	.L64
 4363              	.L63:
 4364              		.loc 1 2452 0 discriminator 25
 4365 0468 7B6A     		ldr	r3, [r7, #36]
 4366 046a DB68     		ldr	r3, [r3, #12]
 4367 046c 3B62     		str	r3, [r7, #32]
 4368              	.L61:
 4369              		.loc 1 2452 0 discriminator 27
 4370 046e FB6B     		ldr	r3, [r7, #60]
 4371 0470 3A6A     		ldr	r2, [r7, #32]
 4372 0472 DA60     		str	r2, [r3, #12]
 4373 0474 FB6B     		ldr	r3, [r7, #60]
 4374 0476 7A6A     		ldr	r2, [r7, #36]
 4375 0478 9A60     		str	r2, [r3, #8]
 4376 047a 3B6A     		ldr	r3, [r7, #32]
 4377 047c FA6B     		ldr	r2, [r7, #60]
 4378 047e 9A60     		str	r2, [r3, #8]
 4379 0480 3B6A     		ldr	r3, [r7, #32]
 4380 0482 9A68     		ldr	r2, [r3, #8]
 4381 0484 7B6A     		ldr	r3, [r7, #36]
 4382 0486 DA60     		str	r2, [r3, #12]
 4383              	.L44:
2453:../target/stm32/malloc/mallocr.c ****   }
2454:../target/stm32/malloc/mallocr.c **** 
2455:../target/stm32/malloc/mallocr.c ****   /* 
2456:../target/stm32/malloc/mallocr.c ****      If there are any possibly nonempty big-enough blocks, 
2457:../target/stm32/malloc/mallocr.c ****      search for best fitting chunk by scanning bins in blockwidth units.
2458:../target/stm32/malloc/mallocr.c ****   */
2459:../target/stm32/malloc/mallocr.c **** 
2460:../target/stm32/malloc/mallocr.c ****   if ( (block = idx2binblock(idx)) <= binblocks)  
 4384              		.loc 1 2460 0 is_stmt 1
 4385 0488 BB6B     		ldr	r3, [r7, #56]
 4386 048a 03F10302 		add	r2, r3, #3
 4387 048e 002B     		cmp	r3, #0
 4388 0490 B8BF     		it	lt
 4389 0492 1346     		movlt	r3, r2
 4390 0494 4FEAA303 		asr	r3, r3, #2
 4391 0498 4FF00102 		mov	r2, #1
 4392 049c 02FA03F3 		lsl	r3, r2, r3
 4393 04a0 FB62     		str	r3, [r7, #44]
 4394 04a2 40F20003 		movw	r3, #:lower16:__malloc_av_
 4395 04a6 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4396 04aa 5B68     		ldr	r3, [r3, #4]
 4397 04ac FA6A     		ldr	r2, [r7, #44]
 4398 04ae 9A42     		cmp	r2, r3
 4399 04b0 00F20181 		bhi	.L65
2461:../target/stm32/malloc/mallocr.c ****   {
2462:../target/stm32/malloc/mallocr.c **** 
2463:../target/stm32/malloc/mallocr.c ****     /* Get to the first marked block */
2464:../target/stm32/malloc/mallocr.c **** 
2465:../target/stm32/malloc/mallocr.c ****     if ( (block & binblocks) == 0) 
 4400              		.loc 1 2465 0
 4401 04b4 40F20003 		movw	r3, #:lower16:__malloc_av_
 4402 04b8 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4403 04bc 5A68     		ldr	r2, [r3, #4]
 4404 04be FB6A     		ldr	r3, [r7, #44]
 4405 04c0 02EA0303 		and	r3, r2, r3
 4406 04c4 002B     		cmp	r3, #0
 4407 04c6 1CD1     		bne	.L66
2466:../target/stm32/malloc/mallocr.c ****     {
2467:../target/stm32/malloc/mallocr.c ****       /* force to an even block boundary */
2468:../target/stm32/malloc/mallocr.c ****       idx = (idx & ~(BINBLOCKWIDTH - 1)) + BINBLOCKWIDTH;
 4408              		.loc 1 2468 0
 4409 04c8 BB6B     		ldr	r3, [r7, #56]
 4410 04ca 23F00303 		bic	r3, r3, #3
 4411 04ce 03F10403 		add	r3, r3, #4
 4412 04d2 BB63     		str	r3, [r7, #56]
2469:../target/stm32/malloc/mallocr.c ****       block <<= 1;
 4413              		.loc 1 2469 0
 4414 04d4 FB6A     		ldr	r3, [r7, #44]
 4415 04d6 4FEA4303 		lsl	r3, r3, #1
 4416 04da FB62     		str	r3, [r7, #44]
2470:../target/stm32/malloc/mallocr.c ****       while ((block & binblocks) == 0)
 4417              		.loc 1 2470 0
 4418 04dc 07E0     		b	.L67
 4419              	.L68:
2471:../target/stm32/malloc/mallocr.c ****       {
2472:../target/stm32/malloc/mallocr.c ****         idx += BINBLOCKWIDTH;
 4420              		.loc 1 2472 0
 4421 04de BB6B     		ldr	r3, [r7, #56]
 4422 04e0 03F10403 		add	r3, r3, #4
 4423 04e4 BB63     		str	r3, [r7, #56]
2473:../target/stm32/malloc/mallocr.c ****         block <<= 1;
 4424              		.loc 1 2473 0
 4425 04e6 FB6A     		ldr	r3, [r7, #44]
 4426 04e8 4FEA4303 		lsl	r3, r3, #1
 4427 04ec FB62     		str	r3, [r7, #44]
 4428              	.L67:
2470:../target/stm32/malloc/mallocr.c ****       while ((block & binblocks) == 0)
 4429              		.loc 1 2470 0 discriminator 1
 4430 04ee 40F20003 		movw	r3, #:lower16:__malloc_av_
 4431 04f2 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4432 04f6 5A68     		ldr	r2, [r3, #4]
 4433 04f8 FB6A     		ldr	r3, [r7, #44]
 4434 04fa 02EA0303 		and	r3, r2, r3
 4435 04fe 002B     		cmp	r3, #0
 4436 0500 EDD0     		beq	.L68
 4437              	.L66:
2474:../target/stm32/malloc/mallocr.c ****       }
2475:../target/stm32/malloc/mallocr.c ****     }
2476:../target/stm32/malloc/mallocr.c ****       
2477:../target/stm32/malloc/mallocr.c ****     /* For each possibly nonempty block ... */
2478:../target/stm32/malloc/mallocr.c ****     for (;;)  
2479:../target/stm32/malloc/mallocr.c ****     {
2480:../target/stm32/malloc/mallocr.c ****       startidx = idx;          /* (track incomplete blocks) */
 4438              		.loc 1 2480 0
 4439 0502 BB6B     		ldr	r3, [r7, #56]
 4440 0504 BB62     		str	r3, [r7, #40]
2481:../target/stm32/malloc/mallocr.c ****       q = bin = bin_at(idx);
 4441              		.loc 1 2481 0
 4442 0506 40F20003 		movw	r3, #:lower16:__malloc_av_
 4443 050a C0F20003 		movt	r3, #:upper16:__malloc_av_
 4444 050e BA6B     		ldr	r2, [r7, #56]
 4445 0510 4FEAC202 		lsl	r2, r2, #3
 4446 0514 9B18     		adds	r3, r3, r2
 4447 0516 7B63     		str	r3, [r7, #52]
 4448 0518 7B6B     		ldr	r3, [r7, #52]
 4449 051a FB61     		str	r3, [r7, #28]
 4450              	.L73:
2482:../target/stm32/malloc/mallocr.c **** 
2483:../target/stm32/malloc/mallocr.c ****       /* For each bin in this block ... */
2484:../target/stm32/malloc/mallocr.c ****       do
2485:../target/stm32/malloc/mallocr.c ****       {
2486:../target/stm32/malloc/mallocr.c ****         /* Find and use first big enough chunk ... */
2487:../target/stm32/malloc/mallocr.c **** 
2488:../target/stm32/malloc/mallocr.c ****         for (victim = last(bin); victim != bin; victim = victim->bk)
 4451              		.loc 1 2488 0
 4452 051c 7B6B     		ldr	r3, [r7, #52]
 4453 051e DB68     		ldr	r3, [r3, #12]
 4454 0520 FB63     		str	r3, [r7, #60]
 4455 0522 71E0     		b	.L69
 4456              	.L72:
2489:../target/stm32/malloc/mallocr.c ****         {
2490:../target/stm32/malloc/mallocr.c ****           victim_size = chunksize(victim);
 4457              		.loc 1 2490 0
 4458 0524 FB6B     		ldr	r3, [r7, #60]
 4459 0526 5B68     		ldr	r3, [r3, #4]
 4460 0528 23F00303 		bic	r3, r3, #3
 4461 052c 7B61     		str	r3, [r7, #20]
2491:../target/stm32/malloc/mallocr.c ****           remainder_size = long_sub_size_t(victim_size, nb);
 4462              		.loc 1 2491 0
 4463 052e 7A69     		ldr	r2, [r7, #20]
 4464 0530 BB69     		ldr	r3, [r7, #24]
 4465 0532 D31A     		subs	r3, r2, r3
 4466 0534 3B63     		str	r3, [r7, #48]
2492:../target/stm32/malloc/mallocr.c **** 
2493:../target/stm32/malloc/mallocr.c ****           if (remainder_size >= (long)MINSIZE) /* split */
 4467              		.loc 1 2493 0
 4468 0536 3B6B     		ldr	r3, [r7, #48]
 4469 0538 0F2B     		cmp	r3, #15
 4470 053a 42DD     		ble	.L70
2494:../target/stm32/malloc/mallocr.c ****           {
2495:../target/stm32/malloc/mallocr.c ****             remainder = chunk_at_offset(victim, nb);
 4471              		.loc 1 2495 0
 4472 053c FA6B     		ldr	r2, [r7, #60]
 4473 053e BB69     		ldr	r3, [r7, #24]
 4474 0540 D318     		adds	r3, r2, r3
 4475 0542 3B61     		str	r3, [r7, #16]
2496:../target/stm32/malloc/mallocr.c ****             set_head(victim, nb | PREV_INUSE);
 4476              		.loc 1 2496 0
 4477 0544 BB69     		ldr	r3, [r7, #24]
 4478 0546 43F00102 		orr	r2, r3, #1
 4479 054a FB6B     		ldr	r3, [r7, #60]
 4480 054c 5A60     		str	r2, [r3, #4]
2497:../target/stm32/malloc/mallocr.c ****             unlink(victim, bck, fwd);
 4481              		.loc 1 2497 0
 4482 054e FB6B     		ldr	r3, [r7, #60]
 4483 0550 DB68     		ldr	r3, [r3, #12]
 4484 0552 3B62     		str	r3, [r7, #32]
 4485 0554 FB6B     		ldr	r3, [r7, #60]
 4486 0556 9B68     		ldr	r3, [r3, #8]
 4487 0558 7B62     		str	r3, [r7, #36]
 4488 055a 7B6A     		ldr	r3, [r7, #36]
 4489 055c 3A6A     		ldr	r2, [r7, #32]
 4490 055e DA60     		str	r2, [r3, #12]
 4491 0560 3B6A     		ldr	r3, [r7, #32]
 4492 0562 7A6A     		ldr	r2, [r7, #36]
 4493 0564 9A60     		str	r2, [r3, #8]
2498:../target/stm32/malloc/mallocr.c ****             link_last_remainder(remainder);
 4494              		.loc 1 2498 0
 4495 0566 40F20003 		movw	r3, #:lower16:__malloc_av_
 4496 056a C0F20003 		movt	r3, #:upper16:__malloc_av_
 4497 056e 03F10802 		add	r2, r3, #8
 4498 0572 40F20003 		movw	r3, #:lower16:__malloc_av_
 4499 0576 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4500 057a 03F10803 		add	r3, r3, #8
 4501 057e 3969     		ldr	r1, [r7, #16]
 4502 0580 D960     		str	r1, [r3, #12]
 4503 0582 DB68     		ldr	r3, [r3, #12]
 4504 0584 9360     		str	r3, [r2, #8]
 4505 0586 40F20003 		movw	r3, #:lower16:__malloc_av_
 4506 058a C0F20003 		movt	r3, #:upper16:__malloc_av_
 4507 058e 03F10802 		add	r2, r3, #8
 4508 0592 3B69     		ldr	r3, [r7, #16]
 4509 0594 DA60     		str	r2, [r3, #12]
 4510 0596 3B69     		ldr	r3, [r7, #16]
 4511 0598 DA68     		ldr	r2, [r3, #12]
 4512 059a 3B69     		ldr	r3, [r7, #16]
 4513 059c 9A60     		str	r2, [r3, #8]
2499:../target/stm32/malloc/mallocr.c ****             set_head(remainder, remainder_size | PREV_INUSE);
 4514              		.loc 1 2499 0
 4515 059e 3B6B     		ldr	r3, [r7, #48]
 4516 05a0 43F00103 		orr	r3, r3, #1
 4517 05a4 1A46     		mov	r2, r3
 4518 05a6 3B69     		ldr	r3, [r7, #16]
 4519 05a8 5A60     		str	r2, [r3, #4]
2500:../target/stm32/malloc/mallocr.c ****             set_foot(remainder, remainder_size);
 4520              		.loc 1 2500 0
 4521 05aa 3B6B     		ldr	r3, [r7, #48]
 4522 05ac 3A69     		ldr	r2, [r7, #16]
 4523 05ae D318     		adds	r3, r2, r3
 4524 05b0 3A6B     		ldr	r2, [r7, #48]
 4525 05b2 1A60     		str	r2, [r3, #0]
2501:../target/stm32/malloc/mallocr.c ****             check_malloced_chunk(victim, nb);
2502:../target/stm32/malloc/mallocr.c **** 	    MALLOC_UNLOCK;
 4526              		.loc 1 2502 0
 4527 05b4 7868     		ldr	r0, [r7, #4]
 4528 05b6 FFF7FEFF 		bl	__malloc_unlock
2503:../target/stm32/malloc/mallocr.c ****             return chunk2mem(victim);
 4529              		.loc 1 2503 0
 4530 05ba FB6B     		ldr	r3, [r7, #60]
 4531 05bc 03F10803 		add	r3, r3, #8
 4532 05c0 D7E0     		b	.L22
 4533              	.L70:
2504:../target/stm32/malloc/mallocr.c ****           }
2505:../target/stm32/malloc/mallocr.c **** 
2506:../target/stm32/malloc/mallocr.c ****           else if (remainder_size >= 0)  /* take */
 4534              		.loc 1 2506 0
 4535 05c2 3B6B     		ldr	r3, [r7, #48]
 4536 05c4 002B     		cmp	r3, #0
 4537 05c6 1CDB     		blt	.L71
2507:../target/stm32/malloc/mallocr.c ****           {
2508:../target/stm32/malloc/mallocr.c ****             set_inuse_bit_at_offset(victim, victim_size);
 4538              		.loc 1 2508 0
 4539 05c8 FA6B     		ldr	r2, [r7, #60]
 4540 05ca 7B69     		ldr	r3, [r7, #20]
 4541 05cc D318     		adds	r3, r2, r3
 4542 05ce F96B     		ldr	r1, [r7, #60]
 4543 05d0 7A69     		ldr	r2, [r7, #20]
 4544 05d2 8A18     		adds	r2, r1, r2
 4545 05d4 5268     		ldr	r2, [r2, #4]
 4546 05d6 42F00102 		orr	r2, r2, #1
 4547 05da 5A60     		str	r2, [r3, #4]
2509:../target/stm32/malloc/mallocr.c ****             unlink(victim, bck, fwd);
 4548              		.loc 1 2509 0
 4549 05dc FB6B     		ldr	r3, [r7, #60]
 4550 05de DB68     		ldr	r3, [r3, #12]
 4551 05e0 3B62     		str	r3, [r7, #32]
 4552 05e2 FB6B     		ldr	r3, [r7, #60]
 4553 05e4 9B68     		ldr	r3, [r3, #8]
 4554 05e6 7B62     		str	r3, [r7, #36]
 4555 05e8 7B6A     		ldr	r3, [r7, #36]
 4556 05ea 3A6A     		ldr	r2, [r7, #32]
 4557 05ec DA60     		str	r2, [r3, #12]
 4558 05ee 3B6A     		ldr	r3, [r7, #32]
 4559 05f0 7A6A     		ldr	r2, [r7, #36]
 4560 05f2 9A60     		str	r2, [r3, #8]
2510:../target/stm32/malloc/mallocr.c ****             check_malloced_chunk(victim, nb);
2511:../target/stm32/malloc/mallocr.c **** 	    MALLOC_UNLOCK;
 4561              		.loc 1 2511 0
 4562 05f4 7868     		ldr	r0, [r7, #4]
 4563 05f6 FFF7FEFF 		bl	__malloc_unlock
2512:../target/stm32/malloc/mallocr.c ****             return chunk2mem(victim);
 4564              		.loc 1 2512 0
 4565 05fa FB6B     		ldr	r3, [r7, #60]
 4566 05fc 03F10803 		add	r3, r3, #8
 4567 0600 B7E0     		b	.L22
 4568              	.L71:
2488:../target/stm32/malloc/mallocr.c ****         for (victim = last(bin); victim != bin; victim = victim->bk)
 4569              		.loc 1 2488 0
 4570 0602 FB6B     		ldr	r3, [r7, #60]
 4571 0604 DB68     		ldr	r3, [r3, #12]
 4572 0606 FB63     		str	r3, [r7, #60]
 4573              	.L69:
2488:../target/stm32/malloc/mallocr.c ****         for (victim = last(bin); victim != bin; victim = victim->bk)
 4574              		.loc 1 2488 0 is_stmt 0 discriminator 1
 4575 0608 FA6B     		ldr	r2, [r7, #60]
 4576 060a 7B6B     		ldr	r3, [r7, #52]
 4577 060c 9A42     		cmp	r2, r3
 4578 060e 89D1     		bne	.L72
2513:../target/stm32/malloc/mallocr.c ****           }
2514:../target/stm32/malloc/mallocr.c **** 
2515:../target/stm32/malloc/mallocr.c ****         }
2516:../target/stm32/malloc/mallocr.c **** 
2517:../target/stm32/malloc/mallocr.c ****        bin = next_bin(bin);
 4579              		.loc 1 2517 0 is_stmt 1
 4580 0610 7B6B     		ldr	r3, [r7, #52]
 4581 0612 03F10803 		add	r3, r3, #8
 4582 0616 7B63     		str	r3, [r7, #52]
2518:../target/stm32/malloc/mallocr.c **** 
2519:../target/stm32/malloc/mallocr.c **** #if MALLOC_ALIGN == 16
2520:../target/stm32/malloc/mallocr.c ****        if (idx < MAX_SMALLBIN)
2521:../target/stm32/malloc/mallocr.c ****          {
2522:../target/stm32/malloc/mallocr.c ****            bin = next_bin(bin);
2523:../target/stm32/malloc/mallocr.c ****            ++idx;
2524:../target/stm32/malloc/mallocr.c ****          }
2525:../target/stm32/malloc/mallocr.c **** #endif
2526:../target/stm32/malloc/mallocr.c ****       } while ((++idx & (BINBLOCKWIDTH - 1)) != 0);
 4583              		.loc 1 2526 0
 4584 0618 BB6B     		ldr	r3, [r7, #56]
 4585 061a 03F10103 		add	r3, r3, #1
 4586 061e BB63     		str	r3, [r7, #56]
 4587 0620 BB6B     		ldr	r3, [r7, #56]
 4588 0622 03F00303 		and	r3, r3, #3
 4589 0626 002B     		cmp	r3, #0
 4590 0628 7FF478AF 		bne	.L73
 4591              	.L76:
2527:../target/stm32/malloc/mallocr.c **** 
2528:../target/stm32/malloc/mallocr.c ****       /* Clear out the block bit. */
2529:../target/stm32/malloc/mallocr.c **** 
2530:../target/stm32/malloc/mallocr.c ****       do   /* Possibly backtrack to try to clear a partial block */
2531:../target/stm32/malloc/mallocr.c ****       {
2532:../target/stm32/malloc/mallocr.c ****         if ((startidx & (BINBLOCKWIDTH - 1)) == 0)
 4592              		.loc 1 2532 0
 4593 062c BB6A     		ldr	r3, [r7, #40]
 4594 062e 03F00303 		and	r3, r3, #3
 4595 0632 002B     		cmp	r3, #0
 4596 0634 0FD1     		bne	.L74
2533:../target/stm32/malloc/mallocr.c ****         {
2534:../target/stm32/malloc/mallocr.c ****           binblocks &= ~block;
 4597              		.loc 1 2534 0
 4598 0636 40F20003 		movw	r3, #:lower16:__malloc_av_
 4599 063a C0F20003 		movt	r3, #:upper16:__malloc_av_
 4600 063e 40F20002 		movw	r2, #:lower16:__malloc_av_
 4601 0642 C0F20002 		movt	r2, #:upper16:__malloc_av_
 4602 0646 5168     		ldr	r1, [r2, #4]
 4603 0648 FA6A     		ldr	r2, [r7, #44]
 4604 064a 6FEA0202 		mvn	r2, r2
 4605 064e 01EA0202 		and	r2, r1, r2
 4606 0652 5A60     		str	r2, [r3, #4]
2535:../target/stm32/malloc/mallocr.c ****           break;
 4607              		.loc 1 2535 0
 4608 0654 0CE0     		b	.L75
 4609              	.L74:
2536:../target/stm32/malloc/mallocr.c ****         }
2537:../target/stm32/malloc/mallocr.c ****         --startidx;
 4610              		.loc 1 2537 0
 4611 0656 BB6A     		ldr	r3, [r7, #40]
 4612 0658 03F1FF33 		add	r3, r3, #-1
 4613 065c BB62     		str	r3, [r7, #40]
2538:../target/stm32/malloc/mallocr.c ****        q = prev_bin(q);
 4614              		.loc 1 2538 0
 4615 065e FB69     		ldr	r3, [r7, #28]
 4616 0660 A3F10803 		sub	r3, r3, #8
 4617 0664 FB61     		str	r3, [r7, #28]
2539:../target/stm32/malloc/mallocr.c ****       } while (first(q) == q);
 4618              		.loc 1 2539 0
 4619 0666 FB69     		ldr	r3, [r7, #28]
 4620 0668 9A68     		ldr	r2, [r3, #8]
 4621 066a FB69     		ldr	r3, [r7, #28]
 4622 066c 9A42     		cmp	r2, r3
 4623 066e DDD0     		beq	.L76
 4624              	.L75:
2540:../target/stm32/malloc/mallocr.c **** 
2541:../target/stm32/malloc/mallocr.c ****       /* Get to the next possibly nonempty block */
2542:../target/stm32/malloc/mallocr.c **** 
2543:../target/stm32/malloc/mallocr.c ****       if ( (block <<= 1) <= binblocks && (block != 0) ) 
 4625              		.loc 1 2543 0
 4626 0670 FB6A     		ldr	r3, [r7, #44]
 4627 0672 4FEA4303 		lsl	r3, r3, #1
 4628 0676 FB62     		str	r3, [r7, #44]
 4629 0678 40F20003 		movw	r3, #:lower16:__malloc_av_
 4630 067c C0F20003 		movt	r3, #:upper16:__malloc_av_
 4631 0680 5B68     		ldr	r3, [r3, #4]
 4632 0682 FA6A     		ldr	r2, [r7, #44]
 4633 0684 9A42     		cmp	r2, r3
 4634 0686 16D8     		bhi	.L65
 4635              		.loc 1 2543 0 is_stmt 0 discriminator 1
 4636 0688 FB6A     		ldr	r3, [r7, #44]
 4637 068a 002B     		cmp	r3, #0
 4638 068c 13D0     		beq	.L65
2544:../target/stm32/malloc/mallocr.c ****       {
2545:../target/stm32/malloc/mallocr.c ****         while ((block & binblocks) == 0)
 4639              		.loc 1 2545 0 is_stmt 1
 4640 068e 07E0     		b	.L77
 4641              	.L78:
2546:../target/stm32/malloc/mallocr.c ****         {
2547:../target/stm32/malloc/mallocr.c ****           idx += BINBLOCKWIDTH;
 4642              		.loc 1 2547 0
 4643 0690 BB6B     		ldr	r3, [r7, #56]
 4644 0692 03F10403 		add	r3, r3, #4
 4645 0696 BB63     		str	r3, [r7, #56]
2548:../target/stm32/malloc/mallocr.c ****           block <<= 1;
 4646              		.loc 1 2548 0
 4647 0698 FB6A     		ldr	r3, [r7, #44]
 4648 069a 4FEA4303 		lsl	r3, r3, #1
 4649 069e FB62     		str	r3, [r7, #44]
 4650              	.L77:
2545:../target/stm32/malloc/mallocr.c ****         while ((block & binblocks) == 0)
 4651              		.loc 1 2545 0 discriminator 1
 4652 06a0 40F20003 		movw	r3, #:lower16:__malloc_av_
 4653 06a4 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4654 06a8 5A68     		ldr	r2, [r3, #4]
 4655 06aa FB6A     		ldr	r3, [r7, #44]
 4656 06ac 02EA0303 		and	r3, r2, r3
 4657 06b0 002B     		cmp	r3, #0
 4658 06b2 EDD0     		beq	.L78
2549:../target/stm32/malloc/mallocr.c ****         }
2550:../target/stm32/malloc/mallocr.c ****       }
2551:../target/stm32/malloc/mallocr.c ****       else
2552:../target/stm32/malloc/mallocr.c ****         break;
2553:../target/stm32/malloc/mallocr.c ****     }
 4659              		.loc 1 2553 0
 4660 06b4 25E7     		b	.L66
 4661              	.L65:
2554:../target/stm32/malloc/mallocr.c ****   }
2555:../target/stm32/malloc/mallocr.c **** 
2556:../target/stm32/malloc/mallocr.c **** 
2557:../target/stm32/malloc/mallocr.c ****   /* Try to use top chunk */
2558:../target/stm32/malloc/mallocr.c **** 
2559:../target/stm32/malloc/mallocr.c ****   /* Require that there be a remainder, ensuring top always exists  */
2560:../target/stm32/malloc/mallocr.c ****   remainder_size = long_sub_size_t(chunksize(top), nb);
 4662              		.loc 1 2560 0
 4663 06b6 40F20003 		movw	r3, #:lower16:__malloc_av_
 4664 06ba C0F20003 		movt	r3, #:upper16:__malloc_av_
 4665 06be 9B68     		ldr	r3, [r3, #8]
 4666 06c0 5B68     		ldr	r3, [r3, #4]
 4667 06c2 23F00302 		bic	r2, r3, #3
 4668 06c6 BB69     		ldr	r3, [r7, #24]
 4669 06c8 D31A     		subs	r3, r2, r3
 4670 06ca 3B63     		str	r3, [r7, #48]
2561:../target/stm32/malloc/mallocr.c ****   if (chunksize(top) < nb || remainder_size < (long)MINSIZE)
 4671              		.loc 1 2561 0
 4672 06cc 40F20003 		movw	r3, #:lower16:__malloc_av_
 4673 06d0 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4674 06d4 9B68     		ldr	r3, [r3, #8]
 4675 06d6 5B68     		ldr	r3, [r3, #4]
 4676 06d8 23F00302 		bic	r2, r3, #3
 4677 06dc BB69     		ldr	r3, [r7, #24]
 4678 06de 9A42     		cmp	r2, r3
 4679 06e0 02D3     		bcc	.L79
 4680              		.loc 1 2561 0 is_stmt 0 discriminator 1
 4681 06e2 3B6B     		ldr	r3, [r7, #48]
 4682 06e4 0F2B     		cmp	r3, #15
 4683 06e6 22DC     		bgt	.L80
 4684              	.L79:
2562:../target/stm32/malloc/mallocr.c ****   {
2563:../target/stm32/malloc/mallocr.c **** 
2564:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
2565:../target/stm32/malloc/mallocr.c ****     /* If big and would otherwise need to extend, try to use mmap instead */
2566:../target/stm32/malloc/mallocr.c ****     if ((unsigned long)nb >= (unsigned long)mmap_threshold &&
2567:../target/stm32/malloc/mallocr.c ****         (victim = mmap_chunk(nb)) != 0)
2568:../target/stm32/malloc/mallocr.c ****     {
2569:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
2570:../target/stm32/malloc/mallocr.c ****       return chunk2mem(victim);
2571:../target/stm32/malloc/mallocr.c ****     }
2572:../target/stm32/malloc/mallocr.c **** #endif
2573:../target/stm32/malloc/mallocr.c **** 
2574:../target/stm32/malloc/mallocr.c ****     /* Try to extend */
2575:../target/stm32/malloc/mallocr.c ****     malloc_extend_top(RCALL nb);
 4685              		.loc 1 2575 0 is_stmt 1
 4686 06e8 7868     		ldr	r0, [r7, #4]
 4687 06ea B969     		ldr	r1, [r7, #24]
 4688 06ec FFF7FEFF 		bl	malloc_extend_top
2576:../target/stm32/malloc/mallocr.c ****     remainder_size = long_sub_size_t(chunksize(top), nb);
 4689              		.loc 1 2576 0
 4690 06f0 40F20003 		movw	r3, #:lower16:__malloc_av_
 4691 06f4 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4692 06f8 9B68     		ldr	r3, [r3, #8]
 4693 06fa 5B68     		ldr	r3, [r3, #4]
 4694 06fc 23F00302 		bic	r2, r3, #3
 4695 0700 BB69     		ldr	r3, [r7, #24]
 4696 0702 D31A     		subs	r3, r2, r3
 4697 0704 3B63     		str	r3, [r7, #48]
2577:../target/stm32/malloc/mallocr.c ****     if (chunksize(top) < nb || remainder_size < (long)MINSIZE)
 4698              		.loc 1 2577 0
 4699 0706 40F20003 		movw	r3, #:lower16:__malloc_av_
 4700 070a C0F20003 		movt	r3, #:upper16:__malloc_av_
 4701 070e 9B68     		ldr	r3, [r3, #8]
 4702 0710 5B68     		ldr	r3, [r3, #4]
 4703 0712 23F00302 		bic	r2, r3, #3
 4704 0716 BB69     		ldr	r3, [r7, #24]
 4705 0718 9A42     		cmp	r2, r3
 4706 071a 02D3     		bcc	.L81
 4707              		.loc 1 2577 0 is_stmt 0 discriminator 1
 4708 071c 3B6B     		ldr	r3, [r7, #48]
 4709 071e 0F2B     		cmp	r3, #15
 4710 0720 05DC     		bgt	.L80
 4711              	.L81:
2578:../target/stm32/malloc/mallocr.c ****     {
2579:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
 4712              		.loc 1 2579 0 is_stmt 1
 4713 0722 7868     		ldr	r0, [r7, #4]
 4714 0724 FFF7FEFF 		bl	__malloc_unlock
2580:../target/stm32/malloc/mallocr.c ****       return 0; /* propagate failure */
 4715              		.loc 1 2580 0
 4716 0728 4FF00003 		mov	r3, #0
 4717 072c 21E0     		b	.L22
 4718              	.L80:
2581:../target/stm32/malloc/mallocr.c ****     }
2582:../target/stm32/malloc/mallocr.c ****   }
2583:../target/stm32/malloc/mallocr.c **** 
2584:../target/stm32/malloc/mallocr.c ****   victim = top;
 4719              		.loc 1 2584 0
 4720 072e 40F20003 		movw	r3, #:lower16:__malloc_av_
 4721 0732 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4722 0736 9B68     		ldr	r3, [r3, #8]
 4723 0738 FB63     		str	r3, [r7, #60]
2585:../target/stm32/malloc/mallocr.c ****   set_head(victim, nb | PREV_INUSE);
 4724              		.loc 1 2585 0
 4725 073a BB69     		ldr	r3, [r7, #24]
 4726 073c 43F00102 		orr	r2, r3, #1
 4727 0740 FB6B     		ldr	r3, [r7, #60]
 4728 0742 5A60     		str	r2, [r3, #4]
2586:../target/stm32/malloc/mallocr.c ****   top = chunk_at_offset(victim, nb);
 4729              		.loc 1 2586 0
 4730 0744 40F20003 		movw	r3, #:lower16:__malloc_av_
 4731 0748 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4732 074c F96B     		ldr	r1, [r7, #60]
 4733 074e BA69     		ldr	r2, [r7, #24]
 4734 0750 8A18     		adds	r2, r1, r2
 4735 0752 9A60     		str	r2, [r3, #8]
2587:../target/stm32/malloc/mallocr.c ****   set_head(top, remainder_size | PREV_INUSE);
 4736              		.loc 1 2587 0
 4737 0754 40F20003 		movw	r3, #:lower16:__malloc_av_
 4738 0758 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4739 075c 9B68     		ldr	r3, [r3, #8]
 4740 075e 3A6B     		ldr	r2, [r7, #48]
 4741 0760 42F00102 		orr	r2, r2, #1
 4742 0764 5A60     		str	r2, [r3, #4]
2588:../target/stm32/malloc/mallocr.c ****   check_malloced_chunk(victim, nb);
2589:../target/stm32/malloc/mallocr.c ****   MALLOC_UNLOCK;
 4743              		.loc 1 2589 0
 4744 0766 7868     		ldr	r0, [r7, #4]
 4745 0768 FFF7FEFF 		bl	__malloc_unlock
2590:../target/stm32/malloc/mallocr.c ****   return chunk2mem(victim);
 4746              		.loc 1 2590 0
 4747 076c FB6B     		ldr	r3, [r7, #60]
 4748 076e 03F10803 		add	r3, r3, #8
 4749              	.L22:
2591:../target/stm32/malloc/mallocr.c **** 
2592:../target/stm32/malloc/mallocr.c **** #endif /* MALLOC_PROVIDED */
2593:../target/stm32/malloc/mallocr.c **** }
 4750              		.loc 1 2593 0
 4751 0772 1846     		mov	r0, r3
 4752 0774 07F14007 		add	r7, r7, #64
 4753 0778 BD46     		mov	sp, r7
 4754 077a 80BD     		pop	{r7, pc}
 4755              		.cfi_endproc
 4756              	.LFE1:
 4758              		.section	.text._free_r,"ax",%progbits
 4759              		.align	2
 4760              		.global	_free_r
 4761              		.thumb
 4762              		.thumb_func
 4764              	_free_r:
 4765              	.LFB2:
2594:../target/stm32/malloc/mallocr.c **** 
2595:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_MALLOC */
2596:../target/stm32/malloc/mallocr.c **** 
2597:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_FREE
2598:../target/stm32/malloc/mallocr.c **** 
2599:../target/stm32/malloc/mallocr.c **** /*
2600:../target/stm32/malloc/mallocr.c **** 
2601:../target/stm32/malloc/mallocr.c ****   free() algorithm :
2602:../target/stm32/malloc/mallocr.c **** 
2603:../target/stm32/malloc/mallocr.c ****     cases:
2604:../target/stm32/malloc/mallocr.c **** 
2605:../target/stm32/malloc/mallocr.c ****        1. free(0) has no effect.  
2606:../target/stm32/malloc/mallocr.c **** 
2607:../target/stm32/malloc/mallocr.c ****        2. If the chunk was allocated via mmap, it is release via munmap().
2608:../target/stm32/malloc/mallocr.c **** 
2609:../target/stm32/malloc/mallocr.c ****        3. If a returned chunk borders the current high end of memory,
2610:../target/stm32/malloc/mallocr.c ****           it is consolidated into the top, and if the total unused
2611:../target/stm32/malloc/mallocr.c ****           topmost memory exceeds the trim threshold, malloc_trim is
2612:../target/stm32/malloc/mallocr.c ****           called.
2613:../target/stm32/malloc/mallocr.c **** 
2614:../target/stm32/malloc/mallocr.c ****        4. Other chunks are consolidated as they arrive, and
2615:../target/stm32/malloc/mallocr.c ****           placed in corresponding bins. (This includes the case of
2616:../target/stm32/malloc/mallocr.c ****           consolidating with the current `last_remainder').
2617:../target/stm32/malloc/mallocr.c **** 
2618:../target/stm32/malloc/mallocr.c **** */
2619:../target/stm32/malloc/mallocr.c **** 
2620:../target/stm32/malloc/mallocr.c **** 
2621:../target/stm32/malloc/mallocr.c **** #if __STD_C
2622:../target/stm32/malloc/mallocr.c **** void fREe(RARG Void_t* mem)
2623:../target/stm32/malloc/mallocr.c **** #else
2624:../target/stm32/malloc/mallocr.c **** void fREe(RARG mem) RDECL Void_t* mem;
2625:../target/stm32/malloc/mallocr.c **** #endif
2626:../target/stm32/malloc/mallocr.c **** {
 4766              		.loc 1 2626 0
 4767              		.cfi_startproc
 4768              		@ args = 0, pretend = 0, frame = 48
 4769              		@ frame_needed = 1, uses_anonymous_args = 0
 4770 0000 80B5     		push	{r7, lr}
 4771              	.LCFI6:
 4772              		.cfi_def_cfa_offset 8
 4773 0002 8CB0     		sub	sp, sp, #48
 4774              	.LCFI7:
 4775              		.cfi_def_cfa_offset 56
 4776 0004 00AF     		add	r7, sp, #0
 4777              		.cfi_offset 14, -4
 4778              		.cfi_offset 7, -8
 4779              	.LCFI8:
 4780              		.cfi_def_cfa_register 7
 4781 0006 7860     		str	r0, [r7, #4]
 4782 0008 3960     		str	r1, [r7, #0]
2627:../target/stm32/malloc/mallocr.c **** #ifdef MALLOC_PROVIDED
2628:../target/stm32/malloc/mallocr.c **** 
2629:../target/stm32/malloc/mallocr.c ****   free (mem);
2630:../target/stm32/malloc/mallocr.c **** 
2631:../target/stm32/malloc/mallocr.c **** #else
2632:../target/stm32/malloc/mallocr.c **** 
2633:../target/stm32/malloc/mallocr.c ****   mchunkptr p;         /* chunk corresponding to mem */
2634:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T hd;  /* its head field */
2635:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T sz;  /* its size */
2636:../target/stm32/malloc/mallocr.c ****   int       idx;       /* its bin index */
2637:../target/stm32/malloc/mallocr.c ****   mchunkptr next;      /* next contiguous chunk */
2638:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T nextsz; /* its size */
2639:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T prevsz; /* size of previous contiguous chunk */
2640:../target/stm32/malloc/mallocr.c ****   mchunkptr bck;       /* misc temp for linking */
2641:../target/stm32/malloc/mallocr.c ****   mchunkptr fwd;       /* misc temp for linking */
2642:../target/stm32/malloc/mallocr.c ****   int       islr;      /* track whether merging with last_remainder */
2643:../target/stm32/malloc/mallocr.c **** 
2644:../target/stm32/malloc/mallocr.c ****   if (mem == 0)                              /* free(0) has no effect */
 4783              		.loc 1 2644 0
 4784 000a 3B68     		ldr	r3, [r7, #0]
 4785 000c 002B     		cmp	r3, #0
 4786 000e 00F0B281 		beq	.L111
 4787              	.L83:
2645:../target/stm32/malloc/mallocr.c ****     return;
2646:../target/stm32/malloc/mallocr.c **** 
2647:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK;
 4788              		.loc 1 2647 0
 4789 0012 7868     		ldr	r0, [r7, #4]
 4790 0014 FFF7FEFF 		bl	__malloc_lock
2648:../target/stm32/malloc/mallocr.c **** 
2649:../target/stm32/malloc/mallocr.c ****   p = mem2chunk(mem);
 4791              		.loc 1 2649 0
 4792 0018 3B68     		ldr	r3, [r7, #0]
 4793 001a A3F10803 		sub	r3, r3, #8
 4794 001e FB62     		str	r3, [r7, #44]
2650:../target/stm32/malloc/mallocr.c ****   hd = p->size;
 4795              		.loc 1 2650 0
 4796 0020 FB6A     		ldr	r3, [r7, #44]
 4797 0022 5B68     		ldr	r3, [r3, #4]
 4798 0024 BB61     		str	r3, [r7, #24]
2651:../target/stm32/malloc/mallocr.c **** 
2652:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
2653:../target/stm32/malloc/mallocr.c ****   if (hd & IS_MMAPPED)                       /* release mmapped memory. */
2654:../target/stm32/malloc/mallocr.c ****   {
2655:../target/stm32/malloc/mallocr.c ****     munmap_chunk(p);
2656:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
2657:../target/stm32/malloc/mallocr.c ****     return;
2658:../target/stm32/malloc/mallocr.c ****   }
2659:../target/stm32/malloc/mallocr.c **** #endif
2660:../target/stm32/malloc/mallocr.c ****   
2661:../target/stm32/malloc/mallocr.c ****   check_inuse_chunk(p);
2662:../target/stm32/malloc/mallocr.c ****   
2663:../target/stm32/malloc/mallocr.c ****   sz = hd & ~PREV_INUSE;
 4799              		.loc 1 2663 0
 4800 0026 BB69     		ldr	r3, [r7, #24]
 4801 0028 23F00103 		bic	r3, r3, #1
 4802 002c BB62     		str	r3, [r7, #40]
2664:../target/stm32/malloc/mallocr.c ****   next = chunk_at_offset(p, sz);
 4803              		.loc 1 2664 0
 4804 002e FA6A     		ldr	r2, [r7, #44]
 4805 0030 BB6A     		ldr	r3, [r7, #40]
 4806 0032 D318     		adds	r3, r2, r3
 4807 0034 7B61     		str	r3, [r7, #20]
2665:../target/stm32/malloc/mallocr.c ****   nextsz = chunksize(next);
 4808              		.loc 1 2665 0
 4809 0036 7B69     		ldr	r3, [r7, #20]
 4810 0038 5B68     		ldr	r3, [r3, #4]
 4811 003a 23F00303 		bic	r3, r3, #3
 4812 003e 3B61     		str	r3, [r7, #16]
2666:../target/stm32/malloc/mallocr.c ****   
2667:../target/stm32/malloc/mallocr.c ****   if (next == top)                            /* merge with top */
 4813              		.loc 1 2667 0
 4814 0040 40F20003 		movw	r3, #:lower16:__malloc_av_
 4815 0044 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4816 0048 9A68     		ldr	r2, [r3, #8]
 4817 004a 7B69     		ldr	r3, [r7, #20]
 4818 004c 9A42     		cmp	r2, r3
 4819 004e 41D1     		bne	.L85
2668:../target/stm32/malloc/mallocr.c ****   {
2669:../target/stm32/malloc/mallocr.c ****     sz += nextsz;
 4820              		.loc 1 2669 0
 4821 0050 BA6A     		ldr	r2, [r7, #40]
 4822 0052 3B69     		ldr	r3, [r7, #16]
 4823 0054 D318     		adds	r3, r2, r3
 4824 0056 BB62     		str	r3, [r7, #40]
2670:../target/stm32/malloc/mallocr.c **** 
2671:../target/stm32/malloc/mallocr.c ****     if (!(hd & PREV_INUSE))                    /* consolidate backward */
 4825              		.loc 1 2671 0
 4826 0058 BB69     		ldr	r3, [r7, #24]
 4827 005a 03F00103 		and	r3, r3, #1
 4828 005e 002B     		cmp	r3, #0
 4829 0060 18D1     		bne	.L86
2672:../target/stm32/malloc/mallocr.c ****     {
2673:../target/stm32/malloc/mallocr.c ****       prevsz = p->prev_size;
 4830              		.loc 1 2673 0
 4831 0062 FB6A     		ldr	r3, [r7, #44]
 4832 0064 1B68     		ldr	r3, [r3, #0]
 4833 0066 FB60     		str	r3, [r7, #12]
2674:../target/stm32/malloc/mallocr.c ****       p = chunk_at_offset(p, -prevsz);
 4834              		.loc 1 2674 0
 4835 0068 FB68     		ldr	r3, [r7, #12]
 4836 006a C3F10003 		rsb	r3, r3, #0
 4837 006e FA6A     		ldr	r2, [r7, #44]
 4838 0070 D318     		adds	r3, r2, r3
 4839 0072 FB62     		str	r3, [r7, #44]
2675:../target/stm32/malloc/mallocr.c ****       sz += prevsz;
 4840              		.loc 1 2675 0
 4841 0074 BA6A     		ldr	r2, [r7, #40]
 4842 0076 FB68     		ldr	r3, [r7, #12]
 4843 0078 D318     		adds	r3, r2, r3
 4844 007a BB62     		str	r3, [r7, #40]
2676:../target/stm32/malloc/mallocr.c ****       unlink(p, bck, fwd);
 4845              		.loc 1 2676 0
 4846 007c FB6A     		ldr	r3, [r7, #44]
 4847 007e DB68     		ldr	r3, [r3, #12]
 4848 0080 7B62     		str	r3, [r7, #36]
 4849 0082 FB6A     		ldr	r3, [r7, #44]
 4850 0084 9B68     		ldr	r3, [r3, #8]
 4851 0086 3B62     		str	r3, [r7, #32]
 4852 0088 3B6A     		ldr	r3, [r7, #32]
 4853 008a 7A6A     		ldr	r2, [r7, #36]
 4854 008c DA60     		str	r2, [r3, #12]
 4855 008e 7B6A     		ldr	r3, [r7, #36]
 4856 0090 3A6A     		ldr	r2, [r7, #32]
 4857 0092 9A60     		str	r2, [r3, #8]
 4858              	.L86:
2677:../target/stm32/malloc/mallocr.c ****     }
2678:../target/stm32/malloc/mallocr.c **** 
2679:../target/stm32/malloc/mallocr.c ****     set_head(p, sz | PREV_INUSE);
 4859              		.loc 1 2679 0
 4860 0094 BB6A     		ldr	r3, [r7, #40]
 4861 0096 43F00102 		orr	r2, r3, #1
 4862 009a FB6A     		ldr	r3, [r7, #44]
 4863 009c 5A60     		str	r2, [r3, #4]
2680:../target/stm32/malloc/mallocr.c ****     top = p;
 4864              		.loc 1 2680 0
 4865 009e 40F20003 		movw	r3, #:lower16:__malloc_av_
 4866 00a2 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4867 00a6 FA6A     		ldr	r2, [r7, #44]
 4868 00a8 9A60     		str	r2, [r3, #8]
2681:../target/stm32/malloc/mallocr.c ****     if ((unsigned long)(sz) >= (unsigned long)trim_threshold) 
 4869              		.loc 1 2681 0
 4870 00aa 40F20003 		movw	r3, #:lower16:__malloc_trim_threshold
 4871 00ae C0F20003 		movt	r3, #:upper16:__malloc_trim_threshold
 4872 00b2 1B68     		ldr	r3, [r3, #0]
 4873 00b4 BA6A     		ldr	r2, [r7, #40]
 4874 00b6 9A42     		cmp	r2, r3
 4875 00b8 08D3     		bcc	.L87
2682:../target/stm32/malloc/mallocr.c ****       malloc_trim(RCALL top_pad); 
 4876              		.loc 1 2682 0
 4877 00ba 40F20003 		movw	r3, #:lower16:__malloc_top_pad
 4878 00be C0F20003 		movt	r3, #:upper16:__malloc_top_pad
 4879 00c2 1B68     		ldr	r3, [r3, #0]
 4880 00c4 7868     		ldr	r0, [r7, #4]
 4881 00c6 1946     		mov	r1, r3
 4882 00c8 FFF7FEFF 		bl	_malloc_trim_r
 4883              	.L87:
2683:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
 4884              		.loc 1 2683 0
 4885 00cc 7868     		ldr	r0, [r7, #4]
 4886 00ce FFF7FEFF 		bl	__malloc_unlock
2684:../target/stm32/malloc/mallocr.c ****     return;
 4887              		.loc 1 2684 0
 4888 00d2 51E1     		b	.L82
 4889              	.L85:
2685:../target/stm32/malloc/mallocr.c ****   }
2686:../target/stm32/malloc/mallocr.c **** 
2687:../target/stm32/malloc/mallocr.c ****   set_head(next, nextsz);                    /* clear inuse bit */
 4890              		.loc 1 2687 0
 4891 00d4 7B69     		ldr	r3, [r7, #20]
 4892 00d6 3A69     		ldr	r2, [r7, #16]
 4893 00d8 5A60     		str	r2, [r3, #4]
2688:../target/stm32/malloc/mallocr.c **** 
2689:../target/stm32/malloc/mallocr.c ****   islr = 0;
 4894              		.loc 1 2689 0
 4895 00da 4FF00003 		mov	r3, #0
 4896 00de FB61     		str	r3, [r7, #28]
2690:../target/stm32/malloc/mallocr.c **** 
2691:../target/stm32/malloc/mallocr.c ****   if (!(hd & PREV_INUSE))                    /* consolidate backward */
 4897              		.loc 1 2691 0
 4898 00e0 BB69     		ldr	r3, [r7, #24]
 4899 00e2 03F00103 		and	r3, r3, #1
 4900 00e6 002B     		cmp	r3, #0
 4901 00e8 26D1     		bne	.L88
2692:../target/stm32/malloc/mallocr.c ****   {
2693:../target/stm32/malloc/mallocr.c ****     prevsz = p->prev_size;
 4902              		.loc 1 2693 0
 4903 00ea FB6A     		ldr	r3, [r7, #44]
 4904 00ec 1B68     		ldr	r3, [r3, #0]
 4905 00ee FB60     		str	r3, [r7, #12]
2694:../target/stm32/malloc/mallocr.c ****     p = chunk_at_offset(p, -prevsz);
 4906              		.loc 1 2694 0
 4907 00f0 FB68     		ldr	r3, [r7, #12]
 4908 00f2 C3F10003 		rsb	r3, r3, #0
 4909 00f6 FA6A     		ldr	r2, [r7, #44]
 4910 00f8 D318     		adds	r3, r2, r3
 4911 00fa FB62     		str	r3, [r7, #44]
2695:../target/stm32/malloc/mallocr.c ****     sz += prevsz;
 4912              		.loc 1 2695 0
 4913 00fc BA6A     		ldr	r2, [r7, #40]
 4914 00fe FB68     		ldr	r3, [r7, #12]
 4915 0100 D318     		adds	r3, r2, r3
 4916 0102 BB62     		str	r3, [r7, #40]
2696:../target/stm32/malloc/mallocr.c ****     
2697:../target/stm32/malloc/mallocr.c ****     if (p->fd == last_remainder)             /* keep as last_remainder */
 4917              		.loc 1 2697 0
 4918 0104 FB6A     		ldr	r3, [r7, #44]
 4919 0106 9A68     		ldr	r2, [r3, #8]
 4920 0108 40F20003 		movw	r3, #:lower16:__malloc_av_
 4921 010c C0F20003 		movt	r3, #:upper16:__malloc_av_
 4922 0110 03F10803 		add	r3, r3, #8
 4923 0114 9A42     		cmp	r2, r3
 4924 0116 03D1     		bne	.L89
2698:../target/stm32/malloc/mallocr.c ****       islr = 1;
 4925              		.loc 1 2698 0
 4926 0118 4FF00103 		mov	r3, #1
 4927 011c FB61     		str	r3, [r7, #28]
 4928 011e 0BE0     		b	.L88
 4929              	.L89:
2699:../target/stm32/malloc/mallocr.c ****     else
2700:../target/stm32/malloc/mallocr.c ****       unlink(p, bck, fwd);
 4930              		.loc 1 2700 0
 4931 0120 FB6A     		ldr	r3, [r7, #44]
 4932 0122 DB68     		ldr	r3, [r3, #12]
 4933 0124 7B62     		str	r3, [r7, #36]
 4934 0126 FB6A     		ldr	r3, [r7, #44]
 4935 0128 9B68     		ldr	r3, [r3, #8]
 4936 012a 3B62     		str	r3, [r7, #32]
 4937 012c 3B6A     		ldr	r3, [r7, #32]
 4938 012e 7A6A     		ldr	r2, [r7, #36]
 4939 0130 DA60     		str	r2, [r3, #12]
 4940 0132 7B6A     		ldr	r3, [r7, #36]
 4941 0134 3A6A     		ldr	r2, [r7, #32]
 4942 0136 9A60     		str	r2, [r3, #8]
 4943              	.L88:
2701:../target/stm32/malloc/mallocr.c ****   }
2702:../target/stm32/malloc/mallocr.c ****   
2703:../target/stm32/malloc/mallocr.c ****   if (!(inuse_bit_at_offset(next, nextsz)))   /* consolidate forward */
 4944              		.loc 1 2703 0
 4945 0138 7A69     		ldr	r2, [r7, #20]
 4946 013a 3B69     		ldr	r3, [r7, #16]
 4947 013c D318     		adds	r3, r2, r3
 4948 013e 5B68     		ldr	r3, [r3, #4]
 4949 0140 03F00103 		and	r3, r3, #1
 4950 0144 002B     		cmp	r3, #0
 4951 0146 3CD1     		bne	.L90
2704:../target/stm32/malloc/mallocr.c ****   {
2705:../target/stm32/malloc/mallocr.c ****     sz += nextsz;
 4952              		.loc 1 2705 0
 4953 0148 BA6A     		ldr	r2, [r7, #40]
 4954 014a 3B69     		ldr	r3, [r7, #16]
 4955 014c D318     		adds	r3, r2, r3
 4956 014e BB62     		str	r3, [r7, #40]
2706:../target/stm32/malloc/mallocr.c ****     
2707:../target/stm32/malloc/mallocr.c ****     if (!islr && next->fd == last_remainder)  /* re-insert last_remainder */
 4957              		.loc 1 2707 0
 4958 0150 FB69     		ldr	r3, [r7, #28]
 4959 0152 002B     		cmp	r3, #0
 4960 0154 29D1     		bne	.L91
 4961              		.loc 1 2707 0 is_stmt 0 discriminator 1
 4962 0156 7B69     		ldr	r3, [r7, #20]
 4963 0158 9A68     		ldr	r2, [r3, #8]
 4964 015a 40F20003 		movw	r3, #:lower16:__malloc_av_
 4965 015e C0F20003 		movt	r3, #:upper16:__malloc_av_
 4966 0162 03F10803 		add	r3, r3, #8
 4967 0166 9A42     		cmp	r2, r3
 4968 0168 1FD1     		bne	.L91
2708:../target/stm32/malloc/mallocr.c ****     {
2709:../target/stm32/malloc/mallocr.c ****       islr = 1;
 4969              		.loc 1 2709 0 is_stmt 1
 4970 016a 4FF00103 		mov	r3, #1
 4971 016e FB61     		str	r3, [r7, #28]
2710:../target/stm32/malloc/mallocr.c ****       link_last_remainder(p);   
 4972              		.loc 1 2710 0
 4973 0170 40F20003 		movw	r3, #:lower16:__malloc_av_
 4974 0174 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4975 0178 03F10802 		add	r2, r3, #8
 4976 017c 40F20003 		movw	r3, #:lower16:__malloc_av_
 4977 0180 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4978 0184 03F10803 		add	r3, r3, #8
 4979 0188 F96A     		ldr	r1, [r7, #44]
 4980 018a D960     		str	r1, [r3, #12]
 4981 018c DB68     		ldr	r3, [r3, #12]
 4982 018e 9360     		str	r3, [r2, #8]
 4983 0190 40F20003 		movw	r3, #:lower16:__malloc_av_
 4984 0194 C0F20003 		movt	r3, #:upper16:__malloc_av_
 4985 0198 03F10802 		add	r2, r3, #8
 4986 019c FB6A     		ldr	r3, [r7, #44]
 4987 019e DA60     		str	r2, [r3, #12]
 4988 01a0 FB6A     		ldr	r3, [r7, #44]
 4989 01a2 DA68     		ldr	r2, [r3, #12]
 4990 01a4 FB6A     		ldr	r3, [r7, #44]
 4991 01a6 9A60     		str	r2, [r3, #8]
 4992 01a8 0BE0     		b	.L90
 4993              	.L91:
2711:../target/stm32/malloc/mallocr.c ****     }
2712:../target/stm32/malloc/mallocr.c ****     else
2713:../target/stm32/malloc/mallocr.c ****       unlink(next, bck, fwd);
 4994              		.loc 1 2713 0
 4995 01aa 7B69     		ldr	r3, [r7, #20]
 4996 01ac DB68     		ldr	r3, [r3, #12]
 4997 01ae 7B62     		str	r3, [r7, #36]
 4998 01b0 7B69     		ldr	r3, [r7, #20]
 4999 01b2 9B68     		ldr	r3, [r3, #8]
 5000 01b4 3B62     		str	r3, [r7, #32]
 5001 01b6 3B6A     		ldr	r3, [r7, #32]
 5002 01b8 7A6A     		ldr	r2, [r7, #36]
 5003 01ba DA60     		str	r2, [r3, #12]
 5004 01bc 7B6A     		ldr	r3, [r7, #36]
 5005 01be 3A6A     		ldr	r2, [r7, #32]
 5006 01c0 9A60     		str	r2, [r3, #8]
 5007              	.L90:
2714:../target/stm32/malloc/mallocr.c ****   }
2715:../target/stm32/malloc/mallocr.c **** 
2716:../target/stm32/malloc/mallocr.c **** 
2717:../target/stm32/malloc/mallocr.c ****   set_head(p, sz | PREV_INUSE);
 5008              		.loc 1 2717 0
 5009 01c2 BB6A     		ldr	r3, [r7, #40]
 5010 01c4 43F00102 		orr	r2, r3, #1
 5011 01c8 FB6A     		ldr	r3, [r7, #44]
 5012 01ca 5A60     		str	r2, [r3, #4]
2718:../target/stm32/malloc/mallocr.c ****   set_foot(p, sz);
 5013              		.loc 1 2718 0
 5014 01cc FA6A     		ldr	r2, [r7, #44]
 5015 01ce BB6A     		ldr	r3, [r7, #40]
 5016 01d0 D318     		adds	r3, r2, r3
 5017 01d2 BA6A     		ldr	r2, [r7, #40]
 5018 01d4 1A60     		str	r2, [r3, #0]
2719:../target/stm32/malloc/mallocr.c ****   if (!islr)
 5019              		.loc 1 2719 0
 5020 01d6 FB69     		ldr	r3, [r7, #28]
 5021 01d8 002B     		cmp	r3, #0
 5022 01da 40F0C880 		bne	.L92
2720:../target/stm32/malloc/mallocr.c ****     frontlink(p, sz, idx, bck, fwd);  
 5023              		.loc 1 2720 0
 5024 01de BA6A     		ldr	r2, [r7, #40]
 5025 01e0 40F2FF13 		movw	r3, #511
 5026 01e4 9A42     		cmp	r2, r3
 5027 01e6 35D8     		bhi	.L93
 5028              		.loc 1 2720 0 is_stmt 0 discriminator 1
 5029 01e8 BB6A     		ldr	r3, [r7, #40]
 5030 01ea 4FEAD303 		lsr	r3, r3, #3
 5031 01ee BB60     		str	r3, [r7, #8]
 5032 01f0 40F20002 		movw	r2, #:lower16:__malloc_av_
 5033 01f4 C0F20002 		movt	r2, #:upper16:__malloc_av_
 5034 01f8 40F20003 		movw	r3, #:lower16:__malloc_av_
 5035 01fc C0F20003 		movt	r3, #:upper16:__malloc_av_
 5036 0200 5968     		ldr	r1, [r3, #4]
 5037 0202 BB68     		ldr	r3, [r7, #8]
 5038 0204 03F10300 		add	r0, r3, #3
 5039 0208 002B     		cmp	r3, #0
 5040 020a B8BF     		it	lt
 5041 020c 0346     		movlt	r3, r0
 5042 020e 4FEAA303 		asr	r3, r3, #2
 5043 0212 4FF00100 		mov	r0, #1
 5044 0216 00FA03F3 		lsl	r3, r0, r3
 5045 021a 41EA0303 		orr	r3, r1, r3
 5046 021e 5360     		str	r3, [r2, #4]
 5047 0220 40F20003 		movw	r3, #:lower16:__malloc_av_
 5048 0224 C0F20003 		movt	r3, #:upper16:__malloc_av_
 5049 0228 BA68     		ldr	r2, [r7, #8]
 5050 022a 4FEAC202 		lsl	r2, r2, #3
 5051 022e 9B18     		adds	r3, r3, r2
 5052 0230 7B62     		str	r3, [r7, #36]
 5053 0232 7B6A     		ldr	r3, [r7, #36]
 5054 0234 9B68     		ldr	r3, [r3, #8]
 5055 0236 3B62     		str	r3, [r7, #32]
 5056 0238 FB6A     		ldr	r3, [r7, #44]
 5057 023a 7A6A     		ldr	r2, [r7, #36]
 5058 023c DA60     		str	r2, [r3, #12]
 5059 023e FB6A     		ldr	r3, [r7, #44]
 5060 0240 3A6A     		ldr	r2, [r7, #32]
 5061 0242 9A60     		str	r2, [r3, #8]
 5062 0244 7B6A     		ldr	r3, [r7, #36]
 5063 0246 FA6A     		ldr	r2, [r7, #44]
 5064 0248 9A60     		str	r2, [r3, #8]
 5065 024a 7B6A     		ldr	r3, [r7, #36]
 5066 024c 9A68     		ldr	r2, [r3, #8]
 5067 024e 3B6A     		ldr	r3, [r7, #32]
 5068 0250 DA60     		str	r2, [r3, #12]
 5069 0252 8CE0     		b	.L92
 5070              	.L93:
 5071              		.loc 1 2720 0 discriminator 2
 5072 0254 BB6A     		ldr	r3, [r7, #40]
 5073 0256 4FEA5323 		lsr	r3, r3, #9
 5074 025a 002B     		cmp	r3, #0
 5075 025c 03D1     		bne	.L94
 5076              		.loc 1 2720 0 discriminator 3
 5077 025e BB6A     		ldr	r3, [r7, #40]
 5078 0260 4FEAD303 		lsr	r3, r3, #3
 5079 0264 3BE0     		b	.L95
 5080              	.L94:
 5081              		.loc 1 2720 0 discriminator 4
 5082 0266 BB6A     		ldr	r3, [r7, #40]
 5083 0268 4FEA5323 		lsr	r3, r3, #9
 5084 026c 042B     		cmp	r3, #4
 5085 026e 05D8     		bhi	.L96
 5086              		.loc 1 2720 0 discriminator 5
 5087 0270 BB6A     		ldr	r3, [r7, #40]
 5088 0272 4FEA9313 		lsr	r3, r3, #6
 5089 0276 03F13803 		add	r3, r3, #56
 5090 027a 30E0     		b	.L97
 5091              	.L96:
 5092              		.loc 1 2720 0 discriminator 6
 5093 027c BB6A     		ldr	r3, [r7, #40]
 5094 027e 4FEA5323 		lsr	r3, r3, #9
 5095 0282 142B     		cmp	r3, #20
 5096 0284 05D8     		bhi	.L98
 5097              		.loc 1 2720 0 discriminator 7
 5098 0286 BB6A     		ldr	r3, [r7, #40]
 5099 0288 4FEA5323 		lsr	r3, r3, #9
 5100 028c 03F15B03 		add	r3, r3, #91
 5101 0290 25E0     		b	.L99
 5102              	.L98:
 5103              		.loc 1 2720 0 discriminator 8
 5104 0292 BB6A     		ldr	r3, [r7, #40]
 5105 0294 4FEA5323 		lsr	r3, r3, #9
 5106 0298 542B     		cmp	r3, #84
 5107 029a 05D8     		bhi	.L100
 5108              		.loc 1 2720 0 discriminator 9
 5109 029c BB6A     		ldr	r3, [r7, #40]
 5110 029e 4FEA1333 		lsr	r3, r3, #12
 5111 02a2 03F16E03 		add	r3, r3, #110
 5112 02a6 1AE0     		b	.L101
 5113              	.L100:
 5114              		.loc 1 2720 0 discriminator 10
 5115 02a8 BB6A     		ldr	r3, [r7, #40]
 5116 02aa 4FEA5323 		lsr	r3, r3, #9
 5117 02ae B3F5AA7F 		cmp	r3, #340
 5118 02b2 05D8     		bhi	.L102
 5119              		.loc 1 2720 0 discriminator 11
 5120 02b4 BB6A     		ldr	r3, [r7, #40]
 5121 02b6 4FEAD333 		lsr	r3, r3, #15
 5122 02ba 03F17703 		add	r3, r3, #119
 5123 02be 0EE0     		b	.L103
 5124              	.L102:
 5125              		.loc 1 2720 0 discriminator 12
 5126 02c0 BB6A     		ldr	r3, [r7, #40]
 5127 02c2 4FEA5322 		lsr	r2, r3, #9
 5128 02c6 40F25453 		movw	r3, #1364
 5129 02ca 9A42     		cmp	r2, r3
 5130 02cc 05D8     		bhi	.L104
 5131              		.loc 1 2720 0 discriminator 13
 5132 02ce BB6A     		ldr	r3, [r7, #40]
 5133 02d0 4FEA9343 		lsr	r3, r3, #18
 5134 02d4 03F17C03 		add	r3, r3, #124
 5135 02d8 01E0     		b	.L105
 5136              	.L104:
 5137              		.loc 1 2720 0 discriminator 14
 5138 02da 4FF07E03 		mov	r3, #126
 5139              	.L105:
 5140              	.L103:
 5141              	.L101:
 5142              	.L99:
 5143              	.L97:
 5144              	.L95:
 5145              		.loc 1 2720 0 discriminator 20
 5146 02de BB60     		str	r3, [r7, #8]
 5147 02e0 40F20003 		movw	r3, #:lower16:__malloc_av_
 5148 02e4 C0F20003 		movt	r3, #:upper16:__malloc_av_
 5149 02e8 BA68     		ldr	r2, [r7, #8]
 5150 02ea 4FEAC202 		lsl	r2, r2, #3
 5151 02ee 9B18     		adds	r3, r3, r2
 5152 02f0 7B62     		str	r3, [r7, #36]
 5153 02f2 7B6A     		ldr	r3, [r7, #36]
 5154 02f4 9B68     		ldr	r3, [r3, #8]
 5155 02f6 3B62     		str	r3, [r7, #32]
 5156 02f8 3A6A     		ldr	r2, [r7, #32]
 5157 02fa 7B6A     		ldr	r3, [r7, #36]
 5158 02fc 9A42     		cmp	r2, r3
 5159 02fe 1BD1     		bne	.L108
 5160              		.loc 1 2720 0 discriminator 21
 5161 0300 40F20002 		movw	r2, #:lower16:__malloc_av_
 5162 0304 C0F20002 		movt	r2, #:upper16:__malloc_av_
 5163 0308 40F20003 		movw	r3, #:lower16:__malloc_av_
 5164 030c C0F20003 		movt	r3, #:upper16:__malloc_av_
 5165 0310 5968     		ldr	r1, [r3, #4]
 5166 0312 BB68     		ldr	r3, [r7, #8]
 5167 0314 03F10300 		add	r0, r3, #3
 5168 0318 002B     		cmp	r3, #0
 5169 031a B8BF     		it	lt
 5170 031c 0346     		movlt	r3, r0
 5171 031e 4FEAA303 		asr	r3, r3, #2
 5172 0322 4FF00100 		mov	r0, #1
 5173 0326 00FA03F3 		lsl	r3, r0, r3
 5174 032a 41EA0303 		orr	r3, r1, r3
 5175 032e 5360     		str	r3, [r2, #4]
 5176 0330 10E0     		b	.L107
 5177              	.L110:
 5178              		.loc 1 2720 0 discriminator 26
 5179 0332 3B6A     		ldr	r3, [r7, #32]
 5180 0334 9B68     		ldr	r3, [r3, #8]
 5181 0336 3B62     		str	r3, [r7, #32]
 5182              	.L108:
 5183              		.loc 1 2720 0 discriminator 23
 5184 0338 3A6A     		ldr	r2, [r7, #32]
 5185 033a 7B6A     		ldr	r3, [r7, #36]
 5186 033c 9A42     		cmp	r2, r3
 5187 033e 06D0     		beq	.L109
 5188              		.loc 1 2720 0 discriminator 24
 5189 0340 3B6A     		ldr	r3, [r7, #32]
 5190 0342 5B68     		ldr	r3, [r3, #4]
 5191 0344 23F00302 		bic	r2, r3, #3
 5192 0348 BB6A     		ldr	r3, [r7, #40]
 5193 034a 9A42     		cmp	r2, r3
 5194 034c F1D8     		bhi	.L110
 5195              	.L109:
 5196              		.loc 1 2720 0 discriminator 25
 5197 034e 3B6A     		ldr	r3, [r7, #32]
 5198 0350 DB68     		ldr	r3, [r3, #12]
 5199 0352 7B62     		str	r3, [r7, #36]
 5200              	.L107:
 5201              		.loc 1 2720 0 discriminator 27
 5202 0354 FB6A     		ldr	r3, [r7, #44]
 5203 0356 7A6A     		ldr	r2, [r7, #36]
 5204 0358 DA60     		str	r2, [r3, #12]
 5205 035a FB6A     		ldr	r3, [r7, #44]
 5206 035c 3A6A     		ldr	r2, [r7, #32]
 5207 035e 9A60     		str	r2, [r3, #8]
 5208 0360 7B6A     		ldr	r3, [r7, #36]
 5209 0362 FA6A     		ldr	r2, [r7, #44]
 5210 0364 9A60     		str	r2, [r3, #8]
 5211 0366 7B6A     		ldr	r3, [r7, #36]
 5212 0368 9A68     		ldr	r2, [r3, #8]
 5213 036a 3B6A     		ldr	r3, [r7, #32]
 5214 036c DA60     		str	r2, [r3, #12]
 5215              	.L92:
2721:../target/stm32/malloc/mallocr.c **** 
2722:../target/stm32/malloc/mallocr.c ****   MALLOC_UNLOCK;
 5216              		.loc 1 2722 0 is_stmt 1
 5217 036e 7868     		ldr	r0, [r7, #4]
 5218 0370 FFF7FEFF 		bl	__malloc_unlock
 5219 0374 00E0     		b	.L82
 5220              	.L111:
2645:../target/stm32/malloc/mallocr.c ****     return;
 5221              		.loc 1 2645 0
 5222 0376 00BF     		nop
 5223              	.L82:
2723:../target/stm32/malloc/mallocr.c **** 
2724:../target/stm32/malloc/mallocr.c **** #endif /* MALLOC_PROVIDED */
2725:../target/stm32/malloc/mallocr.c **** }
 5224              		.loc 1 2725 0
 5225 0378 07F13007 		add	r7, r7, #48
 5226 037c BD46     		mov	sp, r7
 5227 037e 80BD     		pop	{r7, pc}
 5228              		.cfi_endproc
 5229              	.LFE2:
 5231              		.section	.text._realloc_r,"ax",%progbits
 5232              		.align	2
 5233              		.global	_realloc_r
 5234              		.thumb
 5235              		.thumb_func
 5237              	_realloc_r:
 5238              	.LFB3:
2726:../target/stm32/malloc/mallocr.c **** 
2727:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_FREE */
2728:../target/stm32/malloc/mallocr.c **** 
2729:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_REALLOC
2730:../target/stm32/malloc/mallocr.c **** 
2731:../target/stm32/malloc/mallocr.c **** /*
2732:../target/stm32/malloc/mallocr.c **** 
2733:../target/stm32/malloc/mallocr.c ****   Realloc algorithm:
2734:../target/stm32/malloc/mallocr.c **** 
2735:../target/stm32/malloc/mallocr.c ****     Chunks that were obtained via mmap cannot be extended or shrunk
2736:../target/stm32/malloc/mallocr.c ****     unless HAVE_MREMAP is defined, in which case mremap is used.
2737:../target/stm32/malloc/mallocr.c ****     Otherwise, if their reallocation is for additional space, they are
2738:../target/stm32/malloc/mallocr.c ****     copied.  If for less, they are just left alone.
2739:../target/stm32/malloc/mallocr.c **** 
2740:../target/stm32/malloc/mallocr.c ****     Otherwise, if the reallocation is for additional space, and the
2741:../target/stm32/malloc/mallocr.c ****     chunk can be extended, it is, else a malloc-copy-free sequence is
2742:../target/stm32/malloc/mallocr.c ****     taken.  There are several different ways that a chunk could be
2743:../target/stm32/malloc/mallocr.c ****     extended. All are tried:
2744:../target/stm32/malloc/mallocr.c **** 
2745:../target/stm32/malloc/mallocr.c ****        * Extending forward into following adjacent free chunk.
2746:../target/stm32/malloc/mallocr.c ****        * Shifting backwards, joining preceding adjacent space
2747:../target/stm32/malloc/mallocr.c ****        * Both shifting backwards and extending forward.
2748:../target/stm32/malloc/mallocr.c ****        * Extending into newly sbrked space
2749:../target/stm32/malloc/mallocr.c **** 
2750:../target/stm32/malloc/mallocr.c ****     Unless the #define REALLOC_ZERO_BYTES_FREES is set, realloc with a
2751:../target/stm32/malloc/mallocr.c ****     size argument of zero (re)allocates a minimum-sized chunk.
2752:../target/stm32/malloc/mallocr.c **** 
2753:../target/stm32/malloc/mallocr.c ****     If the reallocation is for less space, and the new request is for
2754:../target/stm32/malloc/mallocr.c ****     a `small' (<512 bytes) size, then the newly unused space is lopped
2755:../target/stm32/malloc/mallocr.c ****     off and freed.
2756:../target/stm32/malloc/mallocr.c **** 
2757:../target/stm32/malloc/mallocr.c ****     The old unix realloc convention of allowing the last-free'd chunk
2758:../target/stm32/malloc/mallocr.c ****     to be used as an argument to realloc is no longer supported.
2759:../target/stm32/malloc/mallocr.c ****     I don't know of any programs still relying on this feature,
2760:../target/stm32/malloc/mallocr.c ****     and allowing it would also allow too many other incorrect 
2761:../target/stm32/malloc/mallocr.c ****     usages of realloc to be sensible.
2762:../target/stm32/malloc/mallocr.c **** 
2763:../target/stm32/malloc/mallocr.c **** 
2764:../target/stm32/malloc/mallocr.c **** */
2765:../target/stm32/malloc/mallocr.c **** 
2766:../target/stm32/malloc/mallocr.c **** 
2767:../target/stm32/malloc/mallocr.c **** #if __STD_C
2768:../target/stm32/malloc/mallocr.c **** Void_t* rEALLOc(RARG Void_t* oldmem, size_t bytes)
2769:../target/stm32/malloc/mallocr.c **** #else
2770:../target/stm32/malloc/mallocr.c **** Void_t* rEALLOc(RARG oldmem, bytes) RDECL Void_t* oldmem; size_t bytes;
2771:../target/stm32/malloc/mallocr.c **** #endif
2772:../target/stm32/malloc/mallocr.c **** {
 5239              		.loc 1 2772 0
 5240              		.cfi_startproc
 5241              		@ args = 0, pretend = 0, frame = 120
 5242              		@ frame_needed = 1, uses_anonymous_args = 0
 5243 0000 80B5     		push	{r7, lr}
 5244              	.LCFI9:
 5245              		.cfi_def_cfa_offset 8
 5246 0002 9EB0     		sub	sp, sp, #120
 5247              	.LCFI10:
 5248              		.cfi_def_cfa_offset 128
 5249 0004 00AF     		add	r7, sp, #0
 5250              		.cfi_offset 14, -4
 5251              		.cfi_offset 7, -8
 5252              	.LCFI11:
 5253              		.cfi_def_cfa_register 7
 5254 0006 F860     		str	r0, [r7, #12]
 5255 0008 B960     		str	r1, [r7, #8]
 5256 000a 7A60     		str	r2, [r7, #4]
2773:../target/stm32/malloc/mallocr.c **** #ifdef MALLOC_PROVIDED
2774:../target/stm32/malloc/mallocr.c **** 
2775:../target/stm32/malloc/mallocr.c ****   realloc (oldmem, bytes);
2776:../target/stm32/malloc/mallocr.c **** 
2777:../target/stm32/malloc/mallocr.c **** #else
2778:../target/stm32/malloc/mallocr.c **** 
2779:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T    nb;      /* padded request size */
2780:../target/stm32/malloc/mallocr.c **** 
2781:../target/stm32/malloc/mallocr.c ****   mchunkptr oldp;             /* chunk corresponding to oldmem */
2782:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T    oldsize; /* its size */
2783:../target/stm32/malloc/mallocr.c **** 
2784:../target/stm32/malloc/mallocr.c ****   mchunkptr newp;             /* chunk to return */
2785:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T    newsize; /* its size */
2786:../target/stm32/malloc/mallocr.c ****   Void_t*   newmem;           /* corresponding user mem */
2787:../target/stm32/malloc/mallocr.c **** 
2788:../target/stm32/malloc/mallocr.c ****   mchunkptr next;             /* next contiguous chunk after oldp */
2789:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T  nextsize;  /* its size */
2790:../target/stm32/malloc/mallocr.c **** 
2791:../target/stm32/malloc/mallocr.c ****   mchunkptr prev;             /* previous contiguous chunk before oldp */
2792:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T  prevsize;  /* its size */
2793:../target/stm32/malloc/mallocr.c **** 
2794:../target/stm32/malloc/mallocr.c ****   mchunkptr remainder;        /* holds split off extra space from newp */
2795:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T  remainder_size;   /* its size */
2796:../target/stm32/malloc/mallocr.c **** 
2797:../target/stm32/malloc/mallocr.c ****   mchunkptr bck;              /* misc temp for linking */
2798:../target/stm32/malloc/mallocr.c ****   mchunkptr fwd;              /* misc temp for linking */
2799:../target/stm32/malloc/mallocr.c **** 
2800:../target/stm32/malloc/mallocr.c **** #ifdef REALLOC_ZERO_BYTES_FREES
2801:../target/stm32/malloc/mallocr.c ****   if (bytes == 0) { fREe(RCALL oldmem); return 0; }
2802:../target/stm32/malloc/mallocr.c **** #endif
2803:../target/stm32/malloc/mallocr.c **** 
2804:../target/stm32/malloc/mallocr.c **** 
2805:../target/stm32/malloc/mallocr.c ****   /* realloc of null is supposed to be same as malloc */
2806:../target/stm32/malloc/mallocr.c ****   if (oldmem == 0) return mALLOc(RCALL bytes);
 5257              		.loc 1 2806 0
 5258 000c BB68     		ldr	r3, [r7, #8]
 5259 000e 002B     		cmp	r3, #0
 5260 0010 05D1     		bne	.L113
 5261              		.loc 1 2806 0 is_stmt 0 discriminator 1
 5262 0012 F868     		ldr	r0, [r7, #12]
 5263 0014 7968     		ldr	r1, [r7, #4]
 5264 0016 FFF7FEFF 		bl	_malloc_r
 5265 001a 0346     		mov	r3, r0
 5266 001c C3E3     		b	.L114
 5267              	.L113:
2807:../target/stm32/malloc/mallocr.c **** 
2808:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK;
 5268              		.loc 1 2808 0 is_stmt 1
 5269 001e F868     		ldr	r0, [r7, #12]
 5270 0020 FFF7FEFF 		bl	__malloc_lock
2809:../target/stm32/malloc/mallocr.c **** 
2810:../target/stm32/malloc/mallocr.c ****   newp    = oldp    = mem2chunk(oldmem);
 5271              		.loc 1 2810 0
 5272 0024 BB68     		ldr	r3, [r7, #8]
 5273 0026 A3F10803 		sub	r3, r3, #8
 5274 002a 7B64     		str	r3, [r7, #68]
 5275 002c 7B6C     		ldr	r3, [r7, #68]
 5276 002e 7B67     		str	r3, [r7, #116]
2811:../target/stm32/malloc/mallocr.c ****   newsize = oldsize = chunksize(oldp);
 5277              		.loc 1 2811 0
 5278 0030 7B6C     		ldr	r3, [r7, #68]
 5279 0032 5B68     		ldr	r3, [r3, #4]
 5280 0034 23F00303 		bic	r3, r3, #3
 5281 0038 3B64     		str	r3, [r7, #64]
 5282 003a 3B6C     		ldr	r3, [r7, #64]
 5283 003c 3B67     		str	r3, [r7, #112]
2812:../target/stm32/malloc/mallocr.c **** 
2813:../target/stm32/malloc/mallocr.c **** 
2814:../target/stm32/malloc/mallocr.c ****   nb = request2size(bytes);
 5284              		.loc 1 2814 0
 5285 003e 7B68     		ldr	r3, [r7, #4]
 5286 0040 03F10B03 		add	r3, r3, #11
 5287 0044 162B     		cmp	r3, #22
 5288 0046 05D9     		bls	.L115
 5289              		.loc 1 2814 0 is_stmt 0 discriminator 1
 5290 0048 7B68     		ldr	r3, [r7, #4]
 5291 004a 03F10B03 		add	r3, r3, #11
 5292 004e 23F00703 		bic	r3, r3, #7
 5293 0052 01E0     		b	.L116
 5294              	.L115:
 5295              		.loc 1 2814 0 discriminator 2
 5296 0054 4FF01003 		mov	r3, #16
 5297              	.L116:
 5298              		.loc 1 2814 0 discriminator 3
 5299 0058 FB63     		str	r3, [r7, #60]
2815:../target/stm32/malloc/mallocr.c **** 
2816:../target/stm32/malloc/mallocr.c ****   /* Check for overflow and just fail, if so. */
2817:../target/stm32/malloc/mallocr.c ****   if (nb > INT_MAX || nb < bytes)
 5300              		.loc 1 2817 0 is_stmt 1 discriminator 3
 5301 005a FB6B     		ldr	r3, [r7, #60]
 5302 005c 002B     		cmp	r3, #0
 5303 005e 03DB     		blt	.L117
 5304              		.loc 1 2817 0 is_stmt 0 discriminator 1
 5305 0060 FA6B     		ldr	r2, [r7, #60]
 5306 0062 7B68     		ldr	r3, [r7, #4]
 5307 0064 9A42     		cmp	r2, r3
 5308 0066 06D2     		bcs	.L118
 5309              	.L117:
2818:../target/stm32/malloc/mallocr.c ****   {
2819:../target/stm32/malloc/mallocr.c ****     RERRNO = ENOMEM;
 5310              		.loc 1 2819 0 is_stmt 1
 5311 0068 FB68     		ldr	r3, [r7, #12]
 5312 006a 4FF00C02 		mov	r2, #12
 5313 006e 1A60     		str	r2, [r3, #0]
2820:../target/stm32/malloc/mallocr.c ****     return 0;
 5314              		.loc 1 2820 0
 5315 0070 4FF00003 		mov	r3, #0
 5316 0074 97E3     		b	.L114
 5317              	.L118:
2821:../target/stm32/malloc/mallocr.c ****   }
2822:../target/stm32/malloc/mallocr.c **** 
2823:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
2824:../target/stm32/malloc/mallocr.c ****   if (chunk_is_mmapped(oldp)) 
2825:../target/stm32/malloc/mallocr.c ****   {
2826:../target/stm32/malloc/mallocr.c **** #if HAVE_MREMAP
2827:../target/stm32/malloc/mallocr.c ****     newp = mremap_chunk(oldp, nb);
2828:../target/stm32/malloc/mallocr.c ****     if(newp)
2829:../target/stm32/malloc/mallocr.c ****     {
2830:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
2831:../target/stm32/malloc/mallocr.c ****       return chunk2mem(newp);
2832:../target/stm32/malloc/mallocr.c ****     }
2833:../target/stm32/malloc/mallocr.c **** #endif
2834:../target/stm32/malloc/mallocr.c ****     /* Note the extra SIZE_SZ overhead. */
2835:../target/stm32/malloc/mallocr.c ****     if(oldsize - SIZE_SZ >= nb)
2836:../target/stm32/malloc/mallocr.c ****     {
2837:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
2838:../target/stm32/malloc/mallocr.c ****       return oldmem; /* do nothing */
2839:../target/stm32/malloc/mallocr.c ****     }
2840:../target/stm32/malloc/mallocr.c ****     /* Must alloc, copy, free. */
2841:../target/stm32/malloc/mallocr.c ****     newmem = mALLOc(RCALL bytes);
2842:../target/stm32/malloc/mallocr.c ****     if (newmem == 0)
2843:../target/stm32/malloc/mallocr.c ****     {
2844:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
2845:../target/stm32/malloc/mallocr.c ****       return 0; /* propagate failure */
2846:../target/stm32/malloc/mallocr.c ****     }
2847:../target/stm32/malloc/mallocr.c ****     MALLOC_COPY(newmem, oldmem, oldsize - 2*SIZE_SZ);
2848:../target/stm32/malloc/mallocr.c ****     munmap_chunk(oldp);
2849:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
2850:../target/stm32/malloc/mallocr.c ****     return newmem;
2851:../target/stm32/malloc/mallocr.c ****   }
2852:../target/stm32/malloc/mallocr.c **** #endif
2853:../target/stm32/malloc/mallocr.c **** 
2854:../target/stm32/malloc/mallocr.c ****   check_inuse_chunk(oldp);
2855:../target/stm32/malloc/mallocr.c **** 
2856:../target/stm32/malloc/mallocr.c ****   if ((long)(oldsize) < (long)(nb))  
 5318              		.loc 1 2856 0
 5319 0076 3A6C     		ldr	r2, [r7, #64]
 5320 0078 FB6B     		ldr	r3, [r7, #60]
 5321 007a 9A42     		cmp	r2, r3
 5322 007c 80F25183 		bge	.L119
2857:../target/stm32/malloc/mallocr.c ****   {
2858:../target/stm32/malloc/mallocr.c **** 
2859:../target/stm32/malloc/mallocr.c ****     /* Try expanding forward */
2860:../target/stm32/malloc/mallocr.c **** 
2861:../target/stm32/malloc/mallocr.c ****     next = chunk_at_offset(oldp, oldsize);
 5323              		.loc 1 2861 0
 5324 0080 7A6C     		ldr	r2, [r7, #68]
 5325 0082 3B6C     		ldr	r3, [r7, #64]
 5326 0084 D318     		adds	r3, r2, r3
 5327 0086 FB66     		str	r3, [r7, #108]
2862:../target/stm32/malloc/mallocr.c ****     if (next == top || !inuse(next)) 
 5328              		.loc 1 2862 0
 5329 0088 40F20003 		movw	r3, #:lower16:__malloc_av_
 5330 008c C0F20003 		movt	r3, #:upper16:__malloc_av_
 5331 0090 9A68     		ldr	r2, [r3, #8]
 5332 0092 FB6E     		ldr	r3, [r7, #108]
 5333 0094 9A42     		cmp	r2, r3
 5334 0096 0AD0     		beq	.L120
 5335              		.loc 1 2862 0 is_stmt 0 discriminator 1
 5336 0098 FB6E     		ldr	r3, [r7, #108]
 5337 009a 5B68     		ldr	r3, [r3, #4]
 5338 009c 23F00103 		bic	r3, r3, #1
 5339 00a0 FA6E     		ldr	r2, [r7, #108]
 5340 00a2 D318     		adds	r3, r2, r3
 5341 00a4 5B68     		ldr	r3, [r3, #4]
 5342 00a6 03F00103 		and	r3, r3, #1
 5343 00aa 002B     		cmp	r3, #0
 5344 00ac 53D1     		bne	.L121
 5345              	.L120:
2863:../target/stm32/malloc/mallocr.c ****     {
2864:../target/stm32/malloc/mallocr.c ****       nextsize = chunksize(next);
 5346              		.loc 1 2864 0 is_stmt 1
 5347 00ae FB6E     		ldr	r3, [r7, #108]
 5348 00b0 5B68     		ldr	r3, [r3, #4]
 5349 00b2 23F00303 		bic	r3, r3, #3
 5350 00b6 BB66     		str	r3, [r7, #104]
2865:../target/stm32/malloc/mallocr.c **** 
2866:../target/stm32/malloc/mallocr.c ****       /* Forward into top only if a remainder */
2867:../target/stm32/malloc/mallocr.c ****       if (next == top)
 5351              		.loc 1 2867 0
 5352 00b8 40F20003 		movw	r3, #:lower16:__malloc_av_
 5353 00bc C0F20003 		movt	r3, #:upper16:__malloc_av_
 5354 00c0 9A68     		ldr	r2, [r3, #8]
 5355 00c2 FB6E     		ldr	r3, [r7, #108]
 5356 00c4 9A42     		cmp	r2, r3
 5357 00c6 2ED1     		bne	.L122
2868:../target/stm32/malloc/mallocr.c ****       {
2869:../target/stm32/malloc/mallocr.c ****         if ((long)(nextsize + newsize) >= (long)(nb + MINSIZE))
 5358              		.loc 1 2869 0
 5359 00c8 BA6E     		ldr	r2, [r7, #104]
 5360 00ca 3B6F     		ldr	r3, [r7, #112]
 5361 00cc D318     		adds	r3, r2, r3
 5362 00ce 1A46     		mov	r2, r3
 5363 00d0 FB6B     		ldr	r3, [r7, #60]
 5364 00d2 03F11003 		add	r3, r3, #16
 5365 00d6 9A42     		cmp	r2, r3
 5366 00d8 44DB     		blt	.L144
2870:../target/stm32/malloc/mallocr.c ****         {
2871:../target/stm32/malloc/mallocr.c ****           newsize += nextsize;
 5367              		.loc 1 2871 0
 5368 00da 3A6F     		ldr	r2, [r7, #112]
 5369 00dc BB6E     		ldr	r3, [r7, #104]
 5370 00de D318     		adds	r3, r2, r3
 5371 00e0 3B67     		str	r3, [r7, #112]
2872:../target/stm32/malloc/mallocr.c ****           top = chunk_at_offset(oldp, nb);
 5372              		.loc 1 2872 0
 5373 00e2 40F20003 		movw	r3, #:lower16:__malloc_av_
 5374 00e6 C0F20003 		movt	r3, #:upper16:__malloc_av_
 5375 00ea 796C     		ldr	r1, [r7, #68]
 5376 00ec FA6B     		ldr	r2, [r7, #60]
 5377 00ee 8A18     		adds	r2, r1, r2
 5378 00f0 9A60     		str	r2, [r3, #8]
2873:../target/stm32/malloc/mallocr.c ****           set_head(top, (newsize - nb) | PREV_INUSE);
 5379              		.loc 1 2873 0
 5380 00f2 40F20003 		movw	r3, #:lower16:__malloc_av_
 5381 00f6 C0F20003 		movt	r3, #:upper16:__malloc_av_
 5382 00fa 9B68     		ldr	r3, [r3, #8]
 5383 00fc 396F     		ldr	r1, [r7, #112]
 5384 00fe FA6B     		ldr	r2, [r7, #60]
 5385 0100 8A1A     		subs	r2, r1, r2
 5386 0102 42F00102 		orr	r2, r2, #1
 5387 0106 5A60     		str	r2, [r3, #4]
2874:../target/stm32/malloc/mallocr.c ****           set_head_size(oldp, nb);
 5388              		.loc 1 2874 0
 5389 0108 7B6C     		ldr	r3, [r7, #68]
 5390 010a 5B68     		ldr	r3, [r3, #4]
 5391 010c 03F00102 		and	r2, r3, #1
 5392 0110 FB6B     		ldr	r3, [r7, #60]
 5393 0112 1A43     		orrs	r2, r2, r3
 5394 0114 7B6C     		ldr	r3, [r7, #68]
 5395 0116 5A60     		str	r2, [r3, #4]
2875:../target/stm32/malloc/mallocr.c **** 	  MALLOC_UNLOCK;
 5396              		.loc 1 2875 0
 5397 0118 F868     		ldr	r0, [r7, #12]
 5398 011a FFF7FEFF 		bl	__malloc_unlock
2876:../target/stm32/malloc/mallocr.c ****           return chunk2mem(oldp);
 5399              		.loc 1 2876 0
 5400 011e 7B6C     		ldr	r3, [r7, #68]
 5401 0120 03F10803 		add	r3, r3, #8
 5402 0124 3FE3     		b	.L114
 5403              	.L122:
2877:../target/stm32/malloc/mallocr.c ****         }
2878:../target/stm32/malloc/mallocr.c ****       }
2879:../target/stm32/malloc/mallocr.c **** 
2880:../target/stm32/malloc/mallocr.c ****       /* Forward into next chunk */
2881:../target/stm32/malloc/mallocr.c ****       else if (((long)(nextsize + newsize) >= (long)(nb)))
 5404              		.loc 1 2881 0
 5405 0126 BA6E     		ldr	r2, [r7, #104]
 5406 0128 3B6F     		ldr	r3, [r7, #112]
 5407 012a D318     		adds	r3, r2, r3
 5408 012c 1A46     		mov	r2, r3
 5409 012e FB6B     		ldr	r3, [r7, #60]
 5410 0130 9A42     		cmp	r2, r3
 5411 0132 19DB     		blt	.L145
2882:../target/stm32/malloc/mallocr.c ****       { 
2883:../target/stm32/malloc/mallocr.c ****         unlink(next, bck, fwd);
 5412              		.loc 1 2883 0
 5413 0134 FB6E     		ldr	r3, [r7, #108]
 5414 0136 DB68     		ldr	r3, [r3, #12]
 5415 0138 BB63     		str	r3, [r7, #56]
 5416 013a FB6E     		ldr	r3, [r7, #108]
 5417 013c 9B68     		ldr	r3, [r3, #8]
 5418 013e 7B63     		str	r3, [r7, #52]
 5419 0140 7B6B     		ldr	r3, [r7, #52]
 5420 0142 BA6B     		ldr	r2, [r7, #56]
 5421 0144 DA60     		str	r2, [r3, #12]
 5422 0146 BB6B     		ldr	r3, [r7, #56]
 5423 0148 7A6B     		ldr	r2, [r7, #52]
 5424 014a 9A60     		str	r2, [r3, #8]
2884:../target/stm32/malloc/mallocr.c ****         newsize  += nextsize;
 5425              		.loc 1 2884 0
 5426 014c 3A6F     		ldr	r2, [r7, #112]
 5427 014e BB6E     		ldr	r3, [r7, #104]
 5428 0150 D318     		adds	r3, r2, r3
 5429 0152 3B67     		str	r3, [r7, #112]
2885:../target/stm32/malloc/mallocr.c ****         goto split;
 5430              		.loc 1 2885 0
 5431 0154 E5E2     		b	.L119
 5432              	.L121:
2886:../target/stm32/malloc/mallocr.c ****       }
2887:../target/stm32/malloc/mallocr.c ****     }
2888:../target/stm32/malloc/mallocr.c ****     else
2889:../target/stm32/malloc/mallocr.c ****     {
2890:../target/stm32/malloc/mallocr.c ****       next = 0;
 5433              		.loc 1 2890 0
 5434 0156 4FF00003 		mov	r3, #0
 5435 015a FB66     		str	r3, [r7, #108]
2891:../target/stm32/malloc/mallocr.c ****       nextsize = 0;
 5436              		.loc 1 2891 0
 5437 015c 4FF00003 		mov	r3, #0
 5438 0160 BB66     		str	r3, [r7, #104]
 5439 0162 02E0     		b	.L124
 5440              	.L144:
2867:../target/stm32/malloc/mallocr.c ****       if (next == top)
 5441              		.loc 1 2867 0
 5442 0164 00BF     		nop
 5443 0166 00E0     		b	.L124
 5444              	.L145:
 5445 0168 00BF     		nop
 5446              	.L124:
2892:../target/stm32/malloc/mallocr.c ****     }
2893:../target/stm32/malloc/mallocr.c **** 
2894:../target/stm32/malloc/mallocr.c ****     /* Try shifting backwards. */
2895:../target/stm32/malloc/mallocr.c **** 
2896:../target/stm32/malloc/mallocr.c ****     if (!prev_inuse(oldp))
 5447              		.loc 1 2896 0
 5448 016a 7B6C     		ldr	r3, [r7, #68]
 5449 016c 5B68     		ldr	r3, [r3, #4]
 5450 016e 03F00103 		and	r3, r3, #1
 5451 0172 002B     		cmp	r3, #0
 5452 0174 40F02982 		bne	.L125
2897:../target/stm32/malloc/mallocr.c ****     {
2898:../target/stm32/malloc/mallocr.c ****       prev = prev_chunk(oldp);
 5453              		.loc 1 2898 0
 5454 0178 7B6C     		ldr	r3, [r7, #68]
 5455 017a 1B68     		ldr	r3, [r3, #0]
 5456 017c C3F10003 		rsb	r3, r3, #0
 5457 0180 7A6C     		ldr	r2, [r7, #68]
 5458 0182 D318     		adds	r3, r2, r3
 5459 0184 3B63     		str	r3, [r7, #48]
2899:../target/stm32/malloc/mallocr.c ****       prevsize = chunksize(prev);
 5460              		.loc 1 2899 0
 5461 0186 3B6B     		ldr	r3, [r7, #48]
 5462 0188 5B68     		ldr	r3, [r3, #4]
 5463 018a 23F00303 		bic	r3, r3, #3
 5464 018e FB62     		str	r3, [r7, #44]
2900:../target/stm32/malloc/mallocr.c **** 
2901:../target/stm32/malloc/mallocr.c ****       /* try forward + backward first to save a later consolidation */
2902:../target/stm32/malloc/mallocr.c **** 
2903:../target/stm32/malloc/mallocr.c ****       if (next != 0)
 5465              		.loc 1 2903 0
 5466 0190 FB6E     		ldr	r3, [r7, #108]
 5467 0192 002B     		cmp	r3, #0
 5468 0194 00F07881 		beq	.L126
2904:../target/stm32/malloc/mallocr.c ****       {
2905:../target/stm32/malloc/mallocr.c ****         /* into top */
2906:../target/stm32/malloc/mallocr.c ****         if (next == top)
 5469              		.loc 1 2906 0
 5470 0198 40F20003 		movw	r3, #:lower16:__malloc_av_
 5471 019c C0F20003 		movt	r3, #:upper16:__malloc_av_
 5472 01a0 9A68     		ldr	r2, [r3, #8]
 5473 01a2 FB6E     		ldr	r3, [r7, #108]
 5474 01a4 9A42     		cmp	r2, r3
 5475 01a6 40F0C280 		bne	.L127
2907:../target/stm32/malloc/mallocr.c ****         {
2908:../target/stm32/malloc/mallocr.c ****           if ((long)(nextsize + prevsize + newsize) >= (long)(nb + MINSIZE))
 5476              		.loc 1 2908 0
 5477 01aa BA6E     		ldr	r2, [r7, #104]
 5478 01ac FB6A     		ldr	r3, [r7, #44]
 5479 01ae D218     		adds	r2, r2, r3
 5480 01b0 3B6F     		ldr	r3, [r7, #112]
 5481 01b2 D318     		adds	r3, r2, r3
 5482 01b4 1A46     		mov	r2, r3
 5483 01b6 FB6B     		ldr	r3, [r7, #60]
 5484 01b8 03F11003 		add	r3, r3, #16
 5485 01bc 9A42     		cmp	r2, r3
 5486 01be C0F26381 		blt	.L126
2909:../target/stm32/malloc/mallocr.c ****           {
2910:../target/stm32/malloc/mallocr.c ****             unlink(prev, bck, fwd);
 5487              		.loc 1 2910 0
 5488 01c2 3B6B     		ldr	r3, [r7, #48]
 5489 01c4 DB68     		ldr	r3, [r3, #12]
 5490 01c6 BB63     		str	r3, [r7, #56]
 5491 01c8 3B6B     		ldr	r3, [r7, #48]
 5492 01ca 9B68     		ldr	r3, [r3, #8]
 5493 01cc 7B63     		str	r3, [r7, #52]
 5494 01ce 7B6B     		ldr	r3, [r7, #52]
 5495 01d0 BA6B     		ldr	r2, [r7, #56]
 5496 01d2 DA60     		str	r2, [r3, #12]
 5497 01d4 BB6B     		ldr	r3, [r7, #56]
 5498 01d6 7A6B     		ldr	r2, [r7, #52]
 5499 01d8 9A60     		str	r2, [r3, #8]
2911:../target/stm32/malloc/mallocr.c ****             newp = prev;
 5500              		.loc 1 2911 0
 5501 01da 3B6B     		ldr	r3, [r7, #48]
 5502 01dc 7B67     		str	r3, [r7, #116]
2912:../target/stm32/malloc/mallocr.c ****             newsize += prevsize + nextsize;
 5503              		.loc 1 2912 0
 5504 01de FA6A     		ldr	r2, [r7, #44]
 5505 01e0 BB6E     		ldr	r3, [r7, #104]
 5506 01e2 D318     		adds	r3, r2, r3
 5507 01e4 3A6F     		ldr	r2, [r7, #112]
 5508 01e6 D318     		adds	r3, r2, r3
 5509 01e8 3B67     		str	r3, [r7, #112]
2913:../target/stm32/malloc/mallocr.c ****             newmem = chunk2mem(newp);
 5510              		.loc 1 2913 0
 5511 01ea 7B6F     		ldr	r3, [r7, #116]
 5512 01ec 03F10803 		add	r3, r3, #8
 5513 01f0 BB62     		str	r3, [r7, #40]
 5514              	.LBB2:
2914:../target/stm32/malloc/mallocr.c ****             MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
 5515              		.loc 1 2914 0
 5516 01f2 3B6C     		ldr	r3, [r7, #64]
 5517 01f4 A3F10403 		sub	r3, r3, #4
 5518 01f8 7B62     		str	r3, [r7, #36]
 5519 01fa 7B6A     		ldr	r3, [r7, #36]
 5520 01fc 242B     		cmp	r3, #36
 5521 01fe 71D8     		bhi	.L128
 5522              	.LBB3:
 5523              		.loc 1 2914 0 is_stmt 0 discriminator 1
 5524 0200 BB68     		ldr	r3, [r7, #8]
 5525 0202 7B66     		str	r3, [r7, #100]
 5526 0204 BB6A     		ldr	r3, [r7, #40]
 5527 0206 3B66     		str	r3, [r7, #96]
 5528 0208 7B6A     		ldr	r3, [r7, #36]
 5529 020a 132B     		cmp	r3, #19
 5530 020c 4DD9     		bls	.L129
 5531              		.loc 1 2914 0 discriminator 3
 5532 020e 7B6E     		ldr	r3, [r7, #100]
 5533 0210 1A68     		ldr	r2, [r3, #0]
 5534 0212 3B6E     		ldr	r3, [r7, #96]
 5535 0214 1A60     		str	r2, [r3, #0]
 5536 0216 3B6E     		ldr	r3, [r7, #96]
 5537 0218 03F10403 		add	r3, r3, #4
 5538 021c 3B66     		str	r3, [r7, #96]
 5539 021e 7B6E     		ldr	r3, [r7, #100]
 5540 0220 03F10403 		add	r3, r3, #4
 5541 0224 7B66     		str	r3, [r7, #100]
 5542 0226 7B6E     		ldr	r3, [r7, #100]
 5543 0228 1A68     		ldr	r2, [r3, #0]
 5544 022a 3B6E     		ldr	r3, [r7, #96]
 5545 022c 1A60     		str	r2, [r3, #0]
 5546 022e 3B6E     		ldr	r3, [r7, #96]
 5547 0230 03F10403 		add	r3, r3, #4
 5548 0234 3B66     		str	r3, [r7, #96]
 5549 0236 7B6E     		ldr	r3, [r7, #100]
 5550 0238 03F10403 		add	r3, r3, #4
 5551 023c 7B66     		str	r3, [r7, #100]
 5552 023e 7B6A     		ldr	r3, [r7, #36]
 5553 0240 1B2B     		cmp	r3, #27
 5554 0242 32D9     		bls	.L129
 5555              		.loc 1 2914 0 discriminator 5
 5556 0244 7B6E     		ldr	r3, [r7, #100]
 5557 0246 1A68     		ldr	r2, [r3, #0]
 5558 0248 3B6E     		ldr	r3, [r7, #96]
 5559 024a 1A60     		str	r2, [r3, #0]
 5560 024c 3B6E     		ldr	r3, [r7, #96]
 5561 024e 03F10403 		add	r3, r3, #4
 5562 0252 3B66     		str	r3, [r7, #96]
 5563 0254 7B6E     		ldr	r3, [r7, #100]
 5564 0256 03F10403 		add	r3, r3, #4
 5565 025a 7B66     		str	r3, [r7, #100]
 5566 025c 7B6E     		ldr	r3, [r7, #100]
 5567 025e 1A68     		ldr	r2, [r3, #0]
 5568 0260 3B6E     		ldr	r3, [r7, #96]
 5569 0262 1A60     		str	r2, [r3, #0]
 5570 0264 3B6E     		ldr	r3, [r7, #96]
 5571 0266 03F10403 		add	r3, r3, #4
 5572 026a 3B66     		str	r3, [r7, #96]
 5573 026c 7B6E     		ldr	r3, [r7, #100]
 5574 026e 03F10403 		add	r3, r3, #4
 5575 0272 7B66     		str	r3, [r7, #100]
 5576 0274 7B6A     		ldr	r3, [r7, #36]
 5577 0276 232B     		cmp	r3, #35
 5578 0278 17D9     		bls	.L129
 5579              		.loc 1 2914 0 discriminator 6
 5580 027a 7B6E     		ldr	r3, [r7, #100]
 5581 027c 1A68     		ldr	r2, [r3, #0]
 5582 027e 3B6E     		ldr	r3, [r7, #96]
 5583 0280 1A60     		str	r2, [r3, #0]
 5584 0282 3B6E     		ldr	r3, [r7, #96]
 5585 0284 03F10403 		add	r3, r3, #4
 5586 0288 3B66     		str	r3, [r7, #96]
 5587 028a 7B6E     		ldr	r3, [r7, #100]
 5588 028c 03F10403 		add	r3, r3, #4
 5589 0290 7B66     		str	r3, [r7, #100]
 5590 0292 7B6E     		ldr	r3, [r7, #100]
 5591 0294 1A68     		ldr	r2, [r3, #0]
 5592 0296 3B6E     		ldr	r3, [r7, #96]
 5593 0298 1A60     		str	r2, [r3, #0]
 5594 029a 3B6E     		ldr	r3, [r7, #96]
 5595 029c 03F10403 		add	r3, r3, #4
 5596 02a0 3B66     		str	r3, [r7, #96]
 5597 02a2 7B6E     		ldr	r3, [r7, #100]
 5598 02a4 03F10403 		add	r3, r3, #4
 5599 02a8 7B66     		str	r3, [r7, #100]
 5600              	.L129:
 5601              		.loc 1 2914 0 discriminator 4
 5602 02aa 7B6E     		ldr	r3, [r7, #100]
 5603 02ac 1A68     		ldr	r2, [r3, #0]
 5604 02ae 3B6E     		ldr	r3, [r7, #96]
 5605 02b0 1A60     		str	r2, [r3, #0]
 5606 02b2 3B6E     		ldr	r3, [r7, #96]
 5607 02b4 03F10403 		add	r3, r3, #4
 5608 02b8 3B66     		str	r3, [r7, #96]
 5609 02ba 7B6E     		ldr	r3, [r7, #100]
 5610 02bc 03F10403 		add	r3, r3, #4
 5611 02c0 7B66     		str	r3, [r7, #100]
 5612 02c2 7B6E     		ldr	r3, [r7, #100]
 5613 02c4 1A68     		ldr	r2, [r3, #0]
 5614 02c6 3B6E     		ldr	r3, [r7, #96]
 5615 02c8 1A60     		str	r2, [r3, #0]
 5616 02ca 3B6E     		ldr	r3, [r7, #96]
 5617 02cc 03F10403 		add	r3, r3, #4
 5618 02d0 3B66     		str	r3, [r7, #96]
 5619 02d2 7B6E     		ldr	r3, [r7, #100]
 5620 02d4 03F10403 		add	r3, r3, #4
 5621 02d8 7B66     		str	r3, [r7, #100]
 5622 02da 7B6E     		ldr	r3, [r7, #100]
 5623 02dc 1A68     		ldr	r2, [r3, #0]
 5624 02de 3B6E     		ldr	r3, [r7, #96]
 5625 02e0 1A60     		str	r2, [r3, #0]
 5626 02e2 04E0     		b	.L130
 5627              	.L128:
 5628              	.LBE3:
 5629              		.loc 1 2914 0 discriminator 2
 5630 02e4 B86A     		ldr	r0, [r7, #40]
 5631 02e6 B968     		ldr	r1, [r7, #8]
 5632 02e8 7A6A     		ldr	r2, [r7, #36]
 5633 02ea FFF7FEFF 		bl	memmove
 5634              	.L130:
 5635              	.LBE2:
2915:../target/stm32/malloc/mallocr.c ****             top = chunk_at_offset(newp, nb);
 5636              		.loc 1 2915 0 is_stmt 1
 5637 02ee 40F20003 		movw	r3, #:lower16:__malloc_av_
 5638 02f2 C0F20003 		movt	r3, #:upper16:__malloc_av_
 5639 02f6 796F     		ldr	r1, [r7, #116]
 5640 02f8 FA6B     		ldr	r2, [r7, #60]
 5641 02fa 8A18     		adds	r2, r1, r2
 5642 02fc 9A60     		str	r2, [r3, #8]
2916:../target/stm32/malloc/mallocr.c ****             set_head(top, (newsize - nb) | PREV_INUSE);
 5643              		.loc 1 2916 0
 5644 02fe 40F20003 		movw	r3, #:lower16:__malloc_av_
 5645 0302 C0F20003 		movt	r3, #:upper16:__malloc_av_
 5646 0306 9B68     		ldr	r3, [r3, #8]
 5647 0308 396F     		ldr	r1, [r7, #112]
 5648 030a FA6B     		ldr	r2, [r7, #60]
 5649 030c 8A1A     		subs	r2, r1, r2
 5650 030e 42F00102 		orr	r2, r2, #1
 5651 0312 5A60     		str	r2, [r3, #4]
2917:../target/stm32/malloc/mallocr.c ****             set_head_size(newp, nb);
 5652              		.loc 1 2917 0
 5653 0314 7B6F     		ldr	r3, [r7, #116]
 5654 0316 5B68     		ldr	r3, [r3, #4]
 5655 0318 03F00102 		and	r2, r3, #1
 5656 031c FB6B     		ldr	r3, [r7, #60]
 5657 031e 1A43     		orrs	r2, r2, r3
 5658 0320 7B6F     		ldr	r3, [r7, #116]
 5659 0322 5A60     		str	r2, [r3, #4]
2918:../target/stm32/malloc/mallocr.c **** 	    MALLOC_UNLOCK;
 5660              		.loc 1 2918 0
 5661 0324 F868     		ldr	r0, [r7, #12]
 5662 0326 FFF7FEFF 		bl	__malloc_unlock
2919:../target/stm32/malloc/mallocr.c ****             return newmem;
 5663              		.loc 1 2919 0
 5664 032a BB6A     		ldr	r3, [r7, #40]
 5665 032c 3BE2     		b	.L114
 5666              	.L127:
2920:../target/stm32/malloc/mallocr.c ****           }
2921:../target/stm32/malloc/mallocr.c ****         }
2922:../target/stm32/malloc/mallocr.c **** 
2923:../target/stm32/malloc/mallocr.c ****         /* into next chunk */
2924:../target/stm32/malloc/mallocr.c ****         else if (((long)(nextsize + prevsize + newsize) >= (long)(nb)))
 5667              		.loc 1 2924 0
 5668 032e BA6E     		ldr	r2, [r7, #104]
 5669 0330 FB6A     		ldr	r3, [r7, #44]
 5670 0332 D218     		adds	r2, r2, r3
 5671 0334 3B6F     		ldr	r3, [r7, #112]
 5672 0336 D318     		adds	r3, r2, r3
 5673 0338 1A46     		mov	r2, r3
 5674 033a FB6B     		ldr	r3, [r7, #60]
 5675 033c 9A42     		cmp	r2, r3
 5676 033e C0F2A380 		blt	.L126
2925:../target/stm32/malloc/mallocr.c ****         {
2926:../target/stm32/malloc/mallocr.c ****           unlink(next, bck, fwd);
 5677              		.loc 1 2926 0
 5678 0342 FB6E     		ldr	r3, [r7, #108]
 5679 0344 DB68     		ldr	r3, [r3, #12]
 5680 0346 BB63     		str	r3, [r7, #56]
 5681 0348 FB6E     		ldr	r3, [r7, #108]
 5682 034a 9B68     		ldr	r3, [r3, #8]
 5683 034c 7B63     		str	r3, [r7, #52]
 5684 034e 7B6B     		ldr	r3, [r7, #52]
 5685 0350 BA6B     		ldr	r2, [r7, #56]
 5686 0352 DA60     		str	r2, [r3, #12]
 5687 0354 BB6B     		ldr	r3, [r7, #56]
 5688 0356 7A6B     		ldr	r2, [r7, #52]
 5689 0358 9A60     		str	r2, [r3, #8]
2927:../target/stm32/malloc/mallocr.c ****           unlink(prev, bck, fwd);
 5690              		.loc 1 2927 0
 5691 035a 3B6B     		ldr	r3, [r7, #48]
 5692 035c DB68     		ldr	r3, [r3, #12]
 5693 035e BB63     		str	r3, [r7, #56]
 5694 0360 3B6B     		ldr	r3, [r7, #48]
 5695 0362 9B68     		ldr	r3, [r3, #8]
 5696 0364 7B63     		str	r3, [r7, #52]
 5697 0366 7B6B     		ldr	r3, [r7, #52]
 5698 0368 BA6B     		ldr	r2, [r7, #56]
 5699 036a DA60     		str	r2, [r3, #12]
 5700 036c BB6B     		ldr	r3, [r7, #56]
 5701 036e 7A6B     		ldr	r2, [r7, #52]
 5702 0370 9A60     		str	r2, [r3, #8]
2928:../target/stm32/malloc/mallocr.c ****           newp = prev;
 5703              		.loc 1 2928 0
 5704 0372 3B6B     		ldr	r3, [r7, #48]
 5705 0374 7B67     		str	r3, [r7, #116]
2929:../target/stm32/malloc/mallocr.c ****           newsize += nextsize + prevsize;
 5706              		.loc 1 2929 0
 5707 0376 BA6E     		ldr	r2, [r7, #104]
 5708 0378 FB6A     		ldr	r3, [r7, #44]
 5709 037a D318     		adds	r3, r2, r3
 5710 037c 3A6F     		ldr	r2, [r7, #112]
 5711 037e D318     		adds	r3, r2, r3
 5712 0380 3B67     		str	r3, [r7, #112]
2930:../target/stm32/malloc/mallocr.c ****           newmem = chunk2mem(newp);
 5713              		.loc 1 2930 0
 5714 0382 7B6F     		ldr	r3, [r7, #116]
 5715 0384 03F10803 		add	r3, r3, #8
 5716 0388 BB62     		str	r3, [r7, #40]
 5717              	.LBB4:
2931:../target/stm32/malloc/mallocr.c ****           MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
 5718              		.loc 1 2931 0
 5719 038a 3B6C     		ldr	r3, [r7, #64]
 5720 038c A3F10403 		sub	r3, r3, #4
 5721 0390 3B62     		str	r3, [r7, #32]
 5722 0392 3B6A     		ldr	r3, [r7, #32]
 5723 0394 242B     		cmp	r3, #36
 5724 0396 71D8     		bhi	.L131
 5725              	.LBB5:
 5726              		.loc 1 2931 0 is_stmt 0 discriminator 1
 5727 0398 BB68     		ldr	r3, [r7, #8]
 5728 039a FB65     		str	r3, [r7, #92]
 5729 039c BB6A     		ldr	r3, [r7, #40]
 5730 039e BB65     		str	r3, [r7, #88]
 5731 03a0 3B6A     		ldr	r3, [r7, #32]
 5732 03a2 132B     		cmp	r3, #19
 5733 03a4 4DD9     		bls	.L132
 5734              		.loc 1 2931 0 discriminator 3
 5735 03a6 FB6D     		ldr	r3, [r7, #92]
 5736 03a8 1A68     		ldr	r2, [r3, #0]
 5737 03aa BB6D     		ldr	r3, [r7, #88]
 5738 03ac 1A60     		str	r2, [r3, #0]
 5739 03ae BB6D     		ldr	r3, [r7, #88]
 5740 03b0 03F10403 		add	r3, r3, #4
 5741 03b4 BB65     		str	r3, [r7, #88]
 5742 03b6 FB6D     		ldr	r3, [r7, #92]
 5743 03b8 03F10403 		add	r3, r3, #4
 5744 03bc FB65     		str	r3, [r7, #92]
 5745 03be FB6D     		ldr	r3, [r7, #92]
 5746 03c0 1A68     		ldr	r2, [r3, #0]
 5747 03c2 BB6D     		ldr	r3, [r7, #88]
 5748 03c4 1A60     		str	r2, [r3, #0]
 5749 03c6 BB6D     		ldr	r3, [r7, #88]
 5750 03c8 03F10403 		add	r3, r3, #4
 5751 03cc BB65     		str	r3, [r7, #88]
 5752 03ce FB6D     		ldr	r3, [r7, #92]
 5753 03d0 03F10403 		add	r3, r3, #4
 5754 03d4 FB65     		str	r3, [r7, #92]
 5755 03d6 3B6A     		ldr	r3, [r7, #32]
 5756 03d8 1B2B     		cmp	r3, #27
 5757 03da 32D9     		bls	.L132
 5758              		.loc 1 2931 0 discriminator 5
 5759 03dc FB6D     		ldr	r3, [r7, #92]
 5760 03de 1A68     		ldr	r2, [r3, #0]
 5761 03e0 BB6D     		ldr	r3, [r7, #88]
 5762 03e2 1A60     		str	r2, [r3, #0]
 5763 03e4 BB6D     		ldr	r3, [r7, #88]
 5764 03e6 03F10403 		add	r3, r3, #4
 5765 03ea BB65     		str	r3, [r7, #88]
 5766 03ec FB6D     		ldr	r3, [r7, #92]
 5767 03ee 03F10403 		add	r3, r3, #4
 5768 03f2 FB65     		str	r3, [r7, #92]
 5769 03f4 FB6D     		ldr	r3, [r7, #92]
 5770 03f6 1A68     		ldr	r2, [r3, #0]
 5771 03f8 BB6D     		ldr	r3, [r7, #88]
 5772 03fa 1A60     		str	r2, [r3, #0]
 5773 03fc BB6D     		ldr	r3, [r7, #88]
 5774 03fe 03F10403 		add	r3, r3, #4
 5775 0402 BB65     		str	r3, [r7, #88]
 5776 0404 FB6D     		ldr	r3, [r7, #92]
 5777 0406 03F10403 		add	r3, r3, #4
 5778 040a FB65     		str	r3, [r7, #92]
 5779 040c 3B6A     		ldr	r3, [r7, #32]
 5780 040e 232B     		cmp	r3, #35
 5781 0410 17D9     		bls	.L132
 5782              		.loc 1 2931 0 discriminator 6
 5783 0412 FB6D     		ldr	r3, [r7, #92]
 5784 0414 1A68     		ldr	r2, [r3, #0]
 5785 0416 BB6D     		ldr	r3, [r7, #88]
 5786 0418 1A60     		str	r2, [r3, #0]
 5787 041a BB6D     		ldr	r3, [r7, #88]
 5788 041c 03F10403 		add	r3, r3, #4
 5789 0420 BB65     		str	r3, [r7, #88]
 5790 0422 FB6D     		ldr	r3, [r7, #92]
 5791 0424 03F10403 		add	r3, r3, #4
 5792 0428 FB65     		str	r3, [r7, #92]
 5793 042a FB6D     		ldr	r3, [r7, #92]
 5794 042c 1A68     		ldr	r2, [r3, #0]
 5795 042e BB6D     		ldr	r3, [r7, #88]
 5796 0430 1A60     		str	r2, [r3, #0]
 5797 0432 BB6D     		ldr	r3, [r7, #88]
 5798 0434 03F10403 		add	r3, r3, #4
 5799 0438 BB65     		str	r3, [r7, #88]
 5800 043a FB6D     		ldr	r3, [r7, #92]
 5801 043c 03F10403 		add	r3, r3, #4
 5802 0440 FB65     		str	r3, [r7, #92]
 5803              	.L132:
 5804              		.loc 1 2931 0 discriminator 4
 5805 0442 FB6D     		ldr	r3, [r7, #92]
 5806 0444 1A68     		ldr	r2, [r3, #0]
 5807 0446 BB6D     		ldr	r3, [r7, #88]
 5808 0448 1A60     		str	r2, [r3, #0]
 5809 044a BB6D     		ldr	r3, [r7, #88]
 5810 044c 03F10403 		add	r3, r3, #4
 5811 0450 BB65     		str	r3, [r7, #88]
 5812 0452 FB6D     		ldr	r3, [r7, #92]
 5813 0454 03F10403 		add	r3, r3, #4
 5814 0458 FB65     		str	r3, [r7, #92]
 5815 045a FB6D     		ldr	r3, [r7, #92]
 5816 045c 1A68     		ldr	r2, [r3, #0]
 5817 045e BB6D     		ldr	r3, [r7, #88]
 5818 0460 1A60     		str	r2, [r3, #0]
 5819 0462 BB6D     		ldr	r3, [r7, #88]
 5820 0464 03F10403 		add	r3, r3, #4
 5821 0468 BB65     		str	r3, [r7, #88]
 5822 046a FB6D     		ldr	r3, [r7, #92]
 5823 046c 03F10403 		add	r3, r3, #4
 5824 0470 FB65     		str	r3, [r7, #92]
 5825 0472 FB6D     		ldr	r3, [r7, #92]
 5826 0474 1A68     		ldr	r2, [r3, #0]
 5827 0476 BB6D     		ldr	r3, [r7, #88]
 5828 0478 1A60     		str	r2, [r3, #0]
 5829              	.LBE5:
 5830              	.LBE4:
2932:../target/stm32/malloc/mallocr.c ****           goto split;
 5831              		.loc 1 2932 0 is_stmt 1 discriminator 4
 5832 047a 52E1     		b	.L119
 5833              	.L131:
 5834              	.LBB6:
2931:../target/stm32/malloc/mallocr.c ****           MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
 5835              		.loc 1 2931 0 discriminator 2
 5836 047c B86A     		ldr	r0, [r7, #40]
 5837 047e B968     		ldr	r1, [r7, #8]
 5838 0480 3A6A     		ldr	r2, [r7, #32]
 5839 0482 FFF7FEFF 		bl	memmove
 5840              	.LBE6:
 5841              		.loc 1 2932 0 discriminator 2
 5842 0486 4CE1     		b	.L119
 5843              	.L126:
2933:../target/stm32/malloc/mallocr.c ****         }
2934:../target/stm32/malloc/mallocr.c ****       }
2935:../target/stm32/malloc/mallocr.c ****       
2936:../target/stm32/malloc/mallocr.c ****       /* backward only */
2937:../target/stm32/malloc/mallocr.c ****       if (prev != 0 && (long)(prevsize + newsize) >= (long)nb)  
 5844              		.loc 1 2937 0
 5845 0488 3B6B     		ldr	r3, [r7, #48]
 5846 048a 002B     		cmp	r3, #0
 5847 048c 00F09D80 		beq	.L125
 5848              		.loc 1 2937 0 is_stmt 0 discriminator 1
 5849 0490 FA6A     		ldr	r2, [r7, #44]
 5850 0492 3B6F     		ldr	r3, [r7, #112]
 5851 0494 D318     		adds	r3, r2, r3
 5852 0496 1A46     		mov	r2, r3
 5853 0498 FB6B     		ldr	r3, [r7, #60]
 5854 049a 9A42     		cmp	r2, r3
 5855 049c C0F29580 		blt	.L125
2938:../target/stm32/malloc/mallocr.c ****       {
2939:../target/stm32/malloc/mallocr.c ****         unlink(prev, bck, fwd);
 5856              		.loc 1 2939 0 is_stmt 1
 5857 04a0 3B6B     		ldr	r3, [r7, #48]
 5858 04a2 DB68     		ldr	r3, [r3, #12]
 5859 04a4 BB63     		str	r3, [r7, #56]
 5860 04a6 3B6B     		ldr	r3, [r7, #48]
 5861 04a8 9B68     		ldr	r3, [r3, #8]
 5862 04aa 7B63     		str	r3, [r7, #52]
 5863 04ac 7B6B     		ldr	r3, [r7, #52]
 5864 04ae BA6B     		ldr	r2, [r7, #56]
 5865 04b0 DA60     		str	r2, [r3, #12]
 5866 04b2 BB6B     		ldr	r3, [r7, #56]
 5867 04b4 7A6B     		ldr	r2, [r7, #52]
 5868 04b6 9A60     		str	r2, [r3, #8]
2940:../target/stm32/malloc/mallocr.c ****         newp = prev;
 5869              		.loc 1 2940 0
 5870 04b8 3B6B     		ldr	r3, [r7, #48]
 5871 04ba 7B67     		str	r3, [r7, #116]
2941:../target/stm32/malloc/mallocr.c ****         newsize += prevsize;
 5872              		.loc 1 2941 0
 5873 04bc 3A6F     		ldr	r2, [r7, #112]
 5874 04be FB6A     		ldr	r3, [r7, #44]
 5875 04c0 D318     		adds	r3, r2, r3
 5876 04c2 3B67     		str	r3, [r7, #112]
2942:../target/stm32/malloc/mallocr.c ****         newmem = chunk2mem(newp);
 5877              		.loc 1 2942 0
 5878 04c4 7B6F     		ldr	r3, [r7, #116]
 5879 04c6 03F10803 		add	r3, r3, #8
 5880 04ca BB62     		str	r3, [r7, #40]
 5881              	.LBB7:
2943:../target/stm32/malloc/mallocr.c ****         MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
 5882              		.loc 1 2943 0
 5883 04cc 3B6C     		ldr	r3, [r7, #64]
 5884 04ce A3F10403 		sub	r3, r3, #4
 5885 04d2 FB61     		str	r3, [r7, #28]
 5886 04d4 FB69     		ldr	r3, [r7, #28]
 5887 04d6 242B     		cmp	r3, #36
 5888 04d8 71D8     		bhi	.L134
 5889              	.LBB8:
 5890              		.loc 1 2943 0 is_stmt 0 discriminator 1
 5891 04da BB68     		ldr	r3, [r7, #8]
 5892 04dc 7B65     		str	r3, [r7, #84]
 5893 04de BB6A     		ldr	r3, [r7, #40]
 5894 04e0 3B65     		str	r3, [r7, #80]
 5895 04e2 FB69     		ldr	r3, [r7, #28]
 5896 04e4 132B     		cmp	r3, #19
 5897 04e6 4DD9     		bls	.L135
 5898              		.loc 1 2943 0 discriminator 3
 5899 04e8 7B6D     		ldr	r3, [r7, #84]
 5900 04ea 1A68     		ldr	r2, [r3, #0]
 5901 04ec 3B6D     		ldr	r3, [r7, #80]
 5902 04ee 1A60     		str	r2, [r3, #0]
 5903 04f0 3B6D     		ldr	r3, [r7, #80]
 5904 04f2 03F10403 		add	r3, r3, #4
 5905 04f6 3B65     		str	r3, [r7, #80]
 5906 04f8 7B6D     		ldr	r3, [r7, #84]
 5907 04fa 03F10403 		add	r3, r3, #4
 5908 04fe 7B65     		str	r3, [r7, #84]
 5909 0500 7B6D     		ldr	r3, [r7, #84]
 5910 0502 1A68     		ldr	r2, [r3, #0]
 5911 0504 3B6D     		ldr	r3, [r7, #80]
 5912 0506 1A60     		str	r2, [r3, #0]
 5913 0508 3B6D     		ldr	r3, [r7, #80]
 5914 050a 03F10403 		add	r3, r3, #4
 5915 050e 3B65     		str	r3, [r7, #80]
 5916 0510 7B6D     		ldr	r3, [r7, #84]
 5917 0512 03F10403 		add	r3, r3, #4
 5918 0516 7B65     		str	r3, [r7, #84]
 5919 0518 FB69     		ldr	r3, [r7, #28]
 5920 051a 1B2B     		cmp	r3, #27
 5921 051c 32D9     		bls	.L135
 5922              		.loc 1 2943 0 discriminator 5
 5923 051e 7B6D     		ldr	r3, [r7, #84]
 5924 0520 1A68     		ldr	r2, [r3, #0]
 5925 0522 3B6D     		ldr	r3, [r7, #80]
 5926 0524 1A60     		str	r2, [r3, #0]
 5927 0526 3B6D     		ldr	r3, [r7, #80]
 5928 0528 03F10403 		add	r3, r3, #4
 5929 052c 3B65     		str	r3, [r7, #80]
 5930 052e 7B6D     		ldr	r3, [r7, #84]
 5931 0530 03F10403 		add	r3, r3, #4
 5932 0534 7B65     		str	r3, [r7, #84]
 5933 0536 7B6D     		ldr	r3, [r7, #84]
 5934 0538 1A68     		ldr	r2, [r3, #0]
 5935 053a 3B6D     		ldr	r3, [r7, #80]
 5936 053c 1A60     		str	r2, [r3, #0]
 5937 053e 3B6D     		ldr	r3, [r7, #80]
 5938 0540 03F10403 		add	r3, r3, #4
 5939 0544 3B65     		str	r3, [r7, #80]
 5940 0546 7B6D     		ldr	r3, [r7, #84]
 5941 0548 03F10403 		add	r3, r3, #4
 5942 054c 7B65     		str	r3, [r7, #84]
 5943 054e FB69     		ldr	r3, [r7, #28]
 5944 0550 232B     		cmp	r3, #35
 5945 0552 17D9     		bls	.L135
 5946              		.loc 1 2943 0 discriminator 6
 5947 0554 7B6D     		ldr	r3, [r7, #84]
 5948 0556 1A68     		ldr	r2, [r3, #0]
 5949 0558 3B6D     		ldr	r3, [r7, #80]
 5950 055a 1A60     		str	r2, [r3, #0]
 5951 055c 3B6D     		ldr	r3, [r7, #80]
 5952 055e 03F10403 		add	r3, r3, #4
 5953 0562 3B65     		str	r3, [r7, #80]
 5954 0564 7B6D     		ldr	r3, [r7, #84]
 5955 0566 03F10403 		add	r3, r3, #4
 5956 056a 7B65     		str	r3, [r7, #84]
 5957 056c 7B6D     		ldr	r3, [r7, #84]
 5958 056e 1A68     		ldr	r2, [r3, #0]
 5959 0570 3B6D     		ldr	r3, [r7, #80]
 5960 0572 1A60     		str	r2, [r3, #0]
 5961 0574 3B6D     		ldr	r3, [r7, #80]
 5962 0576 03F10403 		add	r3, r3, #4
 5963 057a 3B65     		str	r3, [r7, #80]
 5964 057c 7B6D     		ldr	r3, [r7, #84]
 5965 057e 03F10403 		add	r3, r3, #4
 5966 0582 7B65     		str	r3, [r7, #84]
 5967              	.L135:
 5968              		.loc 1 2943 0 discriminator 4
 5969 0584 7B6D     		ldr	r3, [r7, #84]
 5970 0586 1A68     		ldr	r2, [r3, #0]
 5971 0588 3B6D     		ldr	r3, [r7, #80]
 5972 058a 1A60     		str	r2, [r3, #0]
 5973 058c 3B6D     		ldr	r3, [r7, #80]
 5974 058e 03F10403 		add	r3, r3, #4
 5975 0592 3B65     		str	r3, [r7, #80]
 5976 0594 7B6D     		ldr	r3, [r7, #84]
 5977 0596 03F10403 		add	r3, r3, #4
 5978 059a 7B65     		str	r3, [r7, #84]
 5979 059c 7B6D     		ldr	r3, [r7, #84]
 5980 059e 1A68     		ldr	r2, [r3, #0]
 5981 05a0 3B6D     		ldr	r3, [r7, #80]
 5982 05a2 1A60     		str	r2, [r3, #0]
 5983 05a4 3B6D     		ldr	r3, [r7, #80]
 5984 05a6 03F10403 		add	r3, r3, #4
 5985 05aa 3B65     		str	r3, [r7, #80]
 5986 05ac 7B6D     		ldr	r3, [r7, #84]
 5987 05ae 03F10403 		add	r3, r3, #4
 5988 05b2 7B65     		str	r3, [r7, #84]
 5989 05b4 7B6D     		ldr	r3, [r7, #84]
 5990 05b6 1A68     		ldr	r2, [r3, #0]
 5991 05b8 3B6D     		ldr	r3, [r7, #80]
 5992 05ba 1A60     		str	r2, [r3, #0]
 5993              	.LBE8:
 5994              	.LBE7:
2944:../target/stm32/malloc/mallocr.c ****         goto split;
 5995              		.loc 1 2944 0 is_stmt 1 discriminator 4
 5996 05bc B1E0     		b	.L119
 5997              	.L134:
 5998              	.LBB9:
2943:../target/stm32/malloc/mallocr.c ****         MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
 5999              		.loc 1 2943 0 discriminator 2
 6000 05be B86A     		ldr	r0, [r7, #40]
 6001 05c0 B968     		ldr	r1, [r7, #8]
 6002 05c2 FA69     		ldr	r2, [r7, #28]
 6003 05c4 FFF7FEFF 		bl	memmove
 6004              	.LBE9:
 6005              		.loc 1 2944 0 discriminator 2
 6006 05c8 ABE0     		b	.L119
 6007              	.L125:
2945:../target/stm32/malloc/mallocr.c ****       }
2946:../target/stm32/malloc/mallocr.c ****     }
2947:../target/stm32/malloc/mallocr.c **** 
2948:../target/stm32/malloc/mallocr.c ****     /* Must allocate */
2949:../target/stm32/malloc/mallocr.c **** 
2950:../target/stm32/malloc/mallocr.c ****     newmem = mALLOc (RCALL bytes);
 6008              		.loc 1 2950 0
 6009 05ca F868     		ldr	r0, [r7, #12]
 6010 05cc 7968     		ldr	r1, [r7, #4]
 6011 05ce FFF7FEFF 		bl	_malloc_r
 6012 05d2 B862     		str	r0, [r7, #40]
2951:../target/stm32/malloc/mallocr.c **** 
2952:../target/stm32/malloc/mallocr.c ****     if (newmem == 0)  /* propagate failure */
 6013              		.loc 1 2952 0
 6014 05d4 BB6A     		ldr	r3, [r7, #40]
 6015 05d6 002B     		cmp	r3, #0
 6016 05d8 05D1     		bne	.L137
2953:../target/stm32/malloc/mallocr.c ****     {
2954:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
 6017              		.loc 1 2954 0
 6018 05da F868     		ldr	r0, [r7, #12]
 6019 05dc FFF7FEFF 		bl	__malloc_unlock
2955:../target/stm32/malloc/mallocr.c ****       return 0;
 6020              		.loc 1 2955 0
 6021 05e0 4FF00003 		mov	r3, #0
 6022 05e4 DFE0     		b	.L114
 6023              	.L137:
2956:../target/stm32/malloc/mallocr.c ****     }
2957:../target/stm32/malloc/mallocr.c **** 
2958:../target/stm32/malloc/mallocr.c ****     /* Avoid copy if newp is next chunk after oldp. */
2959:../target/stm32/malloc/mallocr.c ****     /* (This can only happen when new chunk is sbrk'ed.) */
2960:../target/stm32/malloc/mallocr.c **** 
2961:../target/stm32/malloc/mallocr.c ****     if ( (newp = mem2chunk(newmem)) == next_chunk(oldp)) 
 6024              		.loc 1 2961 0
 6025 05e6 BB6A     		ldr	r3, [r7, #40]
 6026 05e8 A3F10803 		sub	r3, r3, #8
 6027 05ec 7B67     		str	r3, [r7, #116]
 6028 05ee 7B6C     		ldr	r3, [r7, #68]
 6029 05f0 5B68     		ldr	r3, [r3, #4]
 6030 05f2 23F00103 		bic	r3, r3, #1
 6031 05f6 7A6C     		ldr	r2, [r7, #68]
 6032 05f8 D318     		adds	r3, r2, r3
 6033 05fa 7A6F     		ldr	r2, [r7, #116]
 6034 05fc 9A42     		cmp	r2, r3
 6035 05fe 09D1     		bne	.L138
2962:../target/stm32/malloc/mallocr.c ****     {
2963:../target/stm32/malloc/mallocr.c ****       newsize += chunksize(newp);
 6036              		.loc 1 2963 0
 6037 0600 7B6F     		ldr	r3, [r7, #116]
 6038 0602 5B68     		ldr	r3, [r3, #4]
 6039 0604 23F00303 		bic	r3, r3, #3
 6040 0608 3A6F     		ldr	r2, [r7, #112]
 6041 060a D318     		adds	r3, r2, r3
 6042 060c 3B67     		str	r3, [r7, #112]
2964:../target/stm32/malloc/mallocr.c ****       newp = oldp;
 6043              		.loc 1 2964 0
 6044 060e 7B6C     		ldr	r3, [r7, #68]
 6045 0610 7B67     		str	r3, [r7, #116]
2965:../target/stm32/malloc/mallocr.c ****       goto split;
 6046              		.loc 1 2965 0
 6047 0612 86E0     		b	.L119
 6048              	.L138:
 6049              	.LBB10:
2966:../target/stm32/malloc/mallocr.c ****     }
2967:../target/stm32/malloc/mallocr.c **** 
2968:../target/stm32/malloc/mallocr.c ****     /* Otherwise copy, free, and exit */
2969:../target/stm32/malloc/mallocr.c ****     MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
 6050              		.loc 1 2969 0
 6051 0614 3B6C     		ldr	r3, [r7, #64]
 6052 0616 A3F10403 		sub	r3, r3, #4
 6053 061a BB61     		str	r3, [r7, #24]
 6054 061c BB69     		ldr	r3, [r7, #24]
 6055 061e 242B     		cmp	r3, #36
 6056 0620 71D8     		bhi	.L139
 6057              	.LBB11:
 6058              		.loc 1 2969 0 is_stmt 0 discriminator 1
 6059 0622 BB68     		ldr	r3, [r7, #8]
 6060 0624 FB64     		str	r3, [r7, #76]
 6061 0626 BB6A     		ldr	r3, [r7, #40]
 6062 0628 BB64     		str	r3, [r7, #72]
 6063 062a BB69     		ldr	r3, [r7, #24]
 6064 062c 132B     		cmp	r3, #19
 6065 062e 4DD9     		bls	.L140
 6066              		.loc 1 2969 0 discriminator 3
 6067 0630 FB6C     		ldr	r3, [r7, #76]
 6068 0632 1A68     		ldr	r2, [r3, #0]
 6069 0634 BB6C     		ldr	r3, [r7, #72]
 6070 0636 1A60     		str	r2, [r3, #0]
 6071 0638 BB6C     		ldr	r3, [r7, #72]
 6072 063a 03F10403 		add	r3, r3, #4
 6073 063e BB64     		str	r3, [r7, #72]
 6074 0640 FB6C     		ldr	r3, [r7, #76]
 6075 0642 03F10403 		add	r3, r3, #4
 6076 0646 FB64     		str	r3, [r7, #76]
 6077 0648 FB6C     		ldr	r3, [r7, #76]
 6078 064a 1A68     		ldr	r2, [r3, #0]
 6079 064c BB6C     		ldr	r3, [r7, #72]
 6080 064e 1A60     		str	r2, [r3, #0]
 6081 0650 BB6C     		ldr	r3, [r7, #72]
 6082 0652 03F10403 		add	r3, r3, #4
 6083 0656 BB64     		str	r3, [r7, #72]
 6084 0658 FB6C     		ldr	r3, [r7, #76]
 6085 065a 03F10403 		add	r3, r3, #4
 6086 065e FB64     		str	r3, [r7, #76]
 6087 0660 BB69     		ldr	r3, [r7, #24]
 6088 0662 1B2B     		cmp	r3, #27
 6089 0664 32D9     		bls	.L140
 6090              		.loc 1 2969 0 discriminator 5
 6091 0666 FB6C     		ldr	r3, [r7, #76]
 6092 0668 1A68     		ldr	r2, [r3, #0]
 6093 066a BB6C     		ldr	r3, [r7, #72]
 6094 066c 1A60     		str	r2, [r3, #0]
 6095 066e BB6C     		ldr	r3, [r7, #72]
 6096 0670 03F10403 		add	r3, r3, #4
 6097 0674 BB64     		str	r3, [r7, #72]
 6098 0676 FB6C     		ldr	r3, [r7, #76]
 6099 0678 03F10403 		add	r3, r3, #4
 6100 067c FB64     		str	r3, [r7, #76]
 6101 067e FB6C     		ldr	r3, [r7, #76]
 6102 0680 1A68     		ldr	r2, [r3, #0]
 6103 0682 BB6C     		ldr	r3, [r7, #72]
 6104 0684 1A60     		str	r2, [r3, #0]
 6105 0686 BB6C     		ldr	r3, [r7, #72]
 6106 0688 03F10403 		add	r3, r3, #4
 6107 068c BB64     		str	r3, [r7, #72]
 6108 068e FB6C     		ldr	r3, [r7, #76]
 6109 0690 03F10403 		add	r3, r3, #4
 6110 0694 FB64     		str	r3, [r7, #76]
 6111 0696 BB69     		ldr	r3, [r7, #24]
 6112 0698 232B     		cmp	r3, #35
 6113 069a 17D9     		bls	.L140
 6114              		.loc 1 2969 0 discriminator 6
 6115 069c FB6C     		ldr	r3, [r7, #76]
 6116 069e 1A68     		ldr	r2, [r3, #0]
 6117 06a0 BB6C     		ldr	r3, [r7, #72]
 6118 06a2 1A60     		str	r2, [r3, #0]
 6119 06a4 BB6C     		ldr	r3, [r7, #72]
 6120 06a6 03F10403 		add	r3, r3, #4
 6121 06aa BB64     		str	r3, [r7, #72]
 6122 06ac FB6C     		ldr	r3, [r7, #76]
 6123 06ae 03F10403 		add	r3, r3, #4
 6124 06b2 FB64     		str	r3, [r7, #76]
 6125 06b4 FB6C     		ldr	r3, [r7, #76]
 6126 06b6 1A68     		ldr	r2, [r3, #0]
 6127 06b8 BB6C     		ldr	r3, [r7, #72]
 6128 06ba 1A60     		str	r2, [r3, #0]
 6129 06bc BB6C     		ldr	r3, [r7, #72]
 6130 06be 03F10403 		add	r3, r3, #4
 6131 06c2 BB64     		str	r3, [r7, #72]
 6132 06c4 FB6C     		ldr	r3, [r7, #76]
 6133 06c6 03F10403 		add	r3, r3, #4
 6134 06ca FB64     		str	r3, [r7, #76]
 6135              	.L140:
 6136              		.loc 1 2969 0 discriminator 4
 6137 06cc FB6C     		ldr	r3, [r7, #76]
 6138 06ce 1A68     		ldr	r2, [r3, #0]
 6139 06d0 BB6C     		ldr	r3, [r7, #72]
 6140 06d2 1A60     		str	r2, [r3, #0]
 6141 06d4 BB6C     		ldr	r3, [r7, #72]
 6142 06d6 03F10403 		add	r3, r3, #4
 6143 06da BB64     		str	r3, [r7, #72]
 6144 06dc FB6C     		ldr	r3, [r7, #76]
 6145 06de 03F10403 		add	r3, r3, #4
 6146 06e2 FB64     		str	r3, [r7, #76]
 6147 06e4 FB6C     		ldr	r3, [r7, #76]
 6148 06e6 1A68     		ldr	r2, [r3, #0]
 6149 06e8 BB6C     		ldr	r3, [r7, #72]
 6150 06ea 1A60     		str	r2, [r3, #0]
 6151 06ec BB6C     		ldr	r3, [r7, #72]
 6152 06ee 03F10403 		add	r3, r3, #4
 6153 06f2 BB64     		str	r3, [r7, #72]
 6154 06f4 FB6C     		ldr	r3, [r7, #76]
 6155 06f6 03F10403 		add	r3, r3, #4
 6156 06fa FB64     		str	r3, [r7, #76]
 6157 06fc FB6C     		ldr	r3, [r7, #76]
 6158 06fe 1A68     		ldr	r2, [r3, #0]
 6159 0700 BB6C     		ldr	r3, [r7, #72]
 6160 0702 1A60     		str	r2, [r3, #0]
 6161 0704 04E0     		b	.L141
 6162              	.L139:
 6163              	.LBE11:
 6164              		.loc 1 2969 0 discriminator 2
 6165 0706 B86A     		ldr	r0, [r7, #40]
 6166 0708 B968     		ldr	r1, [r7, #8]
 6167 070a BA69     		ldr	r2, [r7, #24]
 6168 070c FFF7FEFF 		bl	memmove
 6169              	.L141:
 6170              	.LBE10:
2970:../target/stm32/malloc/mallocr.c ****     fREe(RCALL oldmem);
 6171              		.loc 1 2970 0 is_stmt 1
 6172 0710 F868     		ldr	r0, [r7, #12]
 6173 0712 B968     		ldr	r1, [r7, #8]
 6174 0714 FFF7FEFF 		bl	_free_r
2971:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
 6175              		.loc 1 2971 0
 6176 0718 F868     		ldr	r0, [r7, #12]
 6177 071a FFF7FEFF 		bl	__malloc_unlock
2972:../target/stm32/malloc/mallocr.c ****     return newmem;
 6178              		.loc 1 2972 0
 6179 071e BB6A     		ldr	r3, [r7, #40]
 6180 0720 41E0     		b	.L114
 6181              	.L119:
2973:../target/stm32/malloc/mallocr.c ****   }
2974:../target/stm32/malloc/mallocr.c **** 
2975:../target/stm32/malloc/mallocr.c **** 
2976:../target/stm32/malloc/mallocr.c ****  split:  /* split off extra room in old or expanded chunk */
2977:../target/stm32/malloc/mallocr.c **** 
2978:../target/stm32/malloc/mallocr.c ****   remainder_size = long_sub_size_t(newsize, nb);
 6182              		.loc 1 2978 0
 6183 0722 3A6F     		ldr	r2, [r7, #112]
 6184 0724 FB6B     		ldr	r3, [r7, #60]
 6185 0726 D31A     		subs	r3, r2, r3
 6186 0728 7B61     		str	r3, [r7, #20]
2979:../target/stm32/malloc/mallocr.c **** 
2980:../target/stm32/malloc/mallocr.c ****   if (remainder_size >= (long)MINSIZE) /* split off remainder */
 6187              		.loc 1 2980 0
 6188 072a 7B69     		ldr	r3, [r7, #20]
 6189 072c 0F2B     		cmp	r3, #15
 6190 072e 22D9     		bls	.L142
2981:../target/stm32/malloc/mallocr.c ****   {
2982:../target/stm32/malloc/mallocr.c ****     remainder = chunk_at_offset(newp, nb);
 6191              		.loc 1 2982 0
 6192 0730 7A6F     		ldr	r2, [r7, #116]
 6193 0732 FB6B     		ldr	r3, [r7, #60]
 6194 0734 D318     		adds	r3, r2, r3
 6195 0736 3B61     		str	r3, [r7, #16]
2983:../target/stm32/malloc/mallocr.c ****     set_head_size(newp, nb);
 6196              		.loc 1 2983 0
 6197 0738 7B6F     		ldr	r3, [r7, #116]
 6198 073a 5B68     		ldr	r3, [r3, #4]
 6199 073c 03F00102 		and	r2, r3, #1
 6200 0740 FB6B     		ldr	r3, [r7, #60]
 6201 0742 1A43     		orrs	r2, r2, r3
 6202 0744 7B6F     		ldr	r3, [r7, #116]
 6203 0746 5A60     		str	r2, [r3, #4]
2984:../target/stm32/malloc/mallocr.c ****     set_head(remainder, remainder_size | PREV_INUSE);
 6204              		.loc 1 2984 0
 6205 0748 7B69     		ldr	r3, [r7, #20]
 6206 074a 43F00102 		orr	r2, r3, #1
 6207 074e 3B69     		ldr	r3, [r7, #16]
 6208 0750 5A60     		str	r2, [r3, #4]
2985:../target/stm32/malloc/mallocr.c ****     set_inuse_bit_at_offset(remainder, remainder_size);
 6209              		.loc 1 2985 0
 6210 0752 3A69     		ldr	r2, [r7, #16]
 6211 0754 7B69     		ldr	r3, [r7, #20]
 6212 0756 D318     		adds	r3, r2, r3
 6213 0758 3969     		ldr	r1, [r7, #16]
 6214 075a 7A69     		ldr	r2, [r7, #20]
 6215 075c 8A18     		adds	r2, r1, r2
 6216 075e 5268     		ldr	r2, [r2, #4]
 6217 0760 42F00102 		orr	r2, r2, #1
 6218 0764 5A60     		str	r2, [r3, #4]
2986:../target/stm32/malloc/mallocr.c ****     fREe(RCALL chunk2mem(remainder)); /* let free() deal with it */
 6219              		.loc 1 2986 0
 6220 0766 3B69     		ldr	r3, [r7, #16]
 6221 0768 03F10803 		add	r3, r3, #8
 6222 076c F868     		ldr	r0, [r7, #12]
 6223 076e 1946     		mov	r1, r3
 6224 0770 FFF7FEFF 		bl	_free_r
 6225 0774 11E0     		b	.L143
 6226              	.L142:
2987:../target/stm32/malloc/mallocr.c ****   }
2988:../target/stm32/malloc/mallocr.c ****   else
2989:../target/stm32/malloc/mallocr.c ****   {
2990:../target/stm32/malloc/mallocr.c ****     set_head_size(newp, newsize);
 6227              		.loc 1 2990 0
 6228 0776 7B6F     		ldr	r3, [r7, #116]
 6229 0778 5B68     		ldr	r3, [r3, #4]
 6230 077a 03F00102 		and	r2, r3, #1
 6231 077e 3B6F     		ldr	r3, [r7, #112]
 6232 0780 1A43     		orrs	r2, r2, r3
 6233 0782 7B6F     		ldr	r3, [r7, #116]
 6234 0784 5A60     		str	r2, [r3, #4]
2991:../target/stm32/malloc/mallocr.c ****     set_inuse_bit_at_offset(newp, newsize);
 6235              		.loc 1 2991 0
 6236 0786 7A6F     		ldr	r2, [r7, #116]
 6237 0788 3B6F     		ldr	r3, [r7, #112]
 6238 078a D318     		adds	r3, r2, r3
 6239 078c 796F     		ldr	r1, [r7, #116]
 6240 078e 3A6F     		ldr	r2, [r7, #112]
 6241 0790 8A18     		adds	r2, r1, r2
 6242 0792 5268     		ldr	r2, [r2, #4]
 6243 0794 42F00102 		orr	r2, r2, #1
 6244 0798 5A60     		str	r2, [r3, #4]
 6245              	.L143:
2992:../target/stm32/malloc/mallocr.c ****   }
2993:../target/stm32/malloc/mallocr.c **** 
2994:../target/stm32/malloc/mallocr.c ****   check_inuse_chunk(newp);
2995:../target/stm32/malloc/mallocr.c ****   MALLOC_UNLOCK;
 6246              		.loc 1 2995 0
 6247 079a F868     		ldr	r0, [r7, #12]
 6248 079c FFF7FEFF 		bl	__malloc_unlock
2996:../target/stm32/malloc/mallocr.c ****   return chunk2mem(newp);
 6249              		.loc 1 2996 0
 6250 07a0 7B6F     		ldr	r3, [r7, #116]
 6251 07a2 03F10803 		add	r3, r3, #8
 6252              	.L114:
2997:../target/stm32/malloc/mallocr.c **** 
2998:../target/stm32/malloc/mallocr.c **** #endif /* MALLOC_PROVIDED */
2999:../target/stm32/malloc/mallocr.c **** }
 6253              		.loc 1 2999 0
 6254 07a6 1846     		mov	r0, r3
 6255 07a8 07F17807 		add	r7, r7, #120
 6256 07ac BD46     		mov	sp, r7
 6257 07ae 80BD     		pop	{r7, pc}
 6258              		.cfi_endproc
 6259              	.LFE3:
 6261              		.section	.text._calloc_r,"ax",%progbits
 6262              		.align	2
 6263              		.global	_calloc_r
 6264              		.thumb
 6265              		.thumb_func
 6267              	_calloc_r:
 6268              	.LFB4:
3000:../target/stm32/malloc/mallocr.c **** 
3001:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_REALLOC */
3002:../target/stm32/malloc/mallocr.c **** 
3003:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_MEMALIGN
3004:../target/stm32/malloc/mallocr.c **** 
3005:../target/stm32/malloc/mallocr.c **** /*
3006:../target/stm32/malloc/mallocr.c **** 
3007:../target/stm32/malloc/mallocr.c ****   memalign algorithm:
3008:../target/stm32/malloc/mallocr.c **** 
3009:../target/stm32/malloc/mallocr.c ****     memalign requests more than enough space from malloc, finds a spot
3010:../target/stm32/malloc/mallocr.c ****     within that chunk that meets the alignment request, and then
3011:../target/stm32/malloc/mallocr.c ****     possibly frees the leading and trailing space. 
3012:../target/stm32/malloc/mallocr.c **** 
3013:../target/stm32/malloc/mallocr.c ****     The alignment argument must be a power of two. This property is not
3014:../target/stm32/malloc/mallocr.c ****     checked by memalign, so misuse may result in random runtime errors.
3015:../target/stm32/malloc/mallocr.c **** 
3016:../target/stm32/malloc/mallocr.c ****     8-byte alignment is guaranteed by normal malloc calls, so don't
3017:../target/stm32/malloc/mallocr.c ****     bother calling memalign with an argument of 8 or less.
3018:../target/stm32/malloc/mallocr.c **** 
3019:../target/stm32/malloc/mallocr.c ****     Overreliance on memalign is a sure way to fragment space.
3020:../target/stm32/malloc/mallocr.c **** 
3021:../target/stm32/malloc/mallocr.c **** */
3022:../target/stm32/malloc/mallocr.c **** 
3023:../target/stm32/malloc/mallocr.c **** 
3024:../target/stm32/malloc/mallocr.c **** #if __STD_C
3025:../target/stm32/malloc/mallocr.c **** Void_t* mEMALIGn(RARG size_t alignment, size_t bytes)
3026:../target/stm32/malloc/mallocr.c **** #else
3027:../target/stm32/malloc/mallocr.c **** Void_t* mEMALIGn(RARG alignment, bytes) RDECL size_t alignment; size_t bytes;
3028:../target/stm32/malloc/mallocr.c **** #endif
3029:../target/stm32/malloc/mallocr.c **** {
3030:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T    nb;      /* padded  request size */
3031:../target/stm32/malloc/mallocr.c ****   char*     m;                /* memory returned by malloc call */
3032:../target/stm32/malloc/mallocr.c ****   mchunkptr p;                /* corresponding chunk */
3033:../target/stm32/malloc/mallocr.c ****   char*     brk;              /* alignment point within p */
3034:../target/stm32/malloc/mallocr.c ****   mchunkptr newp;             /* chunk to return */
3035:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T  newsize;   /* its size */
3036:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T  leadsize;  /* leading space befor alignment point */
3037:../target/stm32/malloc/mallocr.c ****   mchunkptr remainder;        /* spare room at end to split off */
3038:../target/stm32/malloc/mallocr.c ****   long      remainder_size;   /* its size */
3039:../target/stm32/malloc/mallocr.c **** 
3040:../target/stm32/malloc/mallocr.c ****   /* If need less alignment than we give anyway, just relay to malloc */
3041:../target/stm32/malloc/mallocr.c **** 
3042:../target/stm32/malloc/mallocr.c ****   if (alignment <= MALLOC_ALIGNMENT) return mALLOc(RCALL bytes);
3043:../target/stm32/malloc/mallocr.c **** 
3044:../target/stm32/malloc/mallocr.c ****   /* Otherwise, ensure that it is at least a minimum chunk size */
3045:../target/stm32/malloc/mallocr.c ****   
3046:../target/stm32/malloc/mallocr.c ****   if (alignment <  MINSIZE) alignment = MINSIZE;
3047:../target/stm32/malloc/mallocr.c **** 
3048:../target/stm32/malloc/mallocr.c ****   /* Call malloc with worst case padding to hit alignment. */
3049:../target/stm32/malloc/mallocr.c **** 
3050:../target/stm32/malloc/mallocr.c ****   nb = request2size(bytes);
3051:../target/stm32/malloc/mallocr.c **** 
3052:../target/stm32/malloc/mallocr.c ****   /* Check for overflow. */
3053:../target/stm32/malloc/mallocr.c ****   if (nb > INT_MAX || nb < bytes)
3054:../target/stm32/malloc/mallocr.c ****   {
3055:../target/stm32/malloc/mallocr.c ****     RERRNO = ENOMEM;
3056:../target/stm32/malloc/mallocr.c ****     return 0;
3057:../target/stm32/malloc/mallocr.c ****   }
3058:../target/stm32/malloc/mallocr.c **** 
3059:../target/stm32/malloc/mallocr.c ****   m  = (char*)(mALLOc(RCALL nb + alignment + MINSIZE));
3060:../target/stm32/malloc/mallocr.c **** 
3061:../target/stm32/malloc/mallocr.c ****   if (m == 0) return 0; /* propagate failure */
3062:../target/stm32/malloc/mallocr.c **** 
3063:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK;
3064:../target/stm32/malloc/mallocr.c **** 
3065:../target/stm32/malloc/mallocr.c ****   p = mem2chunk(m);
3066:../target/stm32/malloc/mallocr.c **** 
3067:../target/stm32/malloc/mallocr.c ****   if ((((unsigned long)(m)) % alignment) == 0) /* aligned */
3068:../target/stm32/malloc/mallocr.c ****   {
3069:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
3070:../target/stm32/malloc/mallocr.c ****     if(chunk_is_mmapped(p))
3071:../target/stm32/malloc/mallocr.c ****     {
3072:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
3073:../target/stm32/malloc/mallocr.c ****       return chunk2mem(p); /* nothing more to do */
3074:../target/stm32/malloc/mallocr.c ****     }
3075:../target/stm32/malloc/mallocr.c **** #endif
3076:../target/stm32/malloc/mallocr.c ****   }
3077:../target/stm32/malloc/mallocr.c ****   else /* misaligned */
3078:../target/stm32/malloc/mallocr.c ****   {
3079:../target/stm32/malloc/mallocr.c ****     /* 
3080:../target/stm32/malloc/mallocr.c ****       Find an aligned spot inside chunk.
3081:../target/stm32/malloc/mallocr.c ****       Since we need to give back leading space in a chunk of at 
3082:../target/stm32/malloc/mallocr.c ****       least MINSIZE, if the first calculation places us at
3083:../target/stm32/malloc/mallocr.c ****       a spot with less than MINSIZE leader, we can move to the
3084:../target/stm32/malloc/mallocr.c ****       next aligned spot -- we've allocated enough total room so that
3085:../target/stm32/malloc/mallocr.c ****       this is always possible.
3086:../target/stm32/malloc/mallocr.c ****     */
3087:../target/stm32/malloc/mallocr.c **** 
3088:../target/stm32/malloc/mallocr.c ****     brk = (char*)mem2chunk(((unsigned long)(m + alignment - 1)) & -alignment);
3089:../target/stm32/malloc/mallocr.c ****     if ((long)(brk - (char*)(p)) < (long)MINSIZE) brk = brk + alignment;
3090:../target/stm32/malloc/mallocr.c **** 
3091:../target/stm32/malloc/mallocr.c ****     newp = (mchunkptr)brk;
3092:../target/stm32/malloc/mallocr.c ****     leadsize = brk - (char*)(p);
3093:../target/stm32/malloc/mallocr.c ****     newsize = chunksize(p) - leadsize;
3094:../target/stm32/malloc/mallocr.c **** 
3095:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
3096:../target/stm32/malloc/mallocr.c ****     if(chunk_is_mmapped(p)) 
3097:../target/stm32/malloc/mallocr.c ****     {
3098:../target/stm32/malloc/mallocr.c ****       newp->prev_size = p->prev_size + leadsize;
3099:../target/stm32/malloc/mallocr.c ****       set_head(newp, newsize|IS_MMAPPED);
3100:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
3101:../target/stm32/malloc/mallocr.c ****       return chunk2mem(newp);
3102:../target/stm32/malloc/mallocr.c ****     }
3103:../target/stm32/malloc/mallocr.c **** #endif
3104:../target/stm32/malloc/mallocr.c **** 
3105:../target/stm32/malloc/mallocr.c ****     /* give back leader, use the rest */
3106:../target/stm32/malloc/mallocr.c **** 
3107:../target/stm32/malloc/mallocr.c ****     set_head(newp, newsize | PREV_INUSE);
3108:../target/stm32/malloc/mallocr.c ****     set_inuse_bit_at_offset(newp, newsize);
3109:../target/stm32/malloc/mallocr.c ****     set_head_size(p, leadsize);
3110:../target/stm32/malloc/mallocr.c ****     fREe(RCALL chunk2mem(p));
3111:../target/stm32/malloc/mallocr.c ****     p = newp;
3112:../target/stm32/malloc/mallocr.c **** 
3113:../target/stm32/malloc/mallocr.c ****     assert (newsize >= nb && (((unsigned long)(chunk2mem(p))) % alignment) == 0);
3114:../target/stm32/malloc/mallocr.c ****   }
3115:../target/stm32/malloc/mallocr.c **** 
3116:../target/stm32/malloc/mallocr.c ****   /* Also give back spare room at the end */
3117:../target/stm32/malloc/mallocr.c **** 
3118:../target/stm32/malloc/mallocr.c ****   remainder_size = long_sub_size_t(chunksize(p), nb);
3119:../target/stm32/malloc/mallocr.c **** 
3120:../target/stm32/malloc/mallocr.c ****   if (remainder_size >= (long)MINSIZE)
3121:../target/stm32/malloc/mallocr.c ****   {
3122:../target/stm32/malloc/mallocr.c ****     remainder = chunk_at_offset(p, nb);
3123:../target/stm32/malloc/mallocr.c ****     set_head(remainder, remainder_size | PREV_INUSE);
3124:../target/stm32/malloc/mallocr.c ****     set_head_size(p, nb);
3125:../target/stm32/malloc/mallocr.c ****     fREe(RCALL chunk2mem(remainder));
3126:../target/stm32/malloc/mallocr.c ****   }
3127:../target/stm32/malloc/mallocr.c **** 
3128:../target/stm32/malloc/mallocr.c ****   check_inuse_chunk(p);
3129:../target/stm32/malloc/mallocr.c ****   MALLOC_UNLOCK;
3130:../target/stm32/malloc/mallocr.c ****   return chunk2mem(p);
3131:../target/stm32/malloc/mallocr.c **** 
3132:../target/stm32/malloc/mallocr.c **** }
3133:../target/stm32/malloc/mallocr.c **** 
3134:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_MEMALIGN */
3135:../target/stm32/malloc/mallocr.c **** 
3136:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_VALLOC
3137:../target/stm32/malloc/mallocr.c **** 
3138:../target/stm32/malloc/mallocr.c **** /*
3139:../target/stm32/malloc/mallocr.c ****     valloc just invokes memalign with alignment argument equal
3140:../target/stm32/malloc/mallocr.c ****     to the page size of the system (or as near to this as can
3141:../target/stm32/malloc/mallocr.c ****     be figured out from all the includes/defines above.)
3142:../target/stm32/malloc/mallocr.c **** */
3143:../target/stm32/malloc/mallocr.c **** 
3144:../target/stm32/malloc/mallocr.c **** #if __STD_C
3145:../target/stm32/malloc/mallocr.c **** Void_t* vALLOc(RARG size_t bytes)
3146:../target/stm32/malloc/mallocr.c **** #else
3147:../target/stm32/malloc/mallocr.c **** Void_t* vALLOc(RARG bytes) RDECL size_t bytes;
3148:../target/stm32/malloc/mallocr.c **** #endif
3149:../target/stm32/malloc/mallocr.c **** {
3150:../target/stm32/malloc/mallocr.c ****   return mEMALIGn (RCALL malloc_getpagesize, bytes);
3151:../target/stm32/malloc/mallocr.c **** }
3152:../target/stm32/malloc/mallocr.c **** 
3153:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_VALLOC */
3154:../target/stm32/malloc/mallocr.c **** 
3155:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_PVALLOC
3156:../target/stm32/malloc/mallocr.c **** 
3157:../target/stm32/malloc/mallocr.c **** /* 
3158:../target/stm32/malloc/mallocr.c ****   pvalloc just invokes valloc for the nearest pagesize
3159:../target/stm32/malloc/mallocr.c ****   that will accommodate request
3160:../target/stm32/malloc/mallocr.c **** */
3161:../target/stm32/malloc/mallocr.c **** 
3162:../target/stm32/malloc/mallocr.c **** 
3163:../target/stm32/malloc/mallocr.c **** #if __STD_C
3164:../target/stm32/malloc/mallocr.c **** Void_t* pvALLOc(RARG size_t bytes)
3165:../target/stm32/malloc/mallocr.c **** #else
3166:../target/stm32/malloc/mallocr.c **** Void_t* pvALLOc(RARG bytes) RDECL size_t bytes;
3167:../target/stm32/malloc/mallocr.c **** #endif
3168:../target/stm32/malloc/mallocr.c **** {
3169:../target/stm32/malloc/mallocr.c ****   size_t pagesize = malloc_getpagesize;
3170:../target/stm32/malloc/mallocr.c ****   return mEMALIGn (RCALL pagesize, (bytes + pagesize - 1) & ~(pagesize - 1));
3171:../target/stm32/malloc/mallocr.c **** }
3172:../target/stm32/malloc/mallocr.c **** 
3173:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_PVALLOC */
3174:../target/stm32/malloc/mallocr.c **** 
3175:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_CALLOC
3176:../target/stm32/malloc/mallocr.c **** 
3177:../target/stm32/malloc/mallocr.c **** /*
3178:../target/stm32/malloc/mallocr.c **** 
3179:../target/stm32/malloc/mallocr.c ****   calloc calls malloc, then zeroes out the allocated chunk.
3180:../target/stm32/malloc/mallocr.c **** 
3181:../target/stm32/malloc/mallocr.c **** */
3182:../target/stm32/malloc/mallocr.c **** 
3183:../target/stm32/malloc/mallocr.c **** #if __STD_C
3184:../target/stm32/malloc/mallocr.c **** Void_t* cALLOc(RARG size_t n, size_t elem_size)
3185:../target/stm32/malloc/mallocr.c **** #else
3186:../target/stm32/malloc/mallocr.c **** Void_t* cALLOc(RARG n, elem_size) RDECL size_t n; size_t elem_size;
3187:../target/stm32/malloc/mallocr.c **** #endif
3188:../target/stm32/malloc/mallocr.c **** {
 6269              		.loc 1 3188 0
 6270              		.cfi_startproc
 6271              		@ args = 0, pretend = 0, frame = 40
 6272              		@ frame_needed = 1, uses_anonymous_args = 0
 6273 0000 80B5     		push	{r7, lr}
 6274              	.LCFI12:
 6275              		.cfi_def_cfa_offset 8
 6276 0002 8AB0     		sub	sp, sp, #40
 6277              	.LCFI13:
 6278              		.cfi_def_cfa_offset 48
 6279 0004 00AF     		add	r7, sp, #0
 6280              		.cfi_offset 14, -4
 6281              		.cfi_offset 7, -8
 6282              	.LCFI14:
 6283              		.cfi_def_cfa_register 7
 6284 0006 F860     		str	r0, [r7, #12]
 6285 0008 B960     		str	r1, [r7, #8]
 6286 000a 7A60     		str	r2, [r7, #4]
3189:../target/stm32/malloc/mallocr.c ****   mchunkptr p;
3190:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T csz;
3191:../target/stm32/malloc/mallocr.c **** 
3192:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T sz = n * elem_size;
 6287              		.loc 1 3192 0
 6288 000c BB68     		ldr	r3, [r7, #8]
 6289 000e 7A68     		ldr	r2, [r7, #4]
 6290 0010 02FB03F3 		mul	r3, r2, r3
 6291 0014 3B62     		str	r3, [r7, #32]
3193:../target/stm32/malloc/mallocr.c **** 
3194:../target/stm32/malloc/mallocr.c **** #if MORECORE_CLEARS
3195:../target/stm32/malloc/mallocr.c ****   mchunkptr oldtop;
3196:../target/stm32/malloc/mallocr.c ****   INTERNAL_SIZE_T oldtopsize;
3197:../target/stm32/malloc/mallocr.c **** #endif
3198:../target/stm32/malloc/mallocr.c ****   Void_t* mem;
3199:../target/stm32/malloc/mallocr.c **** 
3200:../target/stm32/malloc/mallocr.c ****   /* check if expand_top called, in which case don't need to clear */
3201:../target/stm32/malloc/mallocr.c **** #if MORECORE_CLEARS
3202:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK;
3203:../target/stm32/malloc/mallocr.c ****   oldtop = top;
3204:../target/stm32/malloc/mallocr.c ****   oldtopsize = chunksize(top);
3205:../target/stm32/malloc/mallocr.c **** #endif
3206:../target/stm32/malloc/mallocr.c **** 
3207:../target/stm32/malloc/mallocr.c ****   mem = mALLOc (RCALL sz);
 6292              		.loc 1 3207 0
 6293 0016 F868     		ldr	r0, [r7, #12]
 6294 0018 396A     		ldr	r1, [r7, #32]
 6295 001a FFF7FEFF 		bl	_malloc_r
 6296 001e F861     		str	r0, [r7, #28]
3208:../target/stm32/malloc/mallocr.c **** 
3209:../target/stm32/malloc/mallocr.c ****   if (mem == 0) 
 6297              		.loc 1 3209 0
 6298 0020 FB69     		ldr	r3, [r7, #28]
 6299 0022 002B     		cmp	r3, #0
 6300 0024 02D1     		bne	.L147
3210:../target/stm32/malloc/mallocr.c ****   {
3211:../target/stm32/malloc/mallocr.c **** #if MORECORE_CLEARS
3212:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
3213:../target/stm32/malloc/mallocr.c **** #endif
3214:../target/stm32/malloc/mallocr.c ****     return 0;
 6301              		.loc 1 3214 0
 6302 0026 4FF00003 		mov	r3, #0
 6303 002a 66E0     		b	.L148
 6304              	.L147:
3215:../target/stm32/malloc/mallocr.c ****   }
3216:../target/stm32/malloc/mallocr.c ****   else
3217:../target/stm32/malloc/mallocr.c ****   {
3218:../target/stm32/malloc/mallocr.c ****     p = mem2chunk(mem);
 6305              		.loc 1 3218 0
 6306 002c FB69     		ldr	r3, [r7, #28]
 6307 002e A3F10803 		sub	r3, r3, #8
 6308 0032 BB61     		str	r3, [r7, #24]
3219:../target/stm32/malloc/mallocr.c **** 
3220:../target/stm32/malloc/mallocr.c ****     /* Two optional cases in which clearing not necessary */
3221:../target/stm32/malloc/mallocr.c **** 
3222:../target/stm32/malloc/mallocr.c **** 
3223:../target/stm32/malloc/mallocr.c **** #if HAVE_MMAP
3224:../target/stm32/malloc/mallocr.c ****     if (chunk_is_mmapped(p))
3225:../target/stm32/malloc/mallocr.c ****     {
3226:../target/stm32/malloc/mallocr.c **** #if MORECORE_CLEARS
3227:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
3228:../target/stm32/malloc/mallocr.c **** #endif
3229:../target/stm32/malloc/mallocr.c ****       return mem;
3230:../target/stm32/malloc/mallocr.c ****     }
3231:../target/stm32/malloc/mallocr.c **** #endif
3232:../target/stm32/malloc/mallocr.c **** 
3233:../target/stm32/malloc/mallocr.c ****     csz = chunksize(p);
 6309              		.loc 1 3233 0
 6310 0034 BB69     		ldr	r3, [r7, #24]
 6311 0036 5B68     		ldr	r3, [r3, #4]
 6312 0038 23F00303 		bic	r3, r3, #3
 6313 003c 7B61     		str	r3, [r7, #20]
 6314              	.LBB12:
3234:../target/stm32/malloc/mallocr.c **** 
3235:../target/stm32/malloc/mallocr.c **** #if MORECORE_CLEARS
3236:../target/stm32/malloc/mallocr.c ****     if (p == oldtop && csz > oldtopsize) 
3237:../target/stm32/malloc/mallocr.c ****     {
3238:../target/stm32/malloc/mallocr.c ****       /* clear only the bytes from non-freshly-sbrked memory */
3239:../target/stm32/malloc/mallocr.c ****       csz = oldtopsize;
3240:../target/stm32/malloc/mallocr.c ****     }
3241:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
3242:../target/stm32/malloc/mallocr.c **** #endif
3243:../target/stm32/malloc/mallocr.c **** 
3244:../target/stm32/malloc/mallocr.c ****     MALLOC_ZERO(mem, csz - SIZE_SZ);
 6315              		.loc 1 3244 0
 6316 003e 7B69     		ldr	r3, [r7, #20]
 6317 0040 A3F10403 		sub	r3, r3, #4
 6318 0044 3B61     		str	r3, [r7, #16]
 6319 0046 3B69     		ldr	r3, [r7, #16]
 6320 0048 242B     		cmp	r3, #36
 6321 004a 4FD8     		bhi	.L149
 6322              	.LBB13:
 6323              		.loc 1 3244 0 is_stmt 0 discriminator 1
 6324 004c FB69     		ldr	r3, [r7, #28]
 6325 004e 7B62     		str	r3, [r7, #36]
 6326 0050 3B69     		ldr	r3, [r7, #16]
 6327 0052 132B     		cmp	r3, #19
 6328 0054 35D9     		bls	.L150
 6329              		.loc 1 3244 0 discriminator 3
 6330 0056 7B6A     		ldr	r3, [r7, #36]
 6331 0058 4FF00002 		mov	r2, #0
 6332 005c 1A60     		str	r2, [r3, #0]
 6333 005e 7B6A     		ldr	r3, [r7, #36]
 6334 0060 03F10403 		add	r3, r3, #4
 6335 0064 7B62     		str	r3, [r7, #36]
 6336 0066 7B6A     		ldr	r3, [r7, #36]
 6337 0068 4FF00002 		mov	r2, #0
 6338 006c 1A60     		str	r2, [r3, #0]
 6339 006e 7B6A     		ldr	r3, [r7, #36]
 6340 0070 03F10403 		add	r3, r3, #4
 6341 0074 7B62     		str	r3, [r7, #36]
 6342 0076 3B69     		ldr	r3, [r7, #16]
 6343 0078 1B2B     		cmp	r3, #27
 6344 007a 22D9     		bls	.L150
 6345              		.loc 1 3244 0 discriminator 5
 6346 007c 7B6A     		ldr	r3, [r7, #36]
 6347 007e 4FF00002 		mov	r2, #0
 6348 0082 1A60     		str	r2, [r3, #0]
 6349 0084 7B6A     		ldr	r3, [r7, #36]
 6350 0086 03F10403 		add	r3, r3, #4
 6351 008a 7B62     		str	r3, [r7, #36]
 6352 008c 7B6A     		ldr	r3, [r7, #36]
 6353 008e 4FF00002 		mov	r2, #0
 6354 0092 1A60     		str	r2, [r3, #0]
 6355 0094 7B6A     		ldr	r3, [r7, #36]
 6356 0096 03F10403 		add	r3, r3, #4
 6357 009a 7B62     		str	r3, [r7, #36]
 6358 009c 3B69     		ldr	r3, [r7, #16]
 6359 009e 232B     		cmp	r3, #35
 6360 00a0 0FD9     		bls	.L150
 6361              		.loc 1 3244 0 discriminator 6
 6362 00a2 7B6A     		ldr	r3, [r7, #36]
 6363 00a4 4FF00002 		mov	r2, #0
 6364 00a8 1A60     		str	r2, [r3, #0]
 6365 00aa 7B6A     		ldr	r3, [r7, #36]
 6366 00ac 03F10403 		add	r3, r3, #4
 6367 00b0 7B62     		str	r3, [r7, #36]
 6368 00b2 7B6A     		ldr	r3, [r7, #36]
 6369 00b4 4FF00002 		mov	r2, #0
 6370 00b8 1A60     		str	r2, [r3, #0]
 6371 00ba 7B6A     		ldr	r3, [r7, #36]
 6372 00bc 03F10403 		add	r3, r3, #4
 6373 00c0 7B62     		str	r3, [r7, #36]
 6374              	.L150:
 6375              		.loc 1 3244 0 discriminator 4
 6376 00c2 7B6A     		ldr	r3, [r7, #36]
 6377 00c4 4FF00002 		mov	r2, #0
 6378 00c8 1A60     		str	r2, [r3, #0]
 6379 00ca 7B6A     		ldr	r3, [r7, #36]
 6380 00cc 03F10403 		add	r3, r3, #4
 6381 00d0 7B62     		str	r3, [r7, #36]
 6382 00d2 7B6A     		ldr	r3, [r7, #36]
 6383 00d4 4FF00002 		mov	r2, #0
 6384 00d8 1A60     		str	r2, [r3, #0]
 6385 00da 7B6A     		ldr	r3, [r7, #36]
 6386 00dc 03F10403 		add	r3, r3, #4
 6387 00e0 7B62     		str	r3, [r7, #36]
 6388 00e2 7B6A     		ldr	r3, [r7, #36]
 6389 00e4 4FF00002 		mov	r2, #0
 6390 00e8 1A60     		str	r2, [r3, #0]
 6391 00ea 05E0     		b	.L151
 6392              	.L149:
 6393              	.LBE13:
 6394              		.loc 1 3244 0 discriminator 2
 6395 00ec F869     		ldr	r0, [r7, #28]
 6396 00ee 4FF00001 		mov	r1, #0
 6397 00f2 3A69     		ldr	r2, [r7, #16]
 6398 00f4 FFF7FEFF 		bl	memset
 6399              	.L151:
 6400              	.LBE12:
3245:../target/stm32/malloc/mallocr.c ****     return mem;
 6401              		.loc 1 3245 0 is_stmt 1
 6402 00f8 FB69     		ldr	r3, [r7, #28]
 6403              	.L148:
3246:../target/stm32/malloc/mallocr.c ****   }
3247:../target/stm32/malloc/mallocr.c **** }
 6404              		.loc 1 3247 0
 6405 00fa 1846     		mov	r0, r3
 6406 00fc 07F12807 		add	r7, r7, #40
 6407 0100 BD46     		mov	sp, r7
 6408 0102 80BD     		pop	{r7, pc}
 6409              		.cfi_endproc
 6410              	.LFE4:
 6412              		.section	.text._malloc_trim_r,"ax",%progbits
 6413              		.align	2
 6414              		.global	_malloc_trim_r
 6415              		.thumb
 6416              		.thumb_func
 6418              	_malloc_trim_r:
 6419              	.LFB5:
3248:../target/stm32/malloc/mallocr.c **** 
3249:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_CALLOC */
3250:../target/stm32/malloc/mallocr.c **** 
3251:../target/stm32/malloc/mallocr.c **** #if defined(DEFINE_CFREE) && !defined(__CYGWIN__)
3252:../target/stm32/malloc/mallocr.c **** 
3253:../target/stm32/malloc/mallocr.c **** /*
3254:../target/stm32/malloc/mallocr.c ****  
3255:../target/stm32/malloc/mallocr.c ****   cfree just calls free. It is needed/defined on some systems
3256:../target/stm32/malloc/mallocr.c ****   that pair it with calloc, presumably for odd historical reasons.
3257:../target/stm32/malloc/mallocr.c **** 
3258:../target/stm32/malloc/mallocr.c **** */
3259:../target/stm32/malloc/mallocr.c **** 
3260:../target/stm32/malloc/mallocr.c **** #if !defined(INTERNAL_LINUX_C_LIB) || !defined(__ELF__)
3261:../target/stm32/malloc/mallocr.c **** #if !defined(INTERNAL_NEWLIB) || !defined(_REENT_ONLY)
3262:../target/stm32/malloc/mallocr.c **** #if __STD_C
3263:../target/stm32/malloc/mallocr.c **** void cfree(Void_t *mem)
3264:../target/stm32/malloc/mallocr.c **** #else
3265:../target/stm32/malloc/mallocr.c **** void cfree(mem) Void_t *mem;
3266:../target/stm32/malloc/mallocr.c **** #endif
3267:../target/stm32/malloc/mallocr.c **** {
3268:../target/stm32/malloc/mallocr.c **** #ifdef INTERNAL_NEWLIB
3269:../target/stm32/malloc/mallocr.c ****   fREe(_REENT, mem);
3270:../target/stm32/malloc/mallocr.c **** #else
3271:../target/stm32/malloc/mallocr.c ****   fREe(mem);
3272:../target/stm32/malloc/mallocr.c **** #endif
3273:../target/stm32/malloc/mallocr.c **** }
3274:../target/stm32/malloc/mallocr.c **** #endif
3275:../target/stm32/malloc/mallocr.c **** #endif
3276:../target/stm32/malloc/mallocr.c **** 
3277:../target/stm32/malloc/mallocr.c **** #endif /* DEFINE_CFREE */
3278:../target/stm32/malloc/mallocr.c **** 
3279:../target/stm32/malloc/mallocr.c **** #ifdef DEFINE_FREE
3280:../target/stm32/malloc/mallocr.c **** 
3281:../target/stm32/malloc/mallocr.c **** /*
3282:../target/stm32/malloc/mallocr.c **** 
3283:../target/stm32/malloc/mallocr.c ****     Malloc_trim gives memory back to the system (via negative
3284:../target/stm32/malloc/mallocr.c ****     arguments to sbrk) if there is unused memory at the `high' end of
3285:../target/stm32/malloc/mallocr.c ****     the malloc pool. You can call this after freeing large blocks of
3286:../target/stm32/malloc/mallocr.c ****     memory to potentially reduce the system-level memory requirements
3287:../target/stm32/malloc/mallocr.c ****     of a program. However, it cannot guarantee to reduce memory. Under
3288:../target/stm32/malloc/mallocr.c ****     some allocation patterns, some large free blocks of memory will be
3289:../target/stm32/malloc/mallocr.c ****     locked between two used chunks, so they cannot be given back to
3290:../target/stm32/malloc/mallocr.c ****     the system.
3291:../target/stm32/malloc/mallocr.c **** 
3292:../target/stm32/malloc/mallocr.c ****     The `pad' argument to malloc_trim represents the amount of free
3293:../target/stm32/malloc/mallocr.c ****     trailing space to leave untrimmed. If this argument is zero,
3294:../target/stm32/malloc/mallocr.c ****     only the minimum amount of memory to maintain internal data
3295:../target/stm32/malloc/mallocr.c ****     structures will be left (one page or less). Non-zero arguments
3296:../target/stm32/malloc/mallocr.c ****     can be supplied to maintain enough trailing space to service
3297:../target/stm32/malloc/mallocr.c ****     future expected allocations without having to re-obtain memory
3298:../target/stm32/malloc/mallocr.c ****     from the system.
3299:../target/stm32/malloc/mallocr.c **** 
3300:../target/stm32/malloc/mallocr.c ****     Malloc_trim returns 1 if it actually released any memory, else 0.
3301:../target/stm32/malloc/mallocr.c **** 
3302:../target/stm32/malloc/mallocr.c **** */
3303:../target/stm32/malloc/mallocr.c **** 
3304:../target/stm32/malloc/mallocr.c **** #if __STD_C
3305:../target/stm32/malloc/mallocr.c **** int malloc_trim(RARG size_t pad)
3306:../target/stm32/malloc/mallocr.c **** #else
3307:../target/stm32/malloc/mallocr.c **** int malloc_trim(RARG pad) RDECL size_t pad;
3308:../target/stm32/malloc/mallocr.c **** #endif
3309:../target/stm32/malloc/mallocr.c **** {
 6420              		.loc 1 3309 0
 6421              		.cfi_startproc
 6422              		@ args = 0, pretend = 0, frame = 32
 6423              		@ frame_needed = 1, uses_anonymous_args = 0
 6424 0000 80B5     		push	{r7, lr}
 6425              	.LCFI15:
 6426              		.cfi_def_cfa_offset 8
 6427 0002 88B0     		sub	sp, sp, #32
 6428              	.LCFI16:
 6429              		.cfi_def_cfa_offset 40
 6430 0004 00AF     		add	r7, sp, #0
 6431              		.cfi_offset 14, -4
 6432              		.cfi_offset 7, -8
 6433              	.LCFI17:
 6434              		.cfi_def_cfa_register 7
 6435 0006 7860     		str	r0, [r7, #4]
 6436 0008 3960     		str	r1, [r7, #0]
3310:../target/stm32/malloc/mallocr.c ****   long  top_size;        /* Amount of top-most memory */
3311:../target/stm32/malloc/mallocr.c ****   long  extra;           /* Amount to release */
3312:../target/stm32/malloc/mallocr.c ****   char* current_brk;     /* address returned by pre-check sbrk call */
3313:../target/stm32/malloc/mallocr.c ****   char* new_brk;         /* address returned by negative sbrk call */
3314:../target/stm32/malloc/mallocr.c **** 
3315:../target/stm32/malloc/mallocr.c ****   unsigned long pagesz = malloc_getpagesize;
 6437              		.loc 1 3315 0
 6438 000a 4FF48053 		mov	r3, #4096
 6439 000e FB61     		str	r3, [r7, #28]
3316:../target/stm32/malloc/mallocr.c **** 
3317:../target/stm32/malloc/mallocr.c ****   MALLOC_LOCK;
 6440              		.loc 1 3317 0
 6441 0010 7868     		ldr	r0, [r7, #4]
 6442 0012 FFF7FEFF 		bl	__malloc_lock
3318:../target/stm32/malloc/mallocr.c **** 
3319:../target/stm32/malloc/mallocr.c ****   top_size = chunksize(top);
 6443              		.loc 1 3319 0
 6444 0016 40F20003 		movw	r3, #:lower16:__malloc_av_
 6445 001a C0F20003 		movt	r3, #:upper16:__malloc_av_
 6446 001e 9B68     		ldr	r3, [r3, #8]
 6447 0020 5B68     		ldr	r3, [r3, #4]
 6448 0022 23F00303 		bic	r3, r3, #3
 6449 0026 BB61     		str	r3, [r7, #24]
3320:../target/stm32/malloc/mallocr.c ****   extra = ((top_size - pad - MINSIZE + (pagesz-1)) / pagesz - 1) * pagesz;
 6450              		.loc 1 3320 0
 6451 0028 BA69     		ldr	r2, [r7, #24]
 6452 002a 3B68     		ldr	r3, [r7, #0]
 6453 002c D21A     		subs	r2, r2, r3
 6454 002e FB69     		ldr	r3, [r7, #28]
 6455 0030 D318     		adds	r3, r2, r3
 6456 0032 A3F11102 		sub	r2, r3, #17
 6457 0036 FB69     		ldr	r3, [r7, #28]
 6458 0038 B2FBF3F3 		udiv	r3, r2, r3
 6459 003c 03F1FF33 		add	r3, r3, #-1
 6460 0040 FA69     		ldr	r2, [r7, #28]
 6461 0042 02FB03F3 		mul	r3, r2, r3
 6462 0046 7B61     		str	r3, [r7, #20]
3321:../target/stm32/malloc/mallocr.c **** 
3322:../target/stm32/malloc/mallocr.c ****   if (extra < (long)pagesz)  /* Not enough memory to release */
 6463              		.loc 1 3322 0
 6464 0048 FA69     		ldr	r2, [r7, #28]
 6465 004a 7B69     		ldr	r3, [r7, #20]
 6466 004c 9A42     		cmp	r2, r3
 6467 004e 05DD     		ble	.L153
3323:../target/stm32/malloc/mallocr.c ****   {
3324:../target/stm32/malloc/mallocr.c ****     MALLOC_UNLOCK;
 6468              		.loc 1 3324 0
 6469 0050 7868     		ldr	r0, [r7, #4]
 6470 0052 FFF7FEFF 		bl	__malloc_unlock
3325:../target/stm32/malloc/mallocr.c ****     return 0;
 6471              		.loc 1 3325 0
 6472 0056 4FF00003 		mov	r3, #0
 6473 005a 6DE0     		b	.L154
 6474              	.L153:
3326:../target/stm32/malloc/mallocr.c ****   }
3327:../target/stm32/malloc/mallocr.c **** 
3328:../target/stm32/malloc/mallocr.c ****   else
3329:../target/stm32/malloc/mallocr.c ****   {
3330:../target/stm32/malloc/mallocr.c ****     /* Test to make sure no one else called sbrk */
3331:../target/stm32/malloc/mallocr.c ****     current_brk = (char*)(MORECORE (0));
 6475              		.loc 1 3331 0
 6476 005c 7868     		ldr	r0, [r7, #4]
 6477 005e 4FF00001 		mov	r1, #0
 6478 0062 FFF7FEFF 		bl	_sbrk_r
 6479 0066 0346     		mov	r3, r0
 6480 0068 3B61     		str	r3, [r7, #16]
3332:../target/stm32/malloc/mallocr.c ****     if (current_brk != (char*)(top) + top_size)
 6481              		.loc 1 3332 0
 6482 006a 40F20003 		movw	r3, #:lower16:__malloc_av_
 6483 006e C0F20003 		movt	r3, #:upper16:__malloc_av_
 6484 0072 9B68     		ldr	r3, [r3, #8]
 6485 0074 1A46     		mov	r2, r3
 6486 0076 BB69     		ldr	r3, [r7, #24]
 6487 0078 D218     		adds	r2, r2, r3
 6488 007a 3B69     		ldr	r3, [r7, #16]
 6489 007c 9A42     		cmp	r2, r3
 6490 007e 05D0     		beq	.L155
3333:../target/stm32/malloc/mallocr.c ****     {
3334:../target/stm32/malloc/mallocr.c ****       MALLOC_UNLOCK;
 6491              		.loc 1 3334 0
 6492 0080 7868     		ldr	r0, [r7, #4]
 6493 0082 FFF7FEFF 		bl	__malloc_unlock
3335:../target/stm32/malloc/mallocr.c ****       return 0;     /* Apparently we don't own memory; must fail */
 6494              		.loc 1 3335 0
 6495 0086 4FF00003 		mov	r3, #0
 6496 008a 55E0     		b	.L154
 6497              	.L155:
3336:../target/stm32/malloc/mallocr.c ****     }
3337:../target/stm32/malloc/mallocr.c **** 
3338:../target/stm32/malloc/mallocr.c ****     else
3339:../target/stm32/malloc/mallocr.c ****     {
3340:../target/stm32/malloc/mallocr.c ****       new_brk = (char*)(MORECORE (-extra));
 6498              		.loc 1 3340 0
 6499 008c 7B69     		ldr	r3, [r7, #20]
 6500 008e C3F10003 		rsb	r3, r3, #0
 6501 0092 7868     		ldr	r0, [r7, #4]
 6502 0094 1946     		mov	r1, r3
 6503 0096 FFF7FEFF 		bl	_sbrk_r
 6504 009a 0346     		mov	r3, r0
 6505 009c FB60     		str	r3, [r7, #12]
3341:../target/stm32/malloc/mallocr.c ****       
3342:../target/stm32/malloc/mallocr.c ****       if (new_brk == (char*)(MORECORE_FAILURE)) /* sbrk failed? */
 6506              		.loc 1 3342 0
 6507 009e FB68     		ldr	r3, [r7, #12]
 6508 00a0 B3F1FF3F 		cmp	r3, #-1
 6509 00a4 2CD1     		bne	.L156
3343:../target/stm32/malloc/mallocr.c ****       {
3344:../target/stm32/malloc/mallocr.c ****         /* Try to figure out what we have */
3345:../target/stm32/malloc/mallocr.c ****         current_brk = (char*)(MORECORE (0));
 6510              		.loc 1 3345 0
 6511 00a6 7868     		ldr	r0, [r7, #4]
 6512 00a8 4FF00001 		mov	r1, #0
 6513 00ac FFF7FEFF 		bl	_sbrk_r
 6514 00b0 0346     		mov	r3, r0
 6515 00b2 3B61     		str	r3, [r7, #16]
3346:../target/stm32/malloc/mallocr.c ****         top_size = current_brk - (char*)top;
 6516              		.loc 1 3346 0
 6517 00b4 3A69     		ldr	r2, [r7, #16]
 6518 00b6 40F20003 		movw	r3, #:lower16:__malloc_av_
 6519 00ba C0F20003 		movt	r3, #:upper16:__malloc_av_
 6520 00be 9B68     		ldr	r3, [r3, #8]
 6521 00c0 D31A     		subs	r3, r2, r3
 6522 00c2 BB61     		str	r3, [r7, #24]
3347:../target/stm32/malloc/mallocr.c ****         if (top_size >= (long)MINSIZE) /* if not, we are very very dead! */
 6523              		.loc 1 3347 0
 6524 00c4 BB69     		ldr	r3, [r7, #24]
 6525 00c6 0F2B     		cmp	r3, #15
 6526 00c8 14DD     		ble	.L157
3348:../target/stm32/malloc/mallocr.c ****         {
3349:../target/stm32/malloc/mallocr.c ****           sbrked_mem = current_brk - sbrk_base;
 6527              		.loc 1 3349 0
 6528 00ca 3A69     		ldr	r2, [r7, #16]
 6529 00cc 40F20003 		movw	r3, #:lower16:__malloc_sbrk_base
 6530 00d0 C0F20003 		movt	r3, #:upper16:__malloc_sbrk_base
 6531 00d4 1B68     		ldr	r3, [r3, #0]
 6532 00d6 D21A     		subs	r2, r2, r3
 6533 00d8 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 6534 00dc C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 6535 00e0 1A60     		str	r2, [r3, #0]
3350:../target/stm32/malloc/mallocr.c ****           set_head(top, top_size | PREV_INUSE);
 6536              		.loc 1 3350 0
 6537 00e2 40F20003 		movw	r3, #:lower16:__malloc_av_
 6538 00e6 C0F20003 		movt	r3, #:upper16:__malloc_av_
 6539 00ea 9B68     		ldr	r3, [r3, #8]
 6540 00ec BA69     		ldr	r2, [r7, #24]
 6541 00ee 42F00102 		orr	r2, r2, #1
 6542 00f2 5A60     		str	r2, [r3, #4]
 6543              	.L157:
3351:../target/stm32/malloc/mallocr.c ****         }
3352:../target/stm32/malloc/mallocr.c ****         check_chunk(top);
3353:../target/stm32/malloc/mallocr.c **** 	MALLOC_UNLOCK;
 6544              		.loc 1 3353 0
 6545 00f4 7868     		ldr	r0, [r7, #4]
 6546 00f6 FFF7FEFF 		bl	__malloc_unlock
3354:../target/stm32/malloc/mallocr.c ****         return 0; 
 6547              		.loc 1 3354 0
 6548 00fa 4FF00003 		mov	r3, #0
 6549 00fe 1BE0     		b	.L154
 6550              	.L156:
3355:../target/stm32/malloc/mallocr.c ****       }
3356:../target/stm32/malloc/mallocr.c **** 
3357:../target/stm32/malloc/mallocr.c ****       else
3358:../target/stm32/malloc/mallocr.c ****       {
3359:../target/stm32/malloc/mallocr.c ****         /* Success. Adjust top accordingly. */
3360:../target/stm32/malloc/mallocr.c ****         set_head(top, (top_size - extra) | PREV_INUSE);
 6551              		.loc 1 3360 0
 6552 0100 40F20003 		movw	r3, #:lower16:__malloc_av_
 6553 0104 C0F20003 		movt	r3, #:upper16:__malloc_av_
 6554 0108 9B68     		ldr	r3, [r3, #8]
 6555 010a B969     		ldr	r1, [r7, #24]
 6556 010c 7A69     		ldr	r2, [r7, #20]
 6557 010e 8A1A     		subs	r2, r1, r2
 6558 0110 42F00102 		orr	r2, r2, #1
 6559 0114 5A60     		str	r2, [r3, #4]
3361:../target/stm32/malloc/mallocr.c ****         sbrked_mem -= extra;
 6560              		.loc 1 3361 0
 6561 0116 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 6562 011a C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 6563 011e 1A68     		ldr	r2, [r3, #0]
 6564 0120 7B69     		ldr	r3, [r7, #20]
 6565 0122 D21A     		subs	r2, r2, r3
 6566 0124 40F20003 		movw	r3, #:lower16:__malloc_current_mallinfo
 6567 0128 C0F20003 		movt	r3, #:upper16:__malloc_current_mallinfo
 6568 012c 1A60     		str	r2, [r3, #0]
3362:../target/stm32/malloc/mallocr.c ****         check_chunk(top);
3363:../target/stm32/malloc/mallocr.c **** 	MALLOC_UNLOCK;
 6569              		.loc 1 3363 0
 6570 012e 7868     		ldr	r0, [r7, #4]
 6571 0130 FFF7FEFF 		bl	__malloc_unlock
3364:../target/stm32/malloc/mallocr.c ****         return 1;
 6572              		.loc 1 3364 0
 6573 0134 4FF00103 		mov	r3, #1
 6574              	.L154:
3365:../target/stm32/malloc/mallocr.c ****       }
3366:../target/stm32/malloc/mallocr.c ****     }
3367:../target/stm32/malloc/mallocr.c ****   }
3368:../target/stm32/malloc/mallocr.c **** }
 6575              		.loc 1 3368 0
 6576 0138 1846     		mov	r0, r3
 6577 013a 07F12007 		add	r7, r7, #32
 6578 013e BD46     		mov	sp, r7
 6579 0140 80BD     		pop	{r7, pc}
 6580              		.cfi_endproc
 6581              	.LFE5:
 6583 0142 00BF     		.text
 6584              	.Letext0:
DEFINED SYMBOLS
                            *ABS*:00000000 mallocr.c
     /tmp/ccILx14Q.s:3124   .data.__malloc_av_:00000000 __malloc_av_
     /tmp/ccILx14Q.s:3121   .data.__malloc_av_:00000000 $d
     /tmp/ccILx14Q.s:3388   .data.__malloc_trim_threshold:00000000 __malloc_trim_threshold
     /tmp/ccILx14Q.s:3385   .data.__malloc_trim_threshold:00000000 $d
     /tmp/ccILx14Q.s:3395   .bss.__malloc_top_pad:00000000 __malloc_top_pad
     /tmp/ccILx14Q.s:3392   .bss.__malloc_top_pad:00000000 $d
     /tmp/ccILx14Q.s:3402   .data.__malloc_sbrk_base:00000000 __malloc_sbrk_base
     /tmp/ccILx14Q.s:3399   .data.__malloc_sbrk_base:00000000 $d
     /tmp/ccILx14Q.s:3409   .bss.__malloc_max_sbrked_mem:00000000 __malloc_max_sbrked_mem
     /tmp/ccILx14Q.s:3406   .bss.__malloc_max_sbrked_mem:00000000 $d
     /tmp/ccILx14Q.s:3416   .bss.__malloc_max_total_mem:00000000 __malloc_max_total_mem
     /tmp/ccILx14Q.s:3413   .bss.__malloc_max_total_mem:00000000 $d
     /tmp/ccILx14Q.s:3423   .bss.__malloc_current_mallinfo:00000000 __malloc_current_mallinfo
     /tmp/ccILx14Q.s:3420   .bss.__malloc_current_mallinfo:00000000 $d
     /tmp/ccILx14Q.s:3426   .text.malloc_extend_top:00000000 $t
     /tmp/ccILx14Q.s:3430   .text.malloc_extend_top:00000000 malloc_extend_top
     /tmp/ccILx14Q.s:4764   .text._free_r:00000000 _free_r
     /tmp/ccILx14Q.s:3777   .text._malloc_r:00000000 $t
     /tmp/ccILx14Q.s:3782   .text._malloc_r:00000000 _malloc_r
     /tmp/ccILx14Q.s:4759   .text._free_r:00000000 $t
     /tmp/ccILx14Q.s:6418   .text._malloc_trim_r:00000000 _malloc_trim_r
     /tmp/ccILx14Q.s:5232   .text._realloc_r:00000000 $t
     /tmp/ccILx14Q.s:5237   .text._realloc_r:00000000 _realloc_r
     /tmp/ccILx14Q.s:6262   .text._calloc_r:00000000 $t
     /tmp/ccILx14Q.s:6267   .text._calloc_r:00000000 _calloc_r
     /tmp/ccILx14Q.s:6413   .text._malloc_trim_r:00000000 $t
                     .debug_frame:00000010 $d

UNDEFINED SYMBOLS
_sbrk_r
__malloc_lock
__malloc_unlock
memmove
memset
